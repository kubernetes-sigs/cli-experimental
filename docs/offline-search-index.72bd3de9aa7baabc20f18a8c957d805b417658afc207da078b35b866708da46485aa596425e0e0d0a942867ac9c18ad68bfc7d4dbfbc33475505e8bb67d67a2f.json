[{"body":"Update the annotations on one or more resources\nAll Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and system extensions may use annotations to store their own data.\nCommand $ kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version] Example Current State $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6db489d4b7-zcc8h 1/1 Running 0 5s Command $ kubectl annotate pods nginx-6db489d4b7-zcc8h description=\u0026#39;standard gateway\u0026#39; pod/nginx-6db489d4b7-zcc8h annotated More Examples # Update pod \u0026#39;foo\u0026#39; with the annotation \u0026#39;description\u0026#39; and the value \u0026#39;my frontend\u0026#39;. # If the same annotation is set multiple times, only the last value will be applied kubectl annotate pods foo description=\u0026#39;my frontend\u0026#39; # Update a pod identified by type and name in \u0026#34;pod.json\u0026#34; kubectl annotate -f pod.json description=\u0026#39;my frontend\u0026#39; # Update pod \u0026#39;foo\u0026#39; with the annotation \u0026#39;description\u0026#39; and the value \u0026#39;my frontend running nginx\u0026#39;, overwriting any existing value. kubectl annotate --overwrite pods foo description=\u0026#39;my frontend running nginx\u0026#39; # Update all pods in the namespace kubectl annotate pods --all description=\u0026#39;my frontend running nginx\u0026#39; # Update pod \u0026#39;foo\u0026#39; only if the resource is unchanged from version 1. kubectl annotate pods foo description=\u0026#39;my frontend running nginx\u0026#39; --resource-version=1 # Update pod \u0026#39;foo\u0026#39; by removing an annotation named \u0026#39;description\u0026#39; if it exists. # Does not require the --overwrite flag. kubectl annotate pods foo description- ","excerpt":"Update the annotations on one or more resources\nAll Kubernetes objects support the ability to store …","ref":"/references/kubectl/annotation/","title":"annotation"},{"body":"apply with YAML files Apply can be run directly against Resource Config files or directories using -f\n# deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:5template:containers:- name:the-containerimage:registry/conatiner:latest# Apply the Resource Config kubectl apply -f deployment.yaml This will apply the deployment file on the Kubernetes cluster. You can get the status by using a get command.\n# Get deployments kubectl get deployments apply with Kustomize files Though Apply can be run directly against Resource Config files or directories using -f, it is recommended to run Apply against a kustomization.yaml using -k. The kustomization.yaml allows users to define configuration that cuts across many Resources (e.g. namespace).\n# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomization# list of Resource Config to be Appliedresources:- deployment.yaml# namespace to deploy all Resources tonamespace:default# labels added to all ResourcescommonLabels:app:exampleenv:test# deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:5template:containers:- name:the-containerimage:registry/conatiner:latestUsers run Apply on directories containing kustomization.yaml files using -k or on raw ResourceConfig files using -f.\n# Apply the Resource Config kubectl apply -k . # View the Resources kubectl get -k . ","excerpt":"apply with YAML files Apply can be run directly against Resource Config files or directories using …","ref":"/references/kubectl/apply/","title":"apply"},{"body":"Inspect if you are authorized to perform an action on / with the Kubernetes resources.\nCommand $ kubectl auth  Sub Commands  can-i reconcile   can-i Check whether an action is allowed.\nVERB is a logical Kubernetes API verb like \u0026lsquo;get\u0026rsquo;, \u0026lsquo;list\u0026rsquo;, \u0026lsquo;watch\u0026rsquo;, \u0026lsquo;delete\u0026rsquo;, etc. TYPE is a Kubernetes resource. Shortcuts and groups will be resolved. NONRESOURCEURL is a partial URL starts with \u0026ldquo;/\u0026rdquo;. NAME is the name of a particular Kubernetes resource.\nCommand $ kubectl auth can-i VERB [TYPE | TYPE/NAME | NONRESOURCEURL] Example I Command $ kubectl auth can-i create pods --all-namespaces yes Notice that the command yeilds yes as result - which means you are allowed to create pods on all possible namespaces avaiable.\nExample II Command $ kubectl auth can-i list deployments.apps yes reconcile Reconciles rules for RBAC Role, RoleBinding, ClusterRole, and ClusterRole binding objects.\nMissing objects are created, and the containing namespace is created for namespaced objects, if required.\nExisting roles are updated to include the permissions in the input objects, and remove extra permissions if \u0026ndash;remove-extra-permissions is specified.\nExisting bindings are updated to include the subjects in the input objects, and remove extra subjects if \u0026ndash;remove-extra-subjects is specified.\nThis is preferred to \u0026lsquo;apply\u0026rsquo; for RBAC resources so that semantically-aware merging of rules and subjects is done.\nCommand $ kubectl auth reconcile -f FILENAME Example TODO\n","excerpt":"Inspect if you are authorized to perform an action on / with the Kubernetes resources.\nCommand $ …","ref":"/references/kubectl/auth/","title":"auth"},{"body":"Creates an autoscaler that automatically chooses and sets the number of pods that run in a kubernetes cluster.\nLooks up a Deployment, ReplicaSet, StatefulSet, or ReplicationController by name and creates an autoscaler that uses the given resource as a reference. An autoscaler can automatically increase or decrease number of pods deployed within the system as needed.\nCommand $ kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [OR]\n$ kubectl hpa (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] hpa stands for Horizontal Pod Autoscale\nExample Current State $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6db489d4b7-zcc8h 1/1 Running 0 5s Command $ kubectl autoscale deployment nginx --min=2 --max=10 --cpu-percent=80 horizontalpodautoscaler.autoscaling/nginx autoscaled [OR]\n$ kubectl hpa deployment nginx --min=2 --max=10 --cpu-percent=80 horizontalpodautoscaler.autoscaling/nginx autoscaled This will make sure to auto-scale horizontally when the CPU usage hits 80%.\nOutput $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6db489d4b7-2rrrm 1/1 Running 0 15s nginx-6db489d4b7-vxqwm 1/1 Running 0 53s Notice that the command has an arg that says --min=2, the deployment instantaniously auto-scales to 2 pods.\n","excerpt":"Creates an autoscaler that automatically chooses and sets the number of pods that run in a …","ref":"/references/kubectl/autoscale/","title":"autoscale"},{"body":" The bases field was deprecated in v2.1.0\n Move entries into the resources field. This allows bases - which are still a central concept - to be ordered relative to other input resources.\n","excerpt":"The bases field was deprecated in v2.1.0\n Move entries into the resources field. This allows bases - …","ref":"/references/kustomize/bases/","title":"bases"},{"body":"Install kubectl binary with curl on Linux / macOS   Download the latest release with the command:\ncurl -LO \u0026#34;https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; To download a specific version, replace the $(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt) portion of the command with the specific version.\nFor example, to download version v1.19.0 on Linux, type:\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.19.0/bin/linux/amd64/kubectl   Make the kubectl binary executable.\nchmod +x ./kubectl   Move the binary in to your PATH.\nsudo mv ./kubectl /usr/local/bin/kubectl   Test to ensure the version you installed is up-to-date:\nkubectl version --client   ","excerpt":"Install kubectl binary with curl on Linux / macOS   Download the latest release with the command: …","ref":"/installation/kubectl/binaries/","title":"Binaries"},{"body":"A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.\nA ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.\nWarning ConfigMap does not provide secrecy or encryption. If the data you want to store are confidential, use a Secret rather than a ConfigMap, or use additional (third party) tools to keep your data private.  Command using File kubectl create configmap my-config --from-file=path/to/bar Example Input File # application.propertiesFOO=BarCommand kubectl create configmap my-config --from-file=application.properties Output $ kubectl get configmap NAME DATA AGE my-config 1 21s Command using Literal kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2 Example Command kubectl create configmap my-config --from-literal=FOO=Bar Output $ kubectl get configmap NAME DATA AGE my-config 1 21s Command using env file kubectl create configmap my-config --from-env-file=path/to/bar.env Example Input File # tracing.envENABLE_TRACING=trueSAMPLER_TYPE=probabilisticSAMPLER_PARAMETERS=0.1Command kubectl create configmap my-config --from-file=tracing.env Output $ kubectl get configmap NAME DATA AGE my-config 1 21s ","excerpt":"A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can …","ref":"/references/kubectl/create/configmap/","title":"configmap"},{"body":" TL;DR  Print the Logs of a Container in a cluster   Summarizing Resources Motivation Debugging Workloads by printing out the Logs of containers in a cluster.\nPrint Logs for a Container in a Pod Print the logs for a Pod running a single Container\nkubectl logs echo-c6bc8ccff-nnj52 hello hello  Operations One can also perfrom debugging operations such as:\n Print Logs for all Pods for a Workload Follow Logs for a Container Printing Logs for a Container that has exited Selecting a Container in a Pod Printing Logs After a Time Printing Logs Since a Time Include Timestamps  and so on.\n Command / Examples Check out the reference for commands and examples.  ","excerpt":"TL;DR  Print the Logs of a Container in a cluster   Summarizing Resources Motivation Debugging …","ref":"/guides/container_debugging/container_logs/","title":"Container Logs"},{"body":"Copy files and directories to and from containers.\nCommand $ kubectl cp \u0026lt;file-spec-src\u0026gt; \u0026lt;file-spec-dest\u0026gt; Example Current State files in local machine. Also notice a file named simple.txt which is about to be copied to a pod named nginx-6db489d4b7-qkd5d\n$ ls -l total 8 drwxr-xr-x 2 root root 4096 Mar 1 2020 Desktop -rw-r--r-- 1 root root 6 Sep 21 03:51 simple.txt files in a pod named nginx-6db489d4b7-qkd5d\n$ kubectl exec nginx-6db489d4b7-qkd5d -- ls -al total 84 drwxr-xr-x 1 root root 4096 Sep 21 03:17 . drwxr-xr-x 1 root root 4096 Sep 21 03:17 .. -rwxr-xr-x 1 root root 0 Sep 21 03:17 .dockerenv drwxr-xr-x 2 root root 4096 Sep 8 07:00 bin drwxr-xr-x 2 root root 4096 Jul 10 21:04 boot drwxr-xr-x 5 root root 360 Sep 21 03:17 dev ... drwxr-xr-x 10 root root 4096 Sep 8 07:00 usr drwxr-xr-x 1 root root 4096 Sep 8 07:00 var Command kubectl cp simple.txt nginx-6db489d4b7-qkd5d:. Notice the . followed by nginx-6db489d4b7-qkd5d: this specifies the current directory .i.e., root, if you want to copy the file to another directory - you can use absolute path to paste it there.\nExample: If you would like to copy simple.txt to /temp then use command\nkubectl cp simple.txt nginx-6db489d4b7-qkd5d:./temp Output $ kubectl exec nginx-6db489d4b7-qkd5d -- ls -al total 84 drwxr-xr-x 1 root root 4096 Sep 21 03:17 . drwxr-xr-x 1 root root 4096 Sep 21 03:17 .. -rwxr-xr-x 1 root root 0 Sep 21 03:17 .dockerenv ... -rw-r--r-- 1 root root 6 Sep 21 03:54 simple.txt ... drwxr-xr-x 10 root root 4096 Sep 8 07:00 usr drwxr-xr-x 1 root root 4096 Sep 8 07:00 var  Install Tar Copy requires that tar be installed in the container image.  Local to Remote Copy a local file to a remote Pod in a cluster.\n Local file format is \u0026lt;path\u0026gt; Remote file format is \u0026lt;pod-name\u0026gt;:\u0026lt;path\u0026gt;  kubectl cp /tmp/foo_dir \u0026lt;some-pod\u0026gt;:/tmp/bar_dir Remote to Local Copy a remote file from a Pod to a local file.\n Local file format is \u0026lt;path\u0026gt; Remote file format is \u0026lt;pod-name\u0026gt;:\u0026lt;path\u0026gt;  kubectl cp \u0026lt;some-pod\u0026gt;:/tmp/foo /tmp/bar Specify the Container Specify the Container within a Pod running multiple containers.\n -c \u0026lt;container-name\u0026gt;  kubectl cp /tmp/foo \u0026lt;some-pod\u0026gt;:/tmp/bar -c \u0026lt;specific-container\u0026gt; Namespaces Set the Pod namespace by prefixing the Pod name with \u0026lt;namespace\u0026gt;/ .\n \u0026lt;pod-namespace\u0026gt;/\u0026lt;pod-name\u0026gt;:\u0026lt;path\u0026gt;  kubectl cp /tmp/foo \u0026lt;some-namespace\u0026gt;/\u0026lt;some-pod\u0026gt;:/tmp/bar ","excerpt":"Copy files and directories to and from containers.\nCommand $ kubectl cp \u0026lt;file-spec-src\u0026gt; …","ref":"/references/kubectl/cp/","title":"cp"},{"body":"","excerpt":"","ref":"/references/kubectl/create/","title":"create"},{"body":"Delete resources by filenames, stdin, resources and names, or by resources and label selector.\nJSON and YAML formats are accepted. Only one type of the arguments may be specified: filenames, resources and names, or resources and label selector.\nCommand $ kubectl delete ([-f FILENAME] | [-k DIRECTORY] | TYPE [(NAME | -l label | --all)]) Example Current state $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 44s $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6db489d4b7-9wgn9 1/1 Running 0 28s Command $ kubectl delete deployments nginx deployment.apps \u0026#34;nginx\u0026#34; deleted Output $ kubectl get deployments No resources found in default namespace. ","excerpt":"Delete resources by filenames, stdin, resources and names, or by resources and label selector.\nJSON …","ref":"/references/kubectl/delete/","title":"delete"},{"body":"You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.\nCommand $ kubectl create deployment NAME --image=image -- [COMMAND] [args...] Example Command $ kubectl create deployment my-deployment --image=nginx Output $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE my-deployment 1/1 1 1 35s $ kubectl get pods NAME READY STATUS RESTARTS AGE my-deployment-7d6dd5c955-pr4jt 1/1 Running 0 15s ","excerpt":"You describe a desired state in a Deployment, and the Deployment Controller changes the actual state …","ref":"/references/kubectl/create/deployment/","title":"deployment"},{"body":"Show details of a specific resource or group of resources\nPrint a detailed description of the selected resources, including related resources such as events or controllers. You may select a single object by name, all objects of that type, provide a name prefix, or label selector.\nCommand $ kubectl describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME) Example Command kubectl describe deployments Output Name: nginx Namespace: default CreationTimestamp: Thu, 15 Nov 2018 10:58:03 -0800 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=1 Selector: app=nginx Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-78f5d695bd (1/1 replicas created) Events: \u0026lt;none\u0026gt; ","excerpt":"Show details of a specific resource or group of resources\nPrint a detailed description of the …","ref":"/references/kubectl/describe/","title":"describe"},{"body":"Diff configurations specified by filename or stdin between the current online configuration, and the configuration as it would be if applied.\nOutput is always YAML.\nKUBECTL_EXTERNAL_DIFF environment variable can be used to select your own diff command. By default, the \u0026ldquo;diff\u0026rdquo; command available in your path will be run with \u0026ldquo;-u\u0026rdquo; (unified diff) and \u0026ldquo;-N\u0026rdquo; (treat absent files as empty) options.\nExit status  0 No differences were found. 1 Differences were found. \u0026gt;1 Kubectl or diff failed with an error.   Command $ kubectl diff -f FILENAME Example Input File # deployment.yaml - online configurationapiVersion:apps/v1kind:Deploymentmetadata:name:nginx-devlabels:app:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.14.2# deployment.yaml - local configurationapiVersion:apps/v1kind:Deploymentmetadata:name:nginx-devlabels:app:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:latestNotice that the local configuration refers to the latest nginx container from the registry.\nCommand kubectl diff -f deployment.yaml Output diff -u -N /tmp/LIVE-435797985/apps.v1.Deployment.default.nginx-dev /tmp/MERGED-822429644/apps.v1.Deployment.default.nginx-dev --- /tmp/LIVE-435797985/apps.v1.Deployment.default.nginx-dev 2020-09-20 14:50:30.160820677 +0000 +++ /tmp/MERGED-822429644/apps.v1.Deployment.default.nginx-dev 2020-09-20 14:50:30.172820784 +0000 @@ -6,7 +6,7 @@ kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;nginx-dev\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;selector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;image\u0026#34;:\u0026#34;nginx:1.14.2\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;}]}}}} creationTimestamp: \u0026#34;2020-09-20T14:48:35Z\u0026#34; - generation: 1 + generation: 2 name: nginx-dev namespace: default resourceVersion: \u0026#34;2180\u0026#34; @@ -31,7 +31,7 @@ app: nginx spec: containers: - - image: nginx:1.14.2 + - image: nginx:latest imagePullPolicy: IfNotPresent name: nginx resources: {} exit status 1 Generating a Diff Use the diff program in a user\u0026rsquo;s path to display a diff of the changes that will be made by Apply.\nkubectl diff -k ./dir/ Setting the Diff Program The KUBECTL_EXTERNAL_DIFF environment variable can be used to select your own diff command. By default, the \u0026ldquo;diff\u0026rdquo; command available in your path will be run with \u0026ldquo;-u\u0026rdquo; (unified) and \u0026ldquo;-N\u0026rdquo; (treat new files as empty) options.\nexport KUBECTL_EXTERNAL_DIFF=meld; kubectl diff -k ./dir/ ","excerpt":"Diff configurations specified by filename or stdin between the current online configuration, and the …","ref":"/references/kubectl/diff/","title":"diff"},{"body":" TL;DR  View diff of changes before they are Applied to the cluster   Diffing Local and Cluster State Motivation The ability to view what changes will be made before applying them to a cluster can be useful.\nGenerating a Diff Use the diff program in a user\u0026rsquo;s path to display a diff of the changes that will be made by Apply.\nkubectl diff -k ./dir/ Setting the Diff Program The KUBECTL_EXTERNAL_DIFF environment variable can be used to select your own diff command. By default, the \u0026ldquo;diff\u0026rdquo; command available in your path will be run with \u0026ldquo;-u\u0026rdquo; (unified) and \u0026ldquo;-N\u0026rdquo; (treat new files as empty) options.\nexport KUBECTL_EXTERNAL_DIFF=meld; kubectl diff -k ./dir/ Exit status The following exit values shall be returned:\n0 No differences were found. 1 Differences were found. \u0026gt;1 Kubectl or diff failed with an error.\nNote: KUBECTL_EXTERNAL_DIFF, if used, is expected to follow that convention.\n","excerpt":"TL;DR  View diff of changes before they are Applied to the cluster   Diffing Local and Cluster State …","ref":"/guides/app_deployment/diffing_local_and_remote_resources/","title":"Diffing Local and Remote Resources"},{"body":"Edit a resource from the default editor.\nThe edit command allows you to directly edit any API resource you can retrieve via the command line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to \u0026lsquo;vi\u0026rsquo; for Linux or \u0026lsquo;notepad\u0026rsquo; for Windows. You can edit multiple objects, although changes are applied one at a time. The command accepts filenames as well as command line arguments, although the files you point to must be previously saved versions of resources.\nEditing is done with the API version used to fetch the resource. To edit using a specific API version, fully-qualify the resource, version, and group.\nThe default format is YAML. To edit in JSON, specify \u0026ldquo;-o json\u0026rdquo;.\nCommand $ kubectl edit (RESOURCE/NAME | -f FILENAME) Example Current State apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-devlabels:app:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.14.2$ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nginx-dev 1/1 1 1 13m Command $ kubectl edit deployment/nginx-dev Edit kind:DeploymentapiVersion:apps/v1metadata:name:nginx-devnamespace:defaultselfLink:/apis/apps/v1/namespaces/default/deployments/nginx-devuid:8799f7a6-e971-4285-bfac-0be1af6557d9resourceVersion:\u0026#39;2180\u0026#39;generation:1creationTimestamp:\u0026#39;2020-09-20T14:48:35Z\u0026#39;annotations:deployment.kubernetes.io/revision:\u0026#39;1\u0026#39;kubectl.kubernetes.io/last-applied-configuration:\u0026gt;{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;nginx-dev\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;selector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;image\u0026#34;:\u0026#34;nginx:1.14.2\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;}]}}}}spec:replicas:2selector:matchLabels:app:nginxtemplate:metadata:creationTimestamp:nulllabels:app:nginxspec:containers:- name:nginximage:\u0026#39;nginx:1.14.2\u0026#39;resources:{}terminationMessagePath:/dev/termination-logterminationMessagePolicy:FileimagePullPolicy:IfNotPresentrestartPolicy:AlwaysterminationGracePeriodSeconds:30dnsPolicy:ClusterFirstsecurityContext:{}schedulerName:default-schedulerstrategy:type:RollingUpdaterollingUpdate:maxUnavailable:25%maxSurge:25%revisionHistoryLimit:10progressDeadlineSeconds:600status:observedGeneration:1replicas:1updatedReplicas:1readyReplicas:1availableReplicas:1conditions:- type:Availablestatus:\u0026#39;True\u0026#39;lastUpdateTime:\u0026#39;2020-09-20T14:48:43Z\u0026#39;lastTransitionTime:\u0026#39;2020-09-20T14:48:43Z\u0026#39;reason:MinimumReplicasAvailablemessage:Deployment has minimum availability.- type:Progressingstatus:\u0026#39;True\u0026#39;lastUpdateTime:\u0026#39;2020-09-20T14:48:43Z\u0026#39;lastTransitionTime:\u0026#39;2020-09-20T14:48:35Z\u0026#39;reason:NewReplicaSetAvailablemessage:ReplicaSet \u0026#34;nginx-dev-59d7cd6545\u0026#34; has successfully progressed.Notice that the number of replicas is changes from 1 to 2\ndeployment.apps/nginx-dev edited Result NAME READY UP-TO-DATE AVAILABLE AGE nginx-dev 2/2 2 2 105s ","excerpt":"Edit a resource from the default editor.\nThe edit command allows you to directly edit any API …","ref":"/references/kubectl/edit/","title":"edit"},{"body":"Execute a command in a container\nCommand $ kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...] Example I Current State $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6db489d4b7-qkd5d 1/1 Running 0 20m Command kubectl exec nginx-6db489d4b7-qkd5d -- date Notice that date is the command that we are executing on the pod.\nOutput Mon Sep 21 03:38:53 UTC 2020 Example II Current State $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6db489d4b7-qkd5d 1/1 Running 0 20m Command kubectl exec nginx-6db489d4b7-qkd5d -- ls Notice that ls is the command that we are executing on the pod.\nOutput bin boot dev docker-entrypoint.d docker-entrypoint.sh etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var Exec Shell To get a Shell in a Container, use the -t -i options to get a tty and attach STDIN.\nkubectl exec -t -i nginx-78f5d695bd-czm8z bash root@nginx-78f5d695bd-czm8z:/# ls bin boot dev\tetc home lib\tlib64 media mnt opt\tproc root run sbin srv sys tmp usr var  Specifying the Container For Pods running multiple Containers, the Container should be specified with -c \u0026lt;container-name\u0026gt;.  ","excerpt":"Execute a command in a container\nCommand $ kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- …","ref":"/references/kubectl/exec/","title":"exec"},{"body":"Looks up a deployment, service, replica set, replication controller or pod by name and uses the selector for that resource as the selector for a new service on the specified port. A deployment or replica set will be exposed as a service only if its selector is convertible to a selector that service supports, i.e. when the selector contains only the matchLabels component. Note that if no port is specified via \u0026ndash;port and the exposed resource has multiple ports, all will be re-used by the new service. Also if no labels are specified, the new service will re-use the labels from the resource it exposes.\nResources Possible resources include (case insensitive):\n pod (po) service (svc) replicationcontroller (rc) deployment (deploy) replicaset (rs)   Command $ kubectl run NAME --image=image [--env=\u0026#34;key=value\u0026#34;] [--port=port] [--dry-run=server|client] $ kubectl expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol=TCP|UDP|SCTP] [--target-port=number-or-name] [--name=name] [--external-ip=external-ip-of-service] [--type=type] Example Current state $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6db489d4b7-9wgn9 1/1 Running 0 28s $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 44s Command $ kubectl expose po nginx-6db489d4b7-9wgn9 --port=80 --target-port=8000 Output $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 8m47s nginx-6db489d4b7-9wgn9 ClusterIP 10.106.133.77 \u0026lt;none\u0026gt; 80/TCP 2m56s ","excerpt":"Looks up a deployment, service, replica set, replication controller or pod by name and uses the …","ref":"/references/kubectl/expose/","title":"expose"},{"body":"fields Print the fields from the JSON Path\nNote: JSON Path can also be read from a file using -o custom-columns-file.\n JSON Path template is composed of JSONPath expressions enclosed by {}. In addition to the original JSONPath syntax, several capabilities are added: The $ operator is optional (the expression starts from the root object by default). Use \u0026quot;\u0026quot; to quote text inside JSONPath expressions. Use range operator to iterate lists. Use negative slice indices to step backwards through a list. Negative indices do not “wrap around” a list. They are valid as long as -index + listLength \u0026gt;= 0.  JSON Path Symbols Table    Function Description Example Result     text the plain text kind is {.kind} kind is List   @ the current object {@} the same as input   . or [] child operator {.kind} or {[‘kind’]} List   .. recursive descent {..name} 127.0.0.1 127.0.0.2 myself e2e   * wildcard. Get all objects {.items[*].metadata.name} [127.0.0.1 127.0.0.2]   [start:end :step] subscript operator {.users[0].name} myself   [,] union operator {.items[*][‘metadata.name’, ‘status.capacity’]} 127.0.0.1 127.0.0.2 map[cpu:4] map[cpu:8]   ?() filter {.users[?(@.name==“e2e”)].user.password} secret   range, end iterate list {range .items[*]}[{.metadata.name}, {.status.capacity}] {end} [127.0.0.1, map[cpu:4]] [127.0.0.2, map[cpu:8]]   “ quote interpreted string {range .items[*]}{.metadata.name}{’\\t’} {end} 127.0.0.1 127.0.0.2     Print the JSON representation of the first Deployment in the list on a single line.\nCommand I kubectl get deployment.v1.apps -o=jsonpath=\u0026#39;{.items[0]}{\u0026#34;\\n\u0026#34;}\u0026#39; Output map[apiVersion:apps/v1 kind:Deployment...replicas:1 updatedReplicas:1]]  Print the metadata.name field for the first Deployment in the list.\nCommand II kubectl get deployment.v1.apps -o=jsonpath=\u0026#39;{.items[0].metadata.name}{\u0026#34;\\n\u0026#34;}\u0026#39; Output nginx  For each Deployment, print its metadata.name field and a newline afterward.\nCommand III kubectl get deployment.v1.apps -o=jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; Output nginx nginx2  For each Deployment, print its metadata.name and .status.availableReplicas.\nCommand IV kubectl get deployment.v1.apps -o=jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.status.availableReplicas}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; Output nginx\t1 nginx2\t1  Print the list of Deployments as single line.\nCommand V kubectl get deployment.v1.apps -o=jsonpath=\u0026#39;{@}{\u0026#34;\\n\u0026#34;}\u0026#39; Output map[kind:List apiVersion:v1 metadata:map[selfLink: resourceVersion:] items:[map[apiVersion:apps/v1 kind:Deployment...replicas:1 updatedReplicas:1]]]]  Print each Deployment on a new line.\nCommand VI kubectl get deployment.v1.apps -o=jsonpath=\u0026#39;{range .items[*]}{@}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; Output map[kind:Deployment...readyReplicas:1]] map[kind:Deployment...readyReplicas:1]]  Literal Syntax On Windows, you must double quote any JSONPath template that contains spaces (not single quote as shown above for bash). This in turn means that you must use a single quote or escaped double quote around any literals in the template.\nFor example:\nC:\\\u0026gt; kubectl get pods -o=jsonpath=\u0026#34;{range .items[*]}{.metadata.name}{\u0026#39;\\t\u0026#39;}{.status.startTime}{\u0026#39;\\n\u0026#39;}{end}\u0026#34;   ","excerpt":"fields Print the fields from the JSON Path\nNote: JSON Path can also be read from a file using -o …","ref":"/references/kubectl/get/options/field/","title":"field"},{"body":"The get command is normally used to get the status of the existing Kubernetes resources. The output of the get command can be modified by using a number of options.\n","excerpt":"The get command is normally used to get the status of the existing Kubernetes resources. The output …","ref":"/references/kubectl/get/","title":"get"},{"body":"","excerpt":"","ref":"/guides/","title":"Guides"},{"body":"","excerpt":"","ref":"/installation/","title":"Installation"},{"body":" TL;DR  Apply manages Applications through files defining Kubernetes Resources (i.e. Resource Config) Kustomize is used to author Resource Config   Declarative Application Management This section covers how to declaratively manage Workloads and Applications.\nWorkloads in a cluster may be configured through files called Resource Config. These files are typically checked into source control, and allow cluster state changes to be reviewed before they are audited and applied.\nThere are 2 components to Application Management.\nClient Component The client component consists of authoring Resource Config which defines the desired state of an Application. This may be done as a collection of raw Resource Config files, or by composing and overlaying Resource Config authored by separate teams (using the -k flag with a kustomization.yaml).\nKustomize offers low-level tooling for simplifying the authoring of Resource Config. It provides:\n Generating Resource Config from other canonical sources - e.g. ConfigMaps, Secrets Reusing and Composing one or more collections of Resource Config Customizing Resource Config Setting cross-cutting fields - e.g. namespace, labels, annotations, name-prefixes, etc  Example: One user may define a Base for an application, while another user may customize a specific instance of the Base.\nServer Component The server component consists of a human applying the authored Resource Config to the cluster to create or update Resources. Once Applied, the Kubernetes cluster will set additional desired state on the Resource - e.g. defaulting unspecified fields, filling in IP addresses, autoscaling replica count, etc.\nNote that the process of Application Management is a collaborative one between users and the Kubernetes system itself - where each may contribute to defining the desired state.\nExample: An Autoscaler Controller in the cluster may set the scale field on a Deployment managed by a user.\n","excerpt":"TL;DR  Apply manages Applications through files defining Kubernetes Resources (i.e. Resource Config) …","ref":"/guides/config_management/introduction/","title":"Introduction"},{"body":"","excerpt":"","ref":"/guides/introduction/","title":"Introduction"},{"body":"Attend a sig-cli meeting The best way to get started is to attend sig-cli meetings. The bug scrub is a great place to pick up an issue to work on.\nChecking out the code Install golang Install the latest version of go\nGet a copy of the code Fork and clone the kubernetes repository\ngit clone git@github.com/USER/kubernetes cd kubernetes Build the binary Build the binary using go build\ncd cmd/kubectl go build -v ./kubectl version Edit the code The kubectl code is under staging/src/k8s.io/kubectl.\n Libraries are under staging/src/k8s.io/kubectl/pkg Command implementations are under staging/src/k8s.io/kubectl/pkg/cmd  Learning about libraries Kubectl uses a number of common libraries\n cobra \u0026ndash; a golang framework for CLIs client-go \u0026ndash; libraries for talking to the Kubernetes apiserver api \u0026ndash; Kubernetes types apimachinery \u0026ndash; Kubernetes apimachinery libraries  Additional resources  Everything You Always Wanted to Know About SIG-CLI but Were Afraid to Ask  ","excerpt":"Attend a sig-cli meeting The best way to get started is to attend sig-cli meetings. The bug scrub is …","ref":"/contributing/kubectl/","title":"Kubectl"},{"body":" TL;DR  Kubectl is the Kubernetes cli Kubectl provides a swiss army knife of functionality for working with Kubernetes clusters Kubectl may be used to deploy and manage applications on Kubernetes Kubectl may be used for scripting and building higher-level frameworks   Kubectl is the Kubernetes cli version of a swiss army knife, and can do many things.\nWhile this Book is focused on using Kubectl to declaratively manage Applications in Kubernetes, it also covers other Kubectl functions.\nCommand Families Most Kubectl commands typically fall into one of a few categories:\n   Type Used For Description     Declarative Resource Management Deployment and Operations (e.g. GitOps) Declaratively manage Kubernetes Workloads using Resource Config   Imperative Resource Management Development Only Run commands to manage Kubernetes Workloads using Command Line arguments and flags   Printing Workload State Debugging Print information about Workloads   Interacting with Containers Debugging Exec, Attach, Cp, Logs   Cluster Management Cluster Ops Drain and Cordon Nodes    Declarative Application Management The preferred approach for managing Resources is through declarative files called Resource Config used with the Kubectl Apply command. This command reads a local (or remote) file structure and modifies cluster state to reflect the declared intent.\nApply Apply is the preferred mechanism for managing Resources in a Kubernetes cluster.  Printing state about Workloads Users will need to view Workload state.\n Printing summarize state and information about Resources Printing complete state and information about Resources Printing specific fields from Resources Query Resources matching labels  Debugging Workloads Kubectl supports debugging by providing commands for:\n Printing Container logs Printing cluster events Exec or attaching to a Container Copying files from Containers in the cluster to a user\u0026rsquo;s filesystem  Cluster Management On occasion, users may need to perform operations to the Nodes of cluster. Kubectl supports commands to drain Workloads from a Node so that it can be decommission or debugged.\nPorcelain Users may find using Resource Config overly verbose for Development and prefer to work with the cluster imperatively with a shell-like workflow. Kubectl offers porcelain commands for generating and modifying Resources.\n Generating + creating Resources such as Deployments, StatefulSets, Services, ConfigMaps, etc Setting fields on Resources Editing (live) Resources in a text editor  Porcelain For Dev Only Porcelain commands are time saving for experimenting with workloads in a dev cluster, but shouldn\u0026rsquo;t be used for production.  ","excerpt":"TL;DR  Kubectl is the Kubernetes cli Kubectl provides a swiss army knife of functionality for …","ref":"/guides/introduction/kubectl/","title":"Kubectl"},{"body":"","excerpt":"","ref":"/installation/kubectl/","title":"Kubectl"},{"body":"","excerpt":"","ref":"/references/kubectl/","title":"Kubectl"},{"body":"Print a set of API resources generated from instructions in a kustomization.yaml file.\nThe argument must be the path to the directory containing the file, or a git repository URL with a path suffix specifying same with respect to the repository root.\nCommand $ kubectl kustomize \u0026lt;dir\u0026gt; Example Input File # deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:5template:containers:- name:the-containerimage:registry/conatiner:latest# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationnameSuffix:-devresources:- deployment.yamlCommand // deployment.yaml and kustomization.yaml are in the same directory $ kubectl kustomize Output apiVersion:apps/v1kind:Deploymentmetadata:name:the-deployment-devspec:replicas:5template:containers:- image:registry/conatiner:latestname:the-container","excerpt":"Print a set of API resources generated from instructions in a kustomization.yaml file.\nThe argument …","ref":"/references/kubectl/kustomize/","title":"kustomize"},{"body":" A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each. Optionally, the key can begin with a DNS subdomain prefix and a single \u0026lsquo;/\u0026rsquo;, like example.com/my-app If \u0026ndash;overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error. If \u0026ndash;resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used.  Command $ kubectl label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version] Example Current Status $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6db489d4b7-b5nsn 1/1 Running 0 16m nginx-6db489d4b7-vdhvz 1/1 Running 0 15m Command kubectl label pods nginx-6db489d4b7-b5nsn unhealthy=true Output $ kubectl describe pods nginx-6db489d4b7-b5nsn Name: nginx-6db489d4b7-b5nsn Namespace: default Priority: 0 Node: minikube/172.17.0.32 Start Time: Sun, 20 Sep 2020 15:09:09 +0000 Labels: pod-template-hash=6db489d4b7 run=nginx unhealthy=true Annotations: \u0026lt;none\u0026gt; Status: Running IP: 172.18.0.6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 15m default-scheduler Successfully assigned default/nginx-6db489d4b7-b5nsn to minikube Normal Pulling 15m kubelet, minikube Pulling image \u0026#34;nginx\u0026#34; Normal Pulled 14m kubelet, minikube Successfully pulled image \u0026#34;nginx\u0026#34; Normal Created 14m kubelet, minikube Created container nginx Normal Started 14m kubelet, minikube Started container nginx Notice that the labels has unhealthy=true as a last entry.\nMore Examples # Update pod \u0026#39;foo\u0026#39; with the label \u0026#39;unhealthy\u0026#39; and the value \u0026#39;true\u0026#39;. kubectl label pods foo unhealthy=true # Update pod \u0026#39;foo\u0026#39; with the label \u0026#39;status\u0026#39; and the value \u0026#39;unhealthy\u0026#39;, overwriting any existing value. kubectl label --overwrite pods foo status=unhealthy # Update all pods in the namespace kubectl label pods --all status=unhealthy # Update a pod identified by the type and name in \u0026#34;pod.json\u0026#34; kubectl label -f pod.json status=unhealthy # Update pod \u0026#39;foo\u0026#39; only if the resource is unchanged from version 1. kubectl label pods foo status=unhealthy --resource-version=1 # Update pod \u0026#39;foo\u0026#39; by removing a label named \u0026#39;bar\u0026#39; if it exists. # Does not require the --overwrite flag. kubectl label pods foo bar- ","excerpt":"A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, …","ref":"/references/kubectl/label/","title":"label"},{"body":"Print the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.\nCommand $ kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER] Example Current State $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 13m Command $ kubectl logs deployment/nginx Output /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Configuration complete; ready for start up Print Logs for a Container in a Pod Print the logs for a Pod running a single Container\nkubectl logs echo-c6bc8ccff-nnj52 hello hello  Crash Looping Containers If a container is crash looping and you want to print its logs after it exits, use the -p flag to look at the logs from containers that have exited. e.g. kubectl logs -p -c ruby web-1   Print Logs for all Pods for a Workload Print the logs for all Pods for a Workload\n# Print logs from all containers matching label kubectl logs -l app=nginx  Workloads Logs Print all logs from all containers for a Workload by passing the Workload label selector to the -l flag. e.g. if your Workload label selector is app=nginx usie -l \u0026quot;app=nginx\u0026quot; to print logs for all the Pods from that Workload.   Follow Logs for a Container Stream logs from a container.\n# Follow logs from container kubectl logs nginx-78f5d695bd-czm8z -f  Printing Logs for a Container that has exited Print the logs for the previously running container. This is useful for printing containers that have crashed or are crash looping.\n# Print logs from exited container kubectl logs nginx-78f5d695bd-czm8z -p  Selecting a Container in a Pod Print the logs from a specific container within a Pod. This is necessary for Pods running multiple containers.\n# Print logs from the nginx container in the nginx-78f5d695bd-czm8z Pod kubectl logs nginx-78f5d695bd-czm8z -c nginx  Printing Logs After a Time Print the logs that occurred after an absolute time.\n# Print logs since a date kubectl logs nginx-78f5d695bd-czm8z --since-time=2018-11-01T15:00:00Z  Printing Logs Since a Time Print the logs that are newer than a duration.\nExamples:\n 0s: 0 seconds 1m: 1 minute 2h: 2 hours  # Print logs for the past hour kubectl logs nginx-78f5d695bd-czm8z --since=1h  Include Timestamps Include timestamps in the log lines\n# Print logs with timestamps kubectl logs -l app=echo --timestamps 2018-11-16T05:26:31.38898405Z hello 2018-11-16T05:27:13.363932497Z hello ","excerpt":"Print the logs for a container in a pod or specified resource. If the pod has only one container, …","ref":"/references/kubectl/logs/","title":"logs"},{"body":"kustomize encourages defining multiple variants - e.g. dev, staging and prod, as overlays on a common base.\nIt\u0026rsquo;s possible to create an additional overlay to compose these variants together - just declare the overlays as the bases of a new kustomization.\nThis is also a means to apply a common label or annotation across the variants, if for some reason the base isn\u0026rsquo;t under your control. It also allows one to define a left-most namePrefix across the variants - something that cannot be done by modifying the common base.\nThe following demonstrates this using a base that is just a single pod.\nDefine a place to work:\nDEMO_HOME = $(mktemp -d) /base Define a common base:\n$ cd $DEMO_HOME $ mkdir base $ cd base Create a Sample Pod File and Kustomize file in base\n$ vim kustomization.yaml # kustomization.yaml contentsresources:- pod.yaml# pod.yaml contentsapiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappspec:containers:- name:nginximage:nginx:latest/dev Define a dev variant overlaying base:\n$ cd $DEMO_HOME $ mkdir dev $ cd dev Create a Kustomize file in dev\n# kustomization.yaml contentsresources:- ./../basenamePrefix:dev-/staging Define a staging variant overlaying base:\n$ cd $DEMO_HOME $ mkdir staging $ cd staging Create a Kustomize file in staging\n# kustomization.yaml contentsresources:- ./../basenamePrefix:stag-/production Define a production variant overlaying base:\n$ cd $DEMO_HOME $ mkdir production $ cd production Create a Kustomize file in production\n# kustomization.yaml contentsresources:- ./../basenamePrefix:prod-kustomize @ root dir Then define a Kustomization composing three variants together:\n# kustomization.yaml contentsresources:- ./dev- ./staging- ./productionnamePrefix:cluster-a-directory sturcture  . ├── kustomization.yaml ├── base │ ├── kustomization.yaml │ └── pod.yaml ├── dev │ └── kustomization.yaml ├── production │ └── kustomization.yaml └── staging └── kustomization.yaml  Confirm that the kustomize build output contains three pod objects from dev, staging and production variants.\noutput apiVersion:v1kind:Podmetadata:labels:app:myappname:cluster-a-dev-myapp-podspec:containers:- image:nginx:latestname:nginx---apiVersion:v1kind:Podmetadata:labels:app:myappname:cluster-a-prod-myapp-podspec:containers:- image:nginx:latestname:nginx---apiVersion:v1kind:Podmetadata:labels:app:myappname:cluster-a-stag-myapp-podspec:containers:- image:nginx:latestname:nginxSimilarly to adding different namePrefix in different variants, one can also add different namespace and compose those variants in one kustomization. For more details, take a look at multi-namespaces.\n","excerpt":"kustomize encourages defining multiple variants - e.g. dev, staging and prod, as overlays on a …","ref":"/guides/example/multi_base/","title":"Multibase"},{"body":"Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces.\nCommand $ kubectl create namespace NAME [--dry-run=server|client|none] Example Command $ kubectl create namespace my-namespace Output $ kubectl get namespace NAME STATUS AGE default Active 41s my-namespace Active 11s ","excerpt":"Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual …","ref":"/references/kubectl/create/namespace/","title":"namespace"},{"body":"Update field(s) of a resource using strategic merge patch, a JSON merge patch, or a JSON patch.\nJSON and YAML formats are accepted.\nCommand $ kubectl patch (-f FILENAME | TYPE NAME) -p PATCH Example Current State $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nginx 2/2 2 2 24m Command kubectl patch deployment nginx -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:1}}\u0026#39; deployment.apps/nginx patched This will reduce the number of replicas for nginx from 2 to 1.\nOutput $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 26m More Examples # Partially update a node using a strategic merge patch. Specify the patch as JSON. kubectl patch node k8s-node-1 -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;unschedulable\u0026#34;:true}}\u0026#39; # Partially update a node using a strategic merge patch. Specify the patch as YAML. kubectl patch node k8s-node-1 -p $\u0026#39;spec:\\n unschedulable: true\u0026#39; # Partially update a node identified by the type and name specified in \u0026#34;node.json\u0026#34; using strategic merge patch. kubectl patch -f node.json -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;unschedulable\u0026#34;:true}}\u0026#39; # Update a container\u0026#39;s image; spec.containers[*].name is required because it\u0026#39;s a merge key. kubectl patch pod valid-pod -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;kubernetes-serve-hostname\u0026#34;,\u0026#34;image\u0026#34;:\u0026#34;new image\u0026#34;}]}}\u0026#39; # Update a container\u0026#39;s image using a json patch with positional arrays. kubectl patch pod valid-pod --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;newimage\u0026#34;}]\u0026#39; ","excerpt":"Update field(s) of a resource using strategic merge patch, a JSON merge patch, or a JSON patch.\nJSON …","ref":"/references/kubectl/patch/","title":"patch"},{"body":" TL;DR  Drop executables named kubectl-plugin_name on your PATH and invoke with kubectl plugin-name kubectl plugin list shows available plugins   Kubectl plugins Kubectl plugins are a lightweight mechanism to extend kubectl with custom functionality to suit your needs.\nPlugin mechanism As of version 1.12, kubectl has a simple plugin mechanism to expose binaries on your PATH as kubectl subcommands. When invoking an unknown subcommand kubectl my-plugin, kubectl starts searching for an executable named kubectl-my_plugin on your PATH. Note how the dash is mapped to an underscore. This is to enable plugins that are invoked by multiple words, for example kubectl my plugin would trigger a search for the commands kubectl-my-plugin or kubectl-my. The more specific match always wins over the other, so if both kubectl-my and kubectl-my-plugin exist, the latter will be called. When a matching executable is found, kubectl calls it, forwarding all extra arguments.\nThe reference on kubernetes.io knows more.\nWindows compatibility On windows, the minimum required version to use the plugin mechanism is 1.14.  Listing installed plugins\nkubectl plugin list ","excerpt":"TL;DR  Drop executables named kubectl-plugin_name on your PATH and invoke with kubectl plugin-name …","ref":"/guides/extending_kubectl/plugin_mechanism/","title":"Plugin Mechanism"},{"body":"Forward one or more local ports to a pod. This command requires the node to have \u0026lsquo;socat\u0026rsquo; installed.\nUse resource type/name such as deployment/mydeployment to select a pod. Resource type defaults to \u0026lsquo;pod\u0026rsquo; if omitted.\nIf there are multiple pods matching the criteria, a pod will be selected automatically. The forwarding session ends when the selected pod terminates, and rerun of the command is needed to resume forwarding.\nCommand $ kubectl port-forward TYPE/NAME [options] [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N] Forward Multiple Ports Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in the pod\nkubectl port-forward pod/mypod 5000 6000  Pod in a Workload Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in a pod selected by the deployment\nkubectl port-forward deployment/mydeployment 5000 6000  Different Local and Remote Ports Listen on port 8888 locally, forwarding to 5000 in the pod\nkubectl port-forward pod/mypod 8888:5000  Random Local Port Listen on a random port locally, forwarding to 5000 in the pod\nkubectl port-forward pod/mypod :5000 ","excerpt":"Forward one or more local ports to a pod. This command requires the node to have \u0026lsquo;socat\u0026rsquo; …","ref":"/references/kubectl/port-forward/","title":"port-forward"},{"body":"Creates a proxy server or application-level gateway between localhost and the Kubernetes API Server. It also allows serving static content over specified HTTP path. All incoming data enters through one port and gets forwarded to the remote kubernetes API Server port, except for the path matching the static content path.\nNot all Services running a Kubernetes cluster are exposed externally. However Services only exposed internally to a cluster with a clusterIp are accessible through an apiserver proxy.\nUsers may use Proxy to connect to Kubernetes Services in a cluster that are not externally exposed.\nNote: Services running a type LoadBalancer or type NodePort may be exposed externally and accessed without the need for a Proxy.\nCommand $ kubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix] Connecting to an internal Service Connect to a internal Service using the Proxy command, and the Service Proxy url.\nTo visit the nginx service go to the Proxy URL at http://127.0.0.1:8001/api/v1/namespaces/default/services/nginx/proxy/\nkubectl proxy Starting to serve on 127.0.0.1:8001 curl http://127.0.0.1:8001/api/v1/namespaces/default/services/nginx/proxy/  Literal Syntax To connect to a Service through a proxy the user must build the Proxy URL. The Proxy URL format is:\nhttp://\u0026lt;apiserver-address\u0026gt;/api/v1/namespaces/\u0026lt;service-namespace\u0026gt;/services/[https:]\u0026lt;service-name\u0026gt;[:\u0026lt;port-name\u0026gt;]/proxy\n The apiserver-address should be the URL printed by the Proxy command The Port is optional if you haven’t specified a name for your port The Protocol is optional if you are using http   Builtin Cluster Services A common usecase is to connect to Services running as part of the cluster itself. A user can print out these Services and their Proxy Urls with kubectl cluster-info.\nkubectl cluster-info Kubernetes master is running at https://104.197.5.247 GLBCDefaultBackend is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy Heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/heapster/proxy KubeDNS is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy  More Info For more information on connecting to a cluster, see Accessing Clusters.  ","excerpt":"Creates a proxy server or application-level gateway between localhost and the Kubernetes API Server. …","ref":"/references/kubectl/proxy/","title":"proxy"},{"body":"","excerpt":"","ref":"/references/","title":"Reference"},{"body":"Auth Resources ClusterRole Create a ClusterRole named \u0026ldquo;foo\u0026rdquo; with API Group specified.\nkubectl create clusterrole foo --verb=get,list,watch --resource=rs.extensions ClusterRoleBinding Create a role binding to give a user cluster admin permissions.\nkubectl create clusterrolebinding \u0026lt;choose-a-name\u0026gt; --clusterrole=cluster-admin --user=\u0026lt;your-cloud-email-account\u0026gt;  Required Admin Permissions The cluster-admin role maybe required for creating new RBAC bindings.  Role Create a Role named \u0026ldquo;foo\u0026rdquo; with API Group specified.\nkubectl create role foo --verb=get,list,watch --resource=rs.extensions RoleBinding Create a RoleBinding for user1, user2, and group1 using the admin ClusterRole.\nkubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1 ServiceAccount Create a new service account named my-service-account\nkubectl create serviceaccount my-service-account ","excerpt":"Auth Resources ClusterRole Create a ClusterRole named \u0026ldquo;foo\u0026rdquo; with API Group specified. …","ref":"/references/kubectl/create/auth_resources/","title":"roles"},{"body":"Create and run a particular image in a pod. Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.\nImportant run command is deprecated  Command $ kubectl run NAME --image=image [--env=\u0026#34;key=value\u0026#34;] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...] Example Command kubectl run nginx --image=nginx Output $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6db489d4b7-tsfhq 1/1 Running 0 28s $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 44s ","excerpt":"Create and run a particular image in a pod. Pods are the smallest deployable units of computing that …","ref":"/references/kubectl/run/","title":"run"},{"body":"Set a new size for a Deployment, ReplicaSet, Replication Controller, or StatefulSet.\nScale also allows users to specify one or more preconditions for the scale action.\nIf \u0026ndash;current-replicas or \u0026ndash;resource-version is specified, it is validated before the scale is attempted, and it is guaranteed that the precondition holds true when the scale is sent to the server.\nCommand $ kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME) Example Current State $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 24s Command $ kubectl scale --replicas=3 deployment/nginx deployment.apps/nginx scaled New State $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nginx 3/3 3 3 87s More Examples # Scale a replicaset named \u0026#39;foo\u0026#39; to 3. kubectl scale --replicas=3 rs/foo # Scale a resource identified by type and name specified in \u0026#34;foo.yaml\u0026#34; to 3. kubectl scale --replicas=3 -f foo.yaml # If the deployment named mysql\u0026#39;s current size is 2, scale mysql to 3. kubectl scale --current-replicas=2 --replicas=3 deployment/mysql # Scale multiple replication controllers. kubectl scale --replicas=5 rc/foo rc/bar rc/baz # Scale statefulset named \u0026#39;web\u0026#39; to 3. kubectl scale --replicas=3 statefulset/web  Conditional Scale Update It is possible to conditionally update the replicas if and only if the replicas haven\u0026rsquo;t changed from their last known value using the --current-replicas flag. e.g. kubectl scale --current-replicas=2 --replicas=3 deployment/mysql  ","excerpt":"Set a new size for a Deployment, ReplicaSet, Replication Controller, or StatefulSet.\nScale also …","ref":"/references/kubectl/scale/","title":"scale"},{"body":"Kubernetes Secrets let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys. Storing confidential information in a Secret is safer and more flexible than putting it verbatim in a Pod definition or in a container image.\nNote Secrets can be created by using any one of the subcommands depending on use case.\n docker-registry generic tls   docker-registry  Create a secret for use with a Docker registry  kubectl create secret docker-registry NAME --docker-username=user --docker-password=password --docker-email=email [--docker-server=string] [--from-literal=key1=value1] [--dry-run=server|client|none] Example Command kubectl create secret docker-registry my-secret --docker-username=kubectluser --docker-password=somepassword --docker-email=kubectl@kubectl.com --from-literal=token=GGH132YYu8asbbAA Output $ kubectl get secrets NAME TYPE DATA AGE my-secret Opaque 1 14s generic  Create a secret from a local file, directory or literal value  $ kubectl create generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none] Example Input File // file-name: simplesecret.txt kjbfkadbfkabjnaAdjna Command kubectl create secret generic my-secret --from-file=simplesecret.txt Output $ kubectl get secrets NAME TYPE DATA AGE my-secret Opaque 1 14s tls  Create a secret from tls certificate and key  $ kubectl create secret tls NAME --cert=path/to/cert/file --key=path/to/key/file [--dry-run=server|client|none] Example Input File # tls.certLS0tLS1CRUd...tCg==# tls.keyLS0tLS1CRUd...0tLQo=Command kubectl create secret tls my-secret --cert=tls.cert --ket=tls.key Output $ kubectl get secrets NAME TYPE DATA AGE my-secret Opaque 1 14s ","excerpt":"Kubernetes Secrets let you store and manage sensitive information, such as passwords, OAuth tokens, …","ref":"/references/kubectl/create/secret/","title":"secrets"},{"body":" TL;DR  Get a Summary of Resources Running in the Cluster   Summarizing Resources Motivation Quickly summarizing a collection of Resources and their state.\nSummarizing Resource State using a columnar format is the most common way to view cluster state when developing applications or triaging issues. The columnar view gives a compact summary of the most relevant information for a collection of Resources.\nGet The kubectl get reads Resources from the cluster and formats them as output. The examples in this chapter will query for Resources by providing Get the Resource Type as an argument. For more query options see Queries and Options.\nDefault If no output format is specified, Get will print a default set of columns.\nNote: Some columns may not directly map to fields on the Resource, but instead may be a summary of fields.\nkubectl get deployments nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 1 1 1 0 5s  You can also use a number of options to query out and format the way the resources are displayed. Some of the commonly used options include:\n wide custom columns labels show labels show kind  Command / Examples Check out the reference for commands and examples for get with / without options  ","excerpt":"TL;DR  Get a Summary of Resources Running in the Cluster   Summarizing Resources Motivation Quickly …","ref":"/guides/resource_printing/summary/","title":"Summaries"},{"body":"Print the default columns plus some additional columns.\nNote: Some columns may not directly map to fields on the Resource, but instead may be a summary of fields.\nCommand kubectl get -o=wide deployments nginx Output NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx 1 1 1 1 26s nginx nginx app=nginx ","excerpt":"Print the default columns plus some additional columns.\nNote: Some columns may not directly map to …","ref":"/references/kubectl/get/options/wide/","title":"wide"},{"body":"内置插件包括生成器和转化器。\n每个插件都可以通过如下两种方式触发：\n  通过 kustomization 文件的字段隐式触发插件，例如 AnnotationsTransformer 就是由 commonAnnotations 字段触发的。\n  通过 generators 或 transformers 字段显式触发插件（通过指定插件的配置文件）。\n  直接使用 kustomization.yaml 文件中的字段比如 commonLables、commonAnnotation 他们可以修改一些默认的字段，如果用户想要添加或减少能被 commonLabel 所修改的字段，则不是很容易做到；而使用 transformers 的话，用户可以指定哪些字段能被修改，而不受默认值的影响。\nAnnotationTransformer 使用 kustomization.yaml 字段名称：commonAnnotations 为所有资源添加注释，和标签一样以 key: value 的形式。\ncommonAnnotations:oncallPager:800-555-1212使用插件 Arguments  Annotations map[string]string\nFieldSpecs []config.FieldSpec\n Example  apiVersion:builtinkind:AnnotationsTransformermetadata:name:not-important-to-exampleannotations:app:myAppgreeting/morning:a string with blanksfieldSpecs:- path:metadata/annotationscreate:true ConfigMapGenerator 使用 kustomization.yaml 字段名称：configMapGenerator 列表中的每个条目都将生成一个 ConfigMap （合计可以生成 n 个 ConfigMap）。\n下面的示例会生成 3 个ConfigMap：第一个带有给定文件的名称和内容，第二个将在 data 中添加 key/value，第三个通过 options 为单个 ConfigMap 设置注释和标签。\n每个 configMapGenerator 项均接受的参数 behavior: [create|replace|merge]，这个参数允许修改或替换父级现有的 configMap。\n此外，每个条目都有一个 options 字段，该字段具有与 kustomization 文件的 generatorOptions 字段相同的子字段。\noptions 字段允许用户为生成的实例添加标签和（或）注释，或者分别禁用该实例名称的哈希后缀。此处添加的标签和注释不会被 kustomization 文件 generatorOptions 字段关联的全局选项覆盖。但是如果全局 generatorOptions 字段指定 disableNameSuffixHash: true，其他 options 的设置将无法将其覆盖。\n# These labels are added to all configmaps and secrets.generatorOptions:labels:fruit:appleconfigMapGenerator:- name:my-java-server-propsbehavior:mergefiles:- application.properties- more.properties- name:my-java-server-env-varsliterals:- JAVA_HOME=/opt/java/jdk- JAVA_TOOL_OPTIONS=-agentlib:hprofoptions:disableNameSuffixHash:truelabels:pet:dog- name:dashboardsfiles:- mydashboard.jsonoptions:annotations:dashboard:\u0026#34;1\u0026#34;labels:app.kubernetes.io/name:\u0026#34;app1\u0026#34;这里也可以定义一个 key 来为文件设置不同名称。\n下面这个示例会创建一个 ConfigMap，并将 whatever.ini 重命名为 myFileName.ini：\nconfigMapGenerator:- name:app-whateverfiles:- myFileName.ini=whatever.ini使用插件 Arguments  types.ConfigMapArgs\n Example  apiVersion:builtinkind:ConfigMapGeneratormetadata:name:mymapenvs:- devops.env- uxteam.envliterals:- FRUIT=apple- VEGETABLE=carrot ImageTagTransformer 使用 kustomization.yaml 字段名称：images 修改镜像的名称、tag 或 image digest ，而无需使用 patches 。例如，对于这种 kubernetes Deployment 片段：\ncontainers:- name:mypostgresdbimage:postgres:8- name:nginxappimage:nginx:1.7.9- name:myappimage:my-demo-app:latest- name:alpine-appimage:alpine:3.7想要将 image 做如下更改：\n 将 postgres:8 改为 my-registry/my-postgres:v1 将 nginx tag 从 1.7.9 改为 1.8.0 将镜像名称 my-demo-app 改为 my-app 将 alpine 的 tag 3.7 改为 digest 值  只需在 kustomization 中添加以下内容：\nimages:- name:postgresnewName:my-registry/my-postgresnewTag:v1- name:nginxnewTag:1.8.0- name:my-demo-appnewName:my-app- name:alpinedigest:sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3使用插件 Arguments  ImageTag image.Image\nFieldSpecs []config.FieldSpec\n Example  apiVersion:builtinkind:ImageTagTransformermetadata:name:not-important-to-exampleimageTag:name:nginxnewTag:v2 LabelTransformer 使用 kustomization.yaml 字段名称：commonLabels 为所有资源和 selectors 增加标签。\ncommonLabels:someName:someValueowner:aliceapp:bingo使用插件 Arguments  Labels map[string]string\nFieldSpecs []config.FieldSpec\n Example  apiVersion:builtinkind:LabelTransformermetadata:name:not-important-to-examplelabels:app:myAppenv:productionfieldSpecs:- path:metadata/labelscreate:true NamespaceTransformer 使用 kustomization.yaml 字段名称：namespace 为所有资源添加 namespace。\nnamespace:my-namespace使用插件 Arguments  types.ObjectMeta\nFieldSpecs []config.FieldSpec\n Example  apiVersion:builtinkind:NamespaceTransformermetadata:name:not-important-to-examplenamespace:testfieldSpecs:- path:metadata/namespacecreate:true- path:subjectskind:RoleBindinggroup:rbac.authorization.k8s.io- path:subjectskind:ClusterRoleBindinggroup:rbac.authorization.k8s.io PatchesJson6902 使用 kustomization.yaml 字段名称：patchesJson6902 patchesJson6902 列表中的每个条目都应可以解析为 kubernetes 对象和将应用于该对象的 JSON patch。\n目标字段指向的 kubernetes 对象的 group、 version、 kind、 name 和 namespace 在同一 kustomization 内 path 字段内容是 JSON patch 文件的相对路径。\npatch 文件中的内容可以如下这种 JSON 格式：\n[ {\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/some/new/path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;value\u0026#34;}, {\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/some/existing/path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;new value\u0026#34;} ] 也可以使用 YAML 格式表示：\n- op:addpath:/some/new/pathvalue:value- op:replacepath:/some/existing/pathvalue:new valuepatchesJson6902:- target:version:v1kind:Deploymentname:my-deploymentpath:add_init_container.yaml- target:version:v1kind:Servicename:my-servicepath:add_service_annotation.yamlpatch 内容也可以是一个inline string：\npatchesJson6902:- target:version:v1kind:Deploymentname:my-deploymentpatch:|-- op: add path: /some/new/path value: value - op: replace path: /some/existing/path value: \u0026#34;new value\u0026#34;使用插件 Arguments  Target types.PatchTarget\nPath string\nJsonOp string\n Example  apiVersion:builtinkind:PatchJson6902Transformermetadata:name:not-important-to-exampletarget:group:appsversion:v1kind:Deploymentname:my-deploypath:jsonpatch.json PatchesStrategicMerge 使用 kustomization.yaml 字段名称：patchesStrategicMerge 此列表中的每个条目都应可以解析为 StrategicMergePatch.\n这些（也可能是部分的）资源文件中的 name 必须与已经通过 resources 加载的 name 字段匹配，或者通过 bases 中的 name 字段匹配。这些条目将用于 patch（修改）已知资源。\n推荐使用小的 patches，例如：修改内存的 request/limit，更改 ConfigMap 中的 env 变量等。小的 patches 易于维护和查看，并且易于在 overlays 中混合使用。\npatchesStrategicMerge:- service_port_8888.yaml- deployment_increase_replicas.yaml- deployment_increase_memory.yamlpatch 内容也可以是一个inline string：\npatchesStrategicMerge:- |-apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: template: spec: containers: - name: nginx image: nignx:latest请注意，kustomize 不支持同一个 patch 对象中包含多个 删除 指令。要从一个对象中删除多个字段或切片元素，需要创建一个单独的 patch，以执行所有需要的删除。\n使用插件 Arguments  Paths []types.PatchStrategicMerge\nPatches string\n Example  apiVersion:builtinkind:PatchStrategicMergeTransformermetadata:name:not-important-to-examplepaths:- patch.yaml PatchTransformer 使用 kustomization.yaml 字段名称：patches 这个列表中的每个条目应该解析到一个 Patch 对象，其中包括一个 patch 和一个目标选择器。patch 可以是 Strategic Merge Patch 或 JSON patch，也可以是 patch 文件或 inline string。目标选择器可以通过 group、version、kind、name、namespace、标签选择器和注释选择器来选择资源，选择一个或多个匹配所有指定字段的资源来应用 patch。\npatches:- path:patch.yamltarget:group:appsversion:v1kind:Deploymentname:deploy.*labelSelector:\u0026#34;env=dev\u0026#34;annotationSelector:\u0026#34;zone=west\u0026#34;- patch:|-- op: replace path: /some/existing/path value: new valuetarget:kind:MyKindlabelSelector:\u0026#34;env=dev\u0026#34;The name and namespace fields of the patch target selector are automatically anchored regular expressions. This means that the value myapp is equivalent to ^myapp$。\n使用插件 Arguments  Path string\nPatch string\nTarget *types.Selector\n Example  apiVersion:builtinkind:PatchTransformermetadata:name:not-important-to-examplepatch:\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;nginx:latest\u0026#34;}]\u0026#39;target:name:.*Deploykind:Deployment PrefixSuffixTransformer 使用 kustomization.yaml 字段名称：namePrefix, nameSuffix 为所有资源的名称添加前缀或后缀。\n例如：将 deployment 名称从 wordpress 变为 alices-wordpress 或 wordpress-v2 或 alices-wordpress-v2。\nnamePrefix:alices-nameSuffix:-v2如果资源类型是 ConfigMap 或 Secret，则在哈希值之前添加后缀。\n使用插件 Arguments  Prefix string\nSuffix string\nFieldSpecs []config.FieldSpec\n Example  apiVersion:builtinkind:PrefixSuffixTransformermetadata:name:not-important-to-exampleprefix:baked-suffix:-piefieldSpecs:- path:metadata/name ReplicaCountTransformer 使用 kustomization.yaml 字段名称：replicas 修改资源的副本数。\n例如：对于如下 kubernetes Deployment 片段：\nkind:Deploymentmetadata:name:deployment-namespec:replicas:3在 kustomization 中添加以下内容，将副本数更改为 5：\nreplicas:- name:deployment-namecount:5该字段内容为列表，所以可以同时修改许多资源。\n由于这个声明无法设置 kind: 或 group:，所以他只能匹配如下资源中的一种：\n Deployment ReplicationController ReplicaSet StatefulSet  对于更复杂的用例，请使用 patch 。\n使用插件 Arguments  Replica types.Replica\nFieldSpecs []config.FieldSpec\n Example  apiVersion:builtinkind:ReplicaCountTransformermetadata:name:not-important-to-examplereplica:name:myappcount:23fieldSpecs:- path:spec/replicascreate:truekind:Deployment- path:spec/replicascreate:truekind:ReplicationController SecretGenerator 使用 kustomization.yaml 字段名称：secretGenerator 列表中的每个条目都将生成一个 Secret（合计可以生成 n 个 Secrets）。\n功能与之前描述的 configMapGenerator 字段类似。\nsecretGenerator:- name:app-tlsfiles:- secret/tls.cert- secret/tls.keytype:\u0026#34;kubernetes.io/tls\u0026#34;- name:app-tls-namespaced# you can define a namespace to generate# a secret in, defaults to: \u0026#34;default\u0026#34;namespace:appsfiles:- tls.crt=catsecret/tls.cert- tls.key=secret/tls.keytype:\u0026#34;kubernetes.io/tls\u0026#34;- name:env_file_secretenvs:- env.txttype:Opaque- name:secret-with-annotationfiles:- app-config.yamltype:Opaqueoptions:annotations:app_config:\u0026#34;true\u0026#34;labels:app.kubernetes.io/name:\u0026#34;app2\u0026#34;使用插件 Arguments  types.ObjectMeta\ntypes.SecretArgs\n Example  apiVersion:builtinkind:SecretGeneratormetadata:name:my-secretnamespace:whateverbehavior:mergeenvs:- a.env- b.envfiles:- obscure=longsecret.txtliterals:- FRUIT=apple- VEGETABLE=carrot ","excerpt":"内置插件包括生成器和转化器。\n每个插件都可以通过如下两种方式触发：\n  通过 kustomization 文件的字段隐式触发插件，例如 AnnotationsTransformer …","ref":"/zh/guides/plugins/builtins/","title":"内置插件"},{"body":"在这个工作流方式中，所有的配置文件（ YAML 资源）都为用户所有，存储在用户的私有 repo 中。其他用户是无法使用的。\n1) 创建一个目录用于版本控制 我们希望将一个名为 ldap 的 Kubernetes 集群应用的配置保存在自己的 repo 中。 这里使用 git 进行版本控制。\n git init ~/ldap  2) 创建一个 base  mkdir -p ~/ldap/base  在这个目录中创建并提交 kustomization 文件及一组资源 resources 配置。\n3) 创建 overlays  mkdir -p ~/ldap/overlays/staging mkdir -p ~/ldap/overlays/production  每个目录都包含需要一个 kustomization 文件以及一或多个 patches。\n在 staging 目录可能会有一个用于在 configmap 中打开一个实验标记的补丁。\n在 production 目录可能会有一个在 deployment 中增加副本数的补丁。\n4) 生成 variants 运行 kustomize，将生成的配置用于 kubernetes 应用发布。\n kustomize build ~/ldap/overlays/staging | kubectl apply -f - kustomize build ~/ldap/overlays/production | kubectl apply -f -  也可以在 kubectl-v1.14.0 版，使用 kubectl 命令发布你的 variants 。\n kubectl apply -k ~/ldap/overlays/staging kubectl apply -k ~/ldap/overlays/production  ","excerpt":"在这个工作流方式中，所有的配置文件（ YAML 资源）都为用户所有，存储在用户的私有 repo 中。其他用户是无法使用的。\n1) 创建一个目录用于版本控制 我们希望将一个名为 ldap …","ref":"/zh/guides/bespoke/","title":"配置定制（Bespoke configuration）"},{"body":" TL;DR  Target a cluster for a rollout with the --context flag Target a cluster for a rollout with the --kubeconfig flag   Multi-Cluster Targeting Motivation It is common for users to need to deploy different Variants of an Application to different clusters. This can be done by configuring the different Variants using different kustomization.yaml\u0026rsquo;s, and targeting each variant using the --context or --kubeconfig flag.\nNote: The examples shown in this chapter store the Resource Config in a directory matching the name of the cluster (i.e. as it is referred to be context).\nTargeting a Cluster via Context The kubeconfig file allows multiple contexts to be specified, each with a different cluster + auth.\nList Contexts List the contexts in the kubeconfig file\nkubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE us-central1-c us-central1-c us-central1-c * us-east1-c us-east1-c us-east1-c us-west2-c us-west2-c us-west2-c Print a Context Print information about the current context\nkubectl config --kubeconfig=config-demo view --minify apiVersion:v1clusters:- cluster:certificate-authority:fake-ca-fileserver:https://1.2.3.4name:developmentcontexts:- context:cluster:developmentnamespace:frontenduser:developername:dev-frontendcurrent-context:dev-frontendkind:Configpreferences:{}users:- name:developeruser:client-certificate:fake-cert-fileclient-key:fake-key-fileSpecify a Context Flag Specify the kubeconfig context as part of the command.\nNote: In this example the kustomization.yaml exists in a directory whose name matches the name of the context.\nexport CLUSTER=us-west2-c; kubectl apply -k ${CLUSTER} --context=${CLUSTER} Switch to use a Context Switch the current context before running the command.\nNote: In this example the kustomization.yaml exists in a directory whose name matches the name of the context.\n# change the context to us-west2-c kubectl config use-context us-west2-c # deploy Resources from the ./us-west2-c/kustomization.yaml kubectl apply -k ./us-west2-c Targeting a Cluster via Kubeconfig Alternatively, different kubeconfig files may be used for different clusters. The kubeconfig may be specified with the --kubeconfig flag.\nNote: In this example the kustomization.yaml exists in a directory whose name matches the name of the directory containing the kubeconfig.\nkubectl apply -k ./us-west2-c --kubeconfig /path/to/us-west2-c/config  More Info For more information on configuring kubeconfig and contexts, see the Configure Access to Multiple Clusters k8s.io document.  ","excerpt":"TL;DR  Target a cluster for a rollout with the --context flag Target a cluster for a rollout with …","ref":"/guides/app_deployment/accessing_multiple_clusters/","title":"Accessing Multiple Clusters"},{"body":" TL;DR  Apply Creates and Updates Resources in a cluster through running kubectl apply on Resource Config. Apply manages complexity such as ordering of operations and merging user defined and cluster defined state.   Apply Motivation Apply is a command that will update a Kubernetes cluster to match state defined locally in files.\nkubectl apply  Fully declarative - don\u0026rsquo;t need to specify create or update - just manage files Merges user owned state (e.g. Service selector) with state owned by the cluster (e.g. Service clusterIp)  Definitions  Resources: Objects in a cluster - e.g. Deployments, Services, etc. Resource Config: Files declaring the desired state for Resources - e.g. deployment.yaml. Resources are created and updated using Apply with these files.  kubectl apply Creates and Updates Resources through local or remote files. This may be through either raw Resource Config or kustomization.yaml.\nUsage Though Apply can be run directly against Resource Config files or directories using -f, it is recommended to run Apply against a kustomization.yaml using -k. The kustomization.yaml allows users to define configuration that cuts across many Resources (e.g. namespace).\nCommand / Examples Check out the reference for commands and examples.  Users run Apply on directories containing kustomization.yaml files using -k or on raw ResourceConfig files using -f.\nMulti-Resource Configs A single Resource Config file may declare multiple Resources separated by \\n---\\n.  CRUD Operations Creating Resources Any Resources that do not exist and are declared in Resource Config when Apply is run will be Created.\nUpdating Resources Any Resources that already exist and are declared in Resource Config when Apply is run may be Updated.\nAdded Fields\nAny fields that have been added to the Resource Config will be set on the Resource.\nUpdated Fields\nAny fields that contain different values for the fields specified locally in the Resource Config from what is in the Resource will be updated by merging the Resource Config into the live Resource. See merging for more details.\nDeleted Fields\nFields that were in the Resource Config the last time Apply was run, will be deleted from the Resource, and return to their default values.\nUnmanaged Fields\nFields that were not specified in the Resource Config but are set on the Resource will be left unmodified.\nDeleting Resources Declarative deletion of Resources does not yet exist in a usable form, but is under development.\nContinuously Applying The Hard Way In some cases, it may be useful to automatically Apply changes when ever the Resource Config is changed.\nThis example uses the unix watch command to periodically invoke Apply against a target. watch -n 60 kubectl apply -k https://github.com/myorg/myrepo\n Resource Creation Ordering Certain Resource Types may be dependent on other Resource Types being created first. e.g. Namespaced Resources on the Namespaces, RoleBindings on Roles, CustomResources on the CRDs, etc.\nWhen used with a kustomization.yaml, Apply sorts the Resources by Resource type to ensure Resources with these dependencies are created in the correct order.\n","excerpt":"TL;DR  Apply Creates and Updates Resources in a cluster through running kubectl apply on Resource …","ref":"/guides/config_management/apply/","title":"Apply"},{"body":"Add annotations to all resources. If the annotation key already is present on the resource, the value will be overridden.\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationcommonAnnotations:oncallPager:800-555-1212Example File Input # kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationcommonAnnotations:oncallPager:800-555-1212resources:- deploy.yaml# deploy.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:examplespec:...Build Output apiVersion:apps/v1kind:Deploymentmetadata:name:exampleannotations:oncallPager:800-555-1212spec:...","excerpt":"Add annotations to all resources. If the annotation key already is present on the resource, the …","ref":"/references/kustomize/commonannotations/","title":"commonAnnotations"},{"body":"","excerpt":"","ref":"/guides/config_management/","title":"Configuration Management"},{"body":" TL;DR  Copy files to and from Containers in a cluster   Copying Container Files Motivation  Copying files from Containers in a cluster to a local filesystem Copying files from a local filesystem to Containers in a cluster  Install Tar Copy requires that tar be installed in the container image.  Local to Remote Copy a local file to a remote Pod in a cluster.\n Local file format is \u0026lt;path\u0026gt; Remote file format is \u0026lt;pod-name\u0026gt;:\u0026lt;path\u0026gt;  kubectl cp /tmp/foo_dir \u0026lt;some-pod\u0026gt;:/tmp/bar_dir Remote to Local Copy a remote file from a Pod to a local file.\n Local file format is \u0026lt;path\u0026gt; Remote file format is \u0026lt;pod-name\u0026gt;:\u0026lt;path\u0026gt;  kubectl cp \u0026lt;some-pod\u0026gt;:/tmp/foo /tmp/bar  Operations One can also perfrom operations such as:\n Copy a specific container within a Pod running multiple containers Set the Pod namespace by prefixing the Pod name with \u0026lt;namespace\u0026gt;/.   Command / Examples Check out the reference for commands and examples of cp  ","excerpt":"TL;DR  Copy files to and from Containers in a cluster   Copying Container Files Motivation  Copying …","ref":"/guides/container_debugging/copying_container_files/","title":"Copying Container Files"},{"body":"Print out specific fields as Columns.\nNote: Custom Columns can also be read from a file using -o custom-columns-file.\nCommand kubectl get deployments -o custom-columns=\u0026#34;Name:metadata.name,Replicas:spec.replicas,Strategy:spec.strategy.type\u0026#34; Output Name Replicas Strategy nginx 1 RollingUpdate ","excerpt":"Print out specific fields as Columns.\nNote: Custom Columns can also be read from a file using -o …","ref":"/references/kubectl/get/options/custom_columns/","title":"custom columns"},{"body":"default If no output format is specified, Get will print a default set of columns.\nNote: Some columns may not directly map to fields on the Resource, but instead may be a summary of fields.\nCommand kubectl get deployments nginx Output NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 1 1 1 0 5s ","excerpt":"default If no output format is specified, Get will print a default set of columns.\nNote: Some …","ref":"/references/kubectl/get/default/","title":"default"},{"body":" TL;DR  krew.sigs.k8s.io is a kubernetes sub-project to discover and manage plugins   Krew By design, kubectl does not install plugins. This task is left to the kubernetes sub-project krew.sigs.k8s.io which needs to be installed separately. Krew helps to\n discover plugins get updates for installed plugins remove plugins  Installing krew Krew should be used as a kubectl plugin. To set yourself up to using krew - please check out the Installation section for krew\nKrew capabilities Discover plugins\nkubectl krew search Install a plugin\nkubectl krew install access-matrix Upgrade all installed plugins\nkubectl krew upgrade Show details about a plugin\nkubectl krew info access-matrix Uninstall a plugin\nkubectl krew uninstall access-matrix ","excerpt":"TL;DR  krew.sigs.k8s.io is a kubernetes sub-project to discover and manage plugins   Krew By design, …","ref":"/guides/extending_kubectl/discovering_plugins/","title":"Discovering Plugins"},{"body":"本教程只是一个快速开始的示例，完整的插件文档请看：kustomize 插件\n本示例将使用 bash 编写了一个简单的 exec 插件，用来生成一个 ConfigMap。\n尝试本教程不会破坏你的当前设置。\n环境要求  linux git curl Go 1.13  创建一个工作空间/目录 DEMO=$(mktemp -d) 编写 kustomization 新建一个目录来保存所有的配置：\nMYAPP=$DEMO/myapp mkdir -p $MYAPP 编写一个 Deployment 配置：\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt;$MYAPP/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: the-deployment spec: replicas: 3 template: spec: containers: - name: the-container image: monopole/hello:1 command: [\u0026#34;/hello\u0026#34;, \u0026#34;--port=8080\u0026#34;, \u0026#34;--date=$(THE_DATE)\u0026#34;, \u0026#34;--enableRiskyFeature=$(ENABLE_RISKY)\u0026#34;] ports: - containerPort: 8080 env: - name: THE_DATE valueFrom: configMapKeyRef: name: the-map key: today - name: ALT_GREETING valueFrom: configMapKeyRef: name: the-map key: altGreeting - name: ENABLE_RISKY valueFrom: configMapKeyRef: name: the-map key: enableRisky EOF 编写一个 service 配置：\ncat \u0026lt;\u0026lt;EOF \u0026gt;$MYAPP/service.yaml kind: Service apiVersion: v1 metadata: name: the-service spec: type: LoadBalancer ports: - protocol: TCP port: 8666 targetPort: 8080 EOF 现在为您要编写的插件创建一个配置文件。\n这个配置文件的内容也是 k8s 资源对象。其中 apiVersion 和 kind 字段的值用于在文件系统中查找插件代码（稍后会对此进行更多介绍）。\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt;$MYAPP/cmGenerator.yaml apiVersion: myDevOpsTeam kind: SillyConfigMapGenerator metadata: name: whatever argsOneLiner: Bienvenue true EOF 最后在 kustomization 文件中引用以上所有内容：\ncat \u0026lt;\u0026lt;EOF \u0026gt;$MYAPP/kustomization.yaml commonLabels: app: hello resources: - deployment.yaml - service.yaml generators: - cmGenerator.yaml EOF 检查这些文件\nls -C1 $MYAPP 为插件创建目录 插件必须位于特定的目录，以便 Kustomize 能够找到它们。\n该示例将使用临时目录：\nPLUGIN_ROOT=$DEMO/kustomize/plugin 在上面定义的插件配置 $MYAPP/cmGenerator.yaml 中指定：\n apiVersion:myDevOpsTeamkind:SillyConfigMapGenerator 这意味着该插件必须位于以下目录中：\nMY_PLUGIN_DIR=$PLUGIN_ROOT/myDevOpsTeam/sillyconfigmapgenerator mkdir -p $MY_PLUGIN_DIR 插件的目录结构为： apiVersion 的 value/小写 kind 的 value。\n插件拥有自己的目录，不但可以保存插件代码，还可以保存测试代码以需要的补充数据文件。\n编写插件 插件有 exec 和 Go 两种.\n编写一个 exec 插件，将其安装到正确的目录，文件名必须与插件的类型匹配（在本例中为 SillyConfigMapGenerator）：\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt;$MY_PLUGIN_DIR/SillyConfigMapGenerator #!/bin/bash # Skip the config file name argument. shift today=`date +%F` echo \u0026#34; kind: ConfigMap apiVersion: v1 metadata: name: the-map data: today: $today altGreeting: \u0026#34;$1\u0026#34; enableRisky: \u0026#34;$2\u0026#34; \u0026#34; EOF 根据定义，exec 插件必须是可执行的：\nchmod a+x $MY_PLUGIN_DIR/SillyConfigMapGenerator 安装 kustomize 根据文档安装 kustomize:\ncurl -s \u0026#34;https://raw.githubusercontent.com/\\ kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash mkdir -p $DEMO/bin mv kustomize $DEMO/bin 检查目录结构 tree $DEMO 使用插件构建 APP XDG_CONFIG_HOME=$DEMO $DEMO/bin/kustomize build --enable_alpha_plugins $MYAPP 之前如果您已经设置了 PLUGIN_ROOT=$HOME/.config/kustomize/plugin，则无需在 kustomize 命令前使用 XDG_CONFIG_HOME。\n","excerpt":"本教程只是一个快速开始的示例，完整的插件文档请看：kustomize 插件\n本示例将使用 bash 编写了一个简单的 exec 插件，用来生成一个 ConfigMap。\n尝试本教程不会破坏你的当前设 …","ref":"/zh/guides/plugins/execpluginguidedexample/","title":"Exec 插件示例"},{"body":"Requires Go to be installed.\nInstall the kustomize CLI from source without cloning the repo GOBIN=$(pwd)/ GO111MODULE=on go get sigs.k8s.io/kustomize/kustomize/v3 Install the kustomize CLI from local source with cloning the repo # Need go 1.13 or higher unset GOPATH # see https://golang.org/doc/go1.13#modules unset GO111MODULES # clone the repo git clone git@github.com:kubernetes-sigs/kustomize.git # get into the repo root cd kustomize # Optionally checkout a particular tag if you don\u0026#39;t # want to build at head git checkout kustomize/v3.2.3 # build the binary (cd kustomize; go install .) # run it ~/go/bin/kustomize version ","excerpt":"Requires Go to be installed.\nInstall the kustomize CLI from source without cloning the repo …","ref":"/installation/kustomize/source/","title":"Go Source"},{"body":"需要先安装 Go。\n无需克隆源码库直接构建 kustomize CLI GOBIN=$(pwd)/ GO111MODULE=on go get sigs.k8s.io/kustomize/kustomize/v3 在本地克隆源码库构建 kustomize CLI # 需要 go 1.13 或更高版本 unset GOPATH # 详见 https://golang.org/doc/go1.13#modules unset GO111MODULES # 拉取 repo git clone git@github.com:kubernetes-sigs/kustomize.git # 进入目录 cd kustomize # 如果您不想从 HEAD 开始构建， 则可以选择切换特定的标签 git checkout kustomize/v3.2.3 # 开始构建 (cd kustomize; go install .) # 运行 ~/go/bin/kustomize version ","excerpt":"需要先安装 Go。\n无需克隆源码库直接构建 kustomize CLI GOBIN=$(pwd)/ GO111MODULE=on go get …","ref":"/zh/installation/source/","title":"Go 源码"},{"body":" For Homebrew users:  brew install kubectl or\nbrew install kubernetes-cli  For MacPorts users:  sudo port selfupdate sudo port install kubectl ","excerpt":" For Homebrew users:  brew install kubectl or\nbrew install kubernetes-cli  For MacPorts users:  sudo …","ref":"/installation/kubectl/homebrew/","title":"Homebrew / MacPorts"},{"body":"A kustomization file supports patching in three ways:\n patchesStrategicMerge: A list of patch files where each file is parsed as a Strategic Merge Patch. patchesJSON6902: A list of patches and associated targetes, where each file is parsed as a JSON Patch and can only be applied to one target resource. patches: A list of patches and their associated targets. The patch can be applied to multiple objects. It auto detects whether the patch is a Strategic Merge Patch or JSON Patch.  Since 3.2.0, all three support inline patch, where the patch content is put inside the kustomization file as a single string. With this feature, no separate patch files need to be created.\nMake a base kustomization containing a Deployment resource.\nDefine a place to work:\nDEMO_HOME = $(mktemp -d) /base Define a common base:\n$ cd $DEMO_HOME $ mkdir base $ cd base Create a Sample Pod File and Kustomize file in base\n$ vim kustomization.yaml # kustomization.yaml contentsresources:- deployments.yaml# deployments.yaml contentsapiVersion:apps/v1kind:Deploymentmetadata:name:deployspec:template:metadata:labels:foo:barspec:containers:- name:nginximage:nginxargs:- arg1- arg2PatchesStrategicMerge patch Create an overlay and add an inline patch in patchesStrategicMerge field to the kustomization file to change the image from nginx to nginx:latest.\n$ cd $DEMO_HOME $ mkdir smp_patch $ cd smp_patch Create a Kustomize file in smp_patch\n# kustomization.yaml contentsresources:- ../basepatchesStrategicMerge:- |-apiVersion: apps/v1 kind: Deployment metadata: name: deploy spec: template: spec: containers: - name: nginx image: nginx:latestRunning kustomize build, in the output confirm that image is updated successfully.\napiVersion:apps/v1kind:Deploymentmetadata:name:deployspec:template:metadata:labels:foo:barspec:containers:- args:- arg1- arg2image:nginx:latestname:nginx$patch: delete and $patch: replace also work in the inline patch. Change the inline patch to delete the container nginx.\npatch: delete $ cd $DEMO_HOME $ mkdir smp_delete $ cd smp_delete Create a Kustomize file in smp_delete\n# kustomization.yaml contentsresources:- ../basepatchesStrategicMerge:- |-apiVersion: apps/v1 kind: Deployment metadata: name: deploy spec: template: spec: containers: - name: nginx $patch: deleteRunning kustomize build, in the output confirm that image is updated successfully.\napiVersion:apps/v1kind:Deploymentmetadata:name:deployspec:template:metadata:labels:foo:barspec:containers:[]patch: replace $ cd $DEMO_HOME $ mkdir smp_replace $ cd smp_replace Create a Kustomize file in smp_replace\n# kustomization.yaml contentsresources:- ../basepatchesStrategicMerge:- |-apiVersion: apps/v1 kind: Deployment metadata: name: deploy spec: template: spec: containers: - name: nginx image: nginx:1.7.9 $patch: replaceRunning kustomize build, in the output confirm that image is updated successfully. Since we are replacing notice that the arguments set in the base file are gone.\napiVersion:apps/v1kind:Deploymentmetadata:name:deployspec:template:metadata:labels:foo:barspec:containers:- image:nginx:1.7.9name:nginxPatchesJson6902 Create an overlay and add an inline patch in patchesJSON6902 field to the kustomization file to change the image from nginx to nginx:latest.\n$ cd $DEMO_HOME $ mkdir json $ cd json Create a Kustomize file in json\n# kustomization.yaml contentsresources:- ../basepatchesJSON6902:- target:group:appsversion:v1kind:Deploymentname:deploypatch:|-- op: replace path: /spec/template/spec/containers/0/image value: nginx:latestRunning kustomize build, in the output confirm that image is updated successfully.\napiVersion:apps/v1kind:Deploymentmetadata:name:deployspec:template:metadata:labels:foo:barspec:containers:- args:- arg1- arg2image:nginx:latestname:nginxPatches Create an overlay and add an inline patch in patches field to the kustomization file to change the image from nginx to nginx:latest.\n$ cd $DEMO_HOME $ mkdir patch $ cd patch Create a Kustomize file in patch\n# kustomization.yaml contentsresources:- ../basepatches:- target:kind:Deploymentname:deploypatch:|-apiVersion: apps/v1 kind: Deployment metadata: name: deploy spec: template: spec: containers: - name: nginx image: nginx:latestRunning kustomize build, in the output confirm that image is updated successfully.\napiVersion:apps/v1kind:Deploymentmetadata:name:deployspec:template:metadata:labels:foo:barspec:containers:- args:- arg1- arg2image:nginx:latestname:nginx","excerpt":"A kustomization file supports patching in three ways:\n patchesStrategicMerge: A list of patch files …","ref":"/guides/example/inline_patch/","title":"Inline Patch"},{"body":"","excerpt":"","ref":"/contributing/kustomize/","title":"Kustomize"},{"body":" TL;DR  Kustomize helps customizing config files in a template free way. Kustomize provides a number of handy methods like generators to make customization easier. Kustomize uses patches to introduce environment specific changes on an already existing standard config file without disturbing it.   Kustomize provides a solution for customizing Kubernetes resource configuration free from templates and DSLs.\nKustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.\nKustomize targets kubernetes; it understands and can patch kubernetes style API objects. It\u0026rsquo;s like make, in that what it does is declared in a file, and it\u0026rsquo;s like sed, in that it emits edited text.\nUsage 1) Make a kustomization file In some directory containing your YAML resource files (deployments, services, configmaps, etc.), create a kustomization file.\nThis file should declare those resources, and any customization to apply to them, e.g. add a common label.\nFile structure:\n~/someApp ├── deployment.yaml ├── kustomization.yaml └── service.yaml The resources in this directory could be a fork of someone else\u0026rsquo;s configuration. If so, you can easily rebase from the source material to capture improvements, because you don\u0026rsquo;t modify the resources directly.\nGenerate customized YAML with:\nkustomize build ~/someApp The YAML can be directly applied to a cluster:\nkustomize build ~/someApp | kubectl apply -f - 2) Create variants using overlays Manage traditional variants of a configuration - like development, staging and production - using overlays that modify a common base.\nFile structure:\n~/someApp ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml └── overlays ├── development │ ├── cpu_count.yaml │ ├── kustomization.yaml │ └── replica_count.yaml └── production ├── cpu_count.yaml ├── kustomization.yaml └── replica_count.yaml Take the work from step (1) above, move it into a someApp subdirectory called base, then place overlays in a sibling directory.\nAn overlay is just another kustomization, referring to the base, and referring to patches to apply to that base.\nThis arrangement makes it easy to manage your configuration with git. The base could have files from an upstream repository managed by someone else. The overlays could be in a repository you own. Arranging the repo clones as siblings on disk avoids the need for git submodules (though that works fine, if you are a submodule fan).\nGenerate YAML with\nkustomize build ~/someApp/overlays/production The YAML can be directly applied to a cluster:\nkustomize build ~/someApp/overlays/production | kubectl apply -f - ","excerpt":"TL;DR  Kustomize helps customizing config files in a template free way. Kustomize provides a number …","ref":"/guides/introduction/kustomize/","title":"Kustomize"},{"body":"","excerpt":"","ref":"/installation/kustomize/","title":"Kustomize"},{"body":"","excerpt":"","ref":"/references/kustomize/","title":"Kustomize"},{"body":" TL;DR  Get or List Raw Resources in a cluster as Yaml or Json   Print Raw Resource Motivation Inspecting or Debugging Resources.\nThe Kubernetes Resources stored in etcd by the apiserver have many more fields than are shown in the summarized views. Users can learn much more about a Resource by viewing the Raw Resource as Yaml or Json. The Raw Resource will contain:\n fields specified by the user in the Resource Config (e.g. metadata.name) metadata fields owned by the apiserver (e.g. metadata.creationTimestamp) fields defaulted by the apiserver (e.g. spec..imagePullPolicy) fields set by Controllers (e.g. spec.clusterIp, status)  Get The kubectl get reads Resources from the cluster and formats them as output. The examples in this chapter will query for Resources by providing Get the Resource Type as an argument. For more query options see Queries and Options.\nYAML Print the Raw Resource formatting it as YAML.\nkubectl get deployments -o yaml apiVersion:v1items:- apiVersion:extensions/v1beta1kind:Deploymentmetadata:annotations:deployment.kubernetes.io/revision:\u0026#34;1\u0026#34;creationTimestamp:2018-11-15T18:58:03Zgeneration:1labels:app:nginxname:nginxnamespace:defaultresourceVersion:\u0026#34;1672574\u0026#34;selfLink:/apis/extensions/v1beta1/namespaces/default/deployments/nginxuid:6131547f-e908-11e8-9ff6-42010a8a00d1spec:progressDeadlineSeconds:600replicas:1revisionHistoryLimit:10selector:matchLabels:app:nginxstrategy:rollingUpdate:maxSurge:25%maxUnavailable:25%type:RollingUpdatetemplate:metadata:creationTimestamp:nulllabels:app:nginxspec:containers:- image:nginximagePullPolicy:Alwaysname:nginxresources:{}terminationMessagePath:/dev/termination-logterminationMessagePolicy:FilednsPolicy:ClusterFirstrestartPolicy:AlwaysschedulerName:default-schedulersecurityContext:{}terminationGracePeriodSeconds:30status:availableReplicas:1conditions:- lastTransitionTime:2018-11-15T18:58:10ZlastUpdateTime:2018-11-15T18:58:10Zmessage:Deployment has minimum availability.reason:MinimumReplicasAvailablestatus:\u0026#34;True\u0026#34;type:Available- lastTransitionTime:2018-11-15T18:58:03ZlastUpdateTime:2018-11-15T18:58:10Zmessage:ReplicaSet \u0026#34;nginx-78f5d695bd\u0026#34; has successfully progressed.reason:NewReplicaSetAvailablestatus:\u0026#34;True\u0026#34;type:ProgressingobservedGeneration:1readyReplicas:1replicas:1updatedReplicas:1kind:Listmetadata:resourceVersion:\u0026#34;\u0026#34;selfLink:\u0026#34;\u0026#34; Command / Examples One can also get the raw output as with JSON Check out the reference for commands and examples for get  ","excerpt":"TL;DR  Get or List Raw Resources in a cluster as Yaml or Json   Print Raw Resource Motivation …","ref":"/guides/resource_printing/raw/","title":"Raw"},{"body":"YAML Print the Raw Resource formatting it as YAML.\nkubectl get deployments -o yaml apiVersion:v1items:- apiVersion:extensions/v1beta1kind:Deploymentmetadata:annotations:deployment.kubernetes.io/revision:\u0026#34;1\u0026#34;creationTimestamp:2018-11-15T18:58:03Zgeneration:1labels:app:nginxname:nginxnamespace:defaultresourceVersion:\u0026#34;1672574\u0026#34;selfLink:/apis/extensions/v1beta1/namespaces/default/deployments/nginxuid:6131547f-e908-11e8-9ff6-42010a8a00d1spec:progressDeadlineSeconds:600replicas:1revisionHistoryLimit:10selector:matchLabels:app:nginxstrategy:rollingUpdate:maxSurge:25%maxUnavailable:25%type:RollingUpdatetemplate:metadata:creationTimestamp:nulllabels:app:nginxspec:containers:- image:nginximagePullPolicy:Alwaysname:nginxresources:{}terminationMessagePath:/dev/termination-logterminationMessagePolicy:FilednsPolicy:ClusterFirstrestartPolicy:AlwaysschedulerName:default-schedulersecurityContext:{}terminationGracePeriodSeconds:30status:availableReplicas:1conditions:- lastTransitionTime:2018-11-15T18:58:10ZlastUpdateTime:2018-11-15T18:58:10Zmessage:Deployment has minimum availability.reason:MinimumReplicasAvailablestatus:\u0026#34;True\u0026#34;type:Available- lastTransitionTime:2018-11-15T18:58:03ZlastUpdateTime:2018-11-15T18:58:10Zmessage:ReplicaSet \u0026#34;nginx-78f5d695bd\u0026#34; has successfully progressed.reason:NewReplicaSetAvailablestatus:\u0026#34;True\u0026#34;type:ProgressingobservedGeneration:1readyReplicas:1replicas:1updatedReplicas:1kind:Listmetadata:resourceVersion:\u0026#34;\u0026#34;selfLink:\u0026#34;\u0026#34; JSON Print the Raw Resource formatting it as JSON.\nkubectl get deployments -o json { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;extensions/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;deployment.kubernetes.io/revision\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2018-11-15T18:58:03Z\u0026#34;, \u0026#34;generation\u0026#34;: 1, \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;nginx\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;1672574\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/extensions/v1beta1/namespaces/default/deployments/nginx\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;6131547f-e908-11e8-9ff6-42010a8a00d1\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;progressDeadlineSeconds\u0026#34;: 600, \u0026#34;replicas\u0026#34;: 1, \u0026#34;revisionHistoryLimit\u0026#34;: 10, \u0026#34;selector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;nginx\u0026#34; } }, \u0026#34;strategy\u0026#34;: { \u0026#34;rollingUpdate\u0026#34;: { \u0026#34;maxSurge\u0026#34;: \u0026#34;25%\u0026#34;, \u0026#34;maxUnavailable\u0026#34;: \u0026#34;25%\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;RollingUpdate\u0026#34; }, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: null, \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;nginx\u0026#34; } }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;image\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;imagePullPolicy\u0026#34;: \u0026#34;Always\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;resources\u0026#34;: {}, \u0026#34;terminationMessagePath\u0026#34;: \u0026#34;/dev/termination-log\u0026#34;, \u0026#34;terminationMessagePolicy\u0026#34;: \u0026#34;File\u0026#34; } ], \u0026#34;dnsPolicy\u0026#34;: \u0026#34;ClusterFirst\u0026#34;, \u0026#34;restartPolicy\u0026#34;: \u0026#34;Always\u0026#34;, \u0026#34;schedulerName\u0026#34;: \u0026#34;default-scheduler\u0026#34;, \u0026#34;securityContext\u0026#34;: {}, \u0026#34;terminationGracePeriodSeconds\u0026#34;: 30 } } }, \u0026#34;status\u0026#34;: { \u0026#34;availableReplicas\u0026#34;: 1, \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-11-15T18:58:10Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2018-11-15T18:58:10Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Deployment has minimum availability.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;MinimumReplicasAvailable\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-11-15T18:58:03Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2018-11-15T18:58:10Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;ReplicaSet \\\u0026#34;nginx-78f5d695bd\\\u0026#34; has successfully progressed.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;NewReplicaSetAvailable\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Progressing\u0026#34; } ], \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;readyReplicas\u0026#34;: 1, \u0026#34;replicas\u0026#34;: 1, \u0026#34;updatedReplicas\u0026#34;: 1 } } ], \u0026#34;kind\u0026#34;: \u0026#34;List\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;resourceVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;\u0026#34; } } ","excerpt":"YAML Print the Raw Resource formatting it as YAML.\nkubectl get deployments -o yaml …","ref":"/references/kubectl/get/raw/","title":"raw"},{"body":"","excerpt":"","ref":"/zh/guides/","title":"指南"},{"body":"在这个工作流程中，所有文件都由用户拥有，并维护在他们控制的存储库中，但它们是基于一个现成的（off-the-shelf）配置，定期查询更新。\n1) 寻找并且 fork 一个 OTS 配置 2) 将其克隆为你自己的 base 这个 base 目录维护在上游为 OTS 配置的 repo ，在这个示例中使用 ladp 的 repo 。\n mkdir ~/ldap git clone https://github.com/$USER/ldap ~/ldap/base cd ~/ldap/base git remote add upstream git@github.com:$USER/ldap  3) 创建 overlays 如配置定制方法一样，创建并完善 overlays 目录中的内容。\n所有的 overlays 都依赖于 base 。\n mkdir -p ~/ldap/overlays/staging mkdir -p ~/ldap/overlays/production  用户可以将 overlays 维护在不同的 repo 中。\n4) 生成 variants  kustomize build ~/ldap/overlays/staging | kubectl apply -f - kustomize build ~/ldap/overlays/production | kubectl apply -f -  也可以在 kubectl-v1.14.0 版，使用 kubectl 命令发布你的 variants 。\n kubectl apply -k ~/ldap/overlays/staging kubectl apply -k ~/ldap/overlays/production  5) （可选）从上游更新 用户可以定期从上游 repo 中 rebase 他们的 base 以保证及时更新。\n cd ~/ldap/base git fetch upstream git rebase upstream/master  ","excerpt":"在这个工作流程中，所有文件都由用户拥有，并维护在他们控制的存储库中，但它们是基于一个现成的（off-the-shelf）配置，定期查询更新。\n1) 寻找并且 fork 一个 OTS 配置 2) 将其克 …","ref":"/zh/guides/offtheshelf/","title":"通用配置（Off-the-shelf configuration）"},{"body":"","excerpt":"","ref":"/references/architecture/","title":"Architecture"},{"body":"Binaries at various versions for linux, MacOs and Windows are published on the releases page.\nThe following script detects your OS and downloads the appropriate kustomize binary to your current working directory.\ncurl -s \u0026#34;https://raw.githubusercontent.com/\\ kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash This script doesn\u0026rsquo;t work for ARM architecture. If you want to install ARM binaries, please go to the release page to find the URL.\n","excerpt":"Binaries at various versions for linux, MacOs and Windows are published on the releases page.\nThe …","ref":"/installation/kustomize/binaries/","title":"Binaries"},{"body":" For Chocolatey users:  choco install kubernetes-cli ","excerpt":" For Chocolatey users:  choco install kubernetes-cli ","ref":"/installation/kubectl/chocolatey/","title":"Chocolatey"},{"body":"Add labels and selectors to all resources. If the label key already is present on the resource, the value will be overridden.\nSelectors for resources such as Deployments and Services shouldn\u0026rsquo;t be changed once the resource has been applied to a cluster.\nChanging commonLabels to live resources could result in failures.\n apiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationcommonLabels:someName:someValueowner:aliceapp:bingoExample File Input # kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationcommonLabels:someName:someValueowner:aliceapp:bingoresources:- deploy.yaml- service.yaml# deploy.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:example# service.yamlapiVersion:v1kind:Servicemetadata:name:exampleBuild Output apiVersion:v1kind:Servicemetadata:labels:app:bingoowner:alicesomeName:someValuename:examplespec:selector:app:bingoowner:alicesomeName:someValue---apiVersion:apps/v1kind:Deploymentmetadata:labels:app:bingoowner:alicesomeName:someValuename:examplespec:selector:matchLabels:app:bingoowner:alicesomeName:someValuetemplate:metadata:labels:app:bingoowner:alicesomeName:someValue","excerpt":"Add labels and selectors to all resources. If the label key already is present on the resource, the …","ref":"/references/kustomize/commonlabels/","title":"commonLabels"},{"body":"Starting with Kustomize v3.8.7, docker images are available to run Kustomize. The images are hosted in kubernetes official GCR repositories.\nSee GCR page for available images.\nThe following commands are how to pull and run kustomize 3.8.7 docker image.\ndocker pull k8s.gcr.io/kustomize/kustomize:v3.8.7 docker run k8s.gcr.io/kustomize/kustomize:v3.8.7 version ","excerpt":"Starting with Kustomize v3.8.7, docker images are available to run Kustomize. The images are hosted …","ref":"/installation/kustomize/docker/","title":"Docker Images"},{"body":"This site uses Docsy and was forked from the docsy-example\nPrerequisites  Install hugo Clone kustomize  git clone git@github.com:kubernetes-sigs/cli-experimental \u0026amp;\u0026amp; cd site/    Development The doc input files are in the site directory. The site can be hosted locally using hugo server.\ncd site/ npm install npm install -g postcss-cli npm install autoprefixer npm audit fix hugo server ... Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Publishing Hugo compiles the files under site Hugo into html which it puts in the docs folder:\ncd site/ hugo | EN -------------------+----- Pages | 99 Paginator pages | 0 Non-page files | 0 Static files | 47 Processed images | 0 Aliases | 2 Sitemaps | 1 Cleaned | 0 Add the site/ and docs/ folders to a commit, then create a PR.\nPublishing docs in forked repository We use Netlify to publish changes in the site. You can also enable netlify on you\u0026rsquo;re forked repo by doing the following step.\n Log into Netlify using your Github Credentials. Click New Site from Git button in the Netlify Dashboard. The setup has 3 steps.  Connect to Git Provider - Select Github here and authenticate your Github account if not done earlier. Pick a repository - Select the forked repository here. Build options, and deploy! - Here set Branch to deploy to the branch that has the latest changes also set Publish directory to ./docs.    Raising a PR for changes in the site  Once deployed, you\u0026rsquo;ll have a URL pointing to the newly deployed site. Submit the URL along with the PR. Make sure your changes are working as expected in the newly received netlify URL before PR.  Setting Custom Domain \u0026amp; DNS changes Note This is applicable only for the site adminisrators on the event of site migration.   Make sure you\u0026rsquo;re a part of Kubernetes Docs Netlify team. Under Site Settings you\u0026rsquo;ll find Domain Management, where in you can set the site\u0026rsquo;s custom domain. Ideally, it should match up with the wild card *.k8s.io Once custom domains are set on Netlify, you can raise a PR in k8s.io github repository. You\u0026rsquo;ll have to add this snippet in dns/zone-configs/k8s.io._0_base.yaml file:  # \u0026lt;github repo url\u0026gt; (@maintainers)\u0026lt;custom_name\u0026gt;:type:CNAMEvalue:\u0026lt;current_netlify_url\u0026gt;. Useful Links  Subproject Site Requests: https://github.com/kubernetes/community/blob/master/github-management/subproject-site-requests.md. Issue template for site request: https://github.com/kubernetes/org/issues/new/choose, select Netlify site request.   ","excerpt":"This site uses Docsy and was forked from the docsy-example\nPrerequisites  Install hugo Clone …","ref":"/contributing/docs/","title":"Documentation"},{"body":" TL;DR  Execute a Command in a Container Get a Shell in a Container   Executing Commands Motivation Debugging Workloads by running commands within the Container. Commands may be a Shell with a tty.\nExec Command Run a command in a Container in the cluster by specifying the Pod name.\nkubectl exec nginx-78f5d695bd-czm8z ls bin boot dev\tetc home lib\tlib64 media mnt opt\tproc root run sbin srv sys tmp usr var Exec Shell To get a Shell in a Container, use the -t -i options to get a tty and attach STDIN.\nkubectl exec -t -i nginx-78f5d695bd-czm8z bash root@nginx-78f5d695bd-czm8z:/# ls bin boot dev\tetc home lib\tlib64 media mnt opt\tproc root run sbin srv sys tmp usr var  Specifying the Container For Pods running multiple Containers, the Container should be specified with -c \u0026lt;container-name\u0026gt;.  ","excerpt":"TL;DR  Execute a Command in a Container Get a Shell in a Container   Executing Commands Motivation …","ref":"/guides/container_debugging/executing_a_command_in_a_container/","title":"Executing a command in a container"},{"body":" TL;DR  Format and print specific fields from Resources Use when scripting with Get   Print Resource Fields Motivation Kubectl Get is able to pull out fields from Resources it queries and format them as output.\nThis may be useful for scripting or gathering data about Resources from a Kubernetes cluster.\nGet The kubectl get reads Resources from the cluster and formats them as output. The examples in this chapter will query for Resources by providing Get the Resource Type with the Version and Group as an argument. For more query options see Queries and Options.\nKubectl can format and print specific fields from Resources using Json Path.\nScripting Pitfalls By default, if no API group or version is specified, kubectl will use the group and version preferred by the apiserver.\nBecause the Resource structure may change between API groups and Versions, users should specify the API Group and Version when emitting fields from kubectl get to make sure the command does not break in future releases.\nFailure to do this may result in the different API group / version being used after a cluster upgrade, and this group / version may have changed the representation of fields.\n Example: Print the JSON representation of the first Deployment in the list on a single line\nkubectl get deployment.v1.apps -o=jsonpath=\u0026#39;{.items[0]}{\u0026#34;\\n\u0026#34;}\u0026#39; you get:\nmap[apiVersion:apps/v1 kind:Deployment...replicas:1 updatedReplicas:1]] This ideology can be extended to query out the specific fields in a yaml resource file.\nCommand / Examples Check out the reference for more commands and examples.  ","excerpt":"TL;DR  Format and print specific fields from Resources Use when scripting with Get   Print Resource …","ref":"/guides/resource_printing/fields/","title":"Fields"},{"body":"Go 插件 是一个编译产品/组件，其定义见 plugin package，需要特殊的构建标志，不能单独运行，必须加载到正在运行的 Go 程序中。\n 用 Go 编写的普通程序可以作为 exec 插件，但是不能作为 Go 插件。\n Go 插件允许运行 kustomize 扩展，而无需在每次运行时将资源分配到子流程或从子流程中解封所有资源数据。Go 插件 API 确保一定程度的一致性，以避免混淆下游转换器。\nGo 插件的工作方式与 plugin package 中所述的相同，但与 plugin 一词相关的常见概念不同。\nThe skew problem Go 插件编译会创建一个 ELF 格式的 .so 文件，根据定义，该文件不包含有关目标代码来源的信息。\n主程序 ELF 和插件 ELF 的编译条件（软件包依赖项的版本 GOOS，GOARCH）之间的偏移会导致插件加载失败，并带有无用的错误消息。\nExec 插件也会缺乏来源，但不会因编译不正确而失败。\n在任何情况下，共享插件的最好方法是使用某种 捆绑包（git repo URL、git 存档文件、tar 包等），其中包含可解包至 $XDG_CONFIG_HOME/kustomize/plugin 的源代码，测试和相关数据。\n对于 Go 插件，使用共享插件的最终用户 必须同时编译 kustomize 和 plugin。\n这意味着一次性运行\n# Or whatever is appropriate at time of reading GOPATH=${whatever} GO111MODULE=on go get sigs.k8s.io/kustomize/api 然后使用一个正常的开发周期\ngo build -buildmode plugin \\  -o ${wherever}/${kind}.so ${wherever}/${kind}.go 并根据需要调整路径和发行版本标记（例如 v3.0.0）。\n为了进行比较，可以参考编写 tensorflow plugin 必须做的事情。\n为什么支持 Go 插件 安全 Go 插件开发者可以操作与原生 kustomize 操作相同的 API，可确保某些语义、变量和检查等一致。exec 插件子进程通过 stdin/stdout 来处理这些问题，但对于下游的转化器和使用者来说，会更容易把事情搞砸。\n关键点：如果插件通过 kustomize 提供的文件 Loader 接口读取文件，则会受到 kustomize 文件加载限制的约束。当然，除了代码审计之外，没有什么可以阻止 Go 插件导入 io 包并执行其所需的任何操作。\nDebugging Go 插件开发者可以在功能测试中运行插件时，在 本地 调试插件，并在插件内部和其他位置设置断点。\n为了获得两全其美的方式（共享性和安全性），开发人员可以编写一个 .go 程序作为 exec 插件，同时可以被 go generate 程序处理生成 Go 插件（反之亦然）。\n贡献单元化 所有内置的生成器和转换器本身都是 Go 插件。这意味着 kustomize 维护者可以将贡献的插件升级为内置插件，而无需更改代码（超出常规代码审阅要求的范围）。\n围绕生态系统发展 工具可以简化 Go 插件的 共享，但是这需要大量的 Go 插件的创作，而这又会导致围绕共享插件的混乱。Go modules 一旦被更广泛地采用，将解决共享插件最大的难题：含糊不清的插件 vs 主机依赖性。\n","excerpt":"Go 插件 是一个编译产品/组件，其定义见 plugin package，需要特殊的构建标志，不能单独运行，必须加载到正在运行的 Go 程序中。\n 用 Go 编写的普通程序可以作为 exec 插件，但 …","ref":"/zh/guides/plugins/goplugincaveats/","title":"Go 插件注意事项"},{"body":"For Homebrew users:\nbrew install kustomize For MacPorts users:\nsudo port install kustomize ","excerpt":"For Homebrew users:\nbrew install kustomize For MacPorts users:\nsudo port install kustomize ","ref":"/installation/kustomize/homebrew/","title":"Homebrew / MacPorts"},{"body":"Homebrew 用户可以：\nbrew install kustomize MacPorts 用户可以：\nsudo port install kustomize ","excerpt":"Homebrew 用户可以：\nbrew install kustomize MacPorts 用户可以：\nsudo port install kustomize ","ref":"/zh/installation/homebrew/","title":"Homebrew / MacPorts"},{"body":"Print out specific labels each as their own columns\nCommand kubectl get deployments -L=app Output NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE APP nginx 1 1 1 1 8m nginx ","excerpt":"Print out specific labels each as their own columns\nCommand kubectl get deployments -L=app Output …","ref":"/references/kubectl/get/options/labels/","title":"labels"},{"body":" Experimental Content in this chapter is experimental and will evolve based on user feedback.\nLeave feedback on the conventions by creating an issue in the kubectl GitHub repository.\n TL;DR  Publish a White Box Application as a Base for other users to Kustomize   Publishing Bases Motivation Users may want to run a common White Box Application without writing the Resource Config for the Application from scratch. Instead they may want to consume ready-made Resource Config published specifically for the White Box Application, and add customizations for their specific needs.\n Run a White Box Application (e.g. Cassandra, MongoDB) instance from ready-made Resource Config Publish Resource Config to run an Application  Publishing a White Box Base White Box Applications may be published to a URL and consumed as Bases in an kustomization.yaml. It can then be consumed in the following manner.\nUse Case: Run a White Box Application published to GitHub.\nInput: The kustomization.yaml file\n# kustomization.yamlbases:# GitHub URL- github.com/kubernetes-sigs/kustomize/examples/multibases/dev/?ref=v1.0.6Applied: The Resource that is Applied to the cluster\n# Resource comes from the Remote BaseapiVersion:v1kind:Podmetadata:labels:app:myappname:dev-myapp-podspec:containers:- image:nginx:1.7.9name:nginxVersioning White Box Bases White Box Bases may be versioned using the well known versioning techniques provided by Git.\n  Tag\nBases may be versioned by applying a tag to the repo and modifying the url to point to the tag: github.com/kubernetes-sigs/kustomize/tree/master/examples/multibases?ref=v1.0.6\n  Branch\nBases may be versioned by creating a branch and modifying the url to point to the branch: github.com/Liujingfang1/kustomize/tree/master/examples/helloWorld?ref=repoUrl2\n  Commit\nIf the White Box Base has not been explicitly versioned by the maintainer, users may pin the base to a specific commit: github.com/Liujingfang1/kustomize/tree/master/examples/helloWorld?ref=7050a45134e9848fca214ad7e7007e96e5042c03\n  Forking a White Box Base Uses may fork a White Box Base hosted on GitHub by forking the GitHub repo. This allows the user complete control over changes to the Base. Users should periodically pull changes from the upstream repo back into the fork to get bug fixes and optimizations.\n","excerpt":"Experimental Content in this chapter is experimental and will evolve based on user feedback.\nLeave …","ref":"/guides/app_deployment/publishing_bases/","title":"Publishing Bases"},{"body":"","excerpt":"","ref":"/guides/resource_printing/","title":"Resource Printing"},{"body":" TL;DR  A Kubernetes API has 2 parts - a Resource Type and a Controller Resources are objects declared as json or yaml and written to a cluster Controllers asynchronously actuate Resources after they are stored   This section provides background on the Kubernetes Resource model. This information is also available at the kubernetes.io docs site.\nFor more information on Kubernetes Resources see: kubernetes.io Concepts.\nResources Instances of Kubernetes objects (e.g. Deployment, Services, Namespaces, etc) are called Resources.\nResources which run containers are referred to as Workloads.\nExamples of Workloads:\n Deployments StatefulSets Jobs CronJobs DaemonSets  Users work with Resource APIs by declaring them in files which are then Applied to a Kubernetes cluster. These declarative files are called Resource Config.\nResource Config is Applied (declarative Create/Update/Delete) to a Kubernetes cluster using tools such as Kubectl, and then actuated by a Controller.\nResources are uniquely identified:\n apiVersion (API Type Group and Version) kind (API Type Name) metadata.namespace (Instance namespace) metadata.name (Instance name)  Default Namespace If namespace is omitted from the Resource Config, the default namespace is used. Users should almost always explicitly specify the namespace for their Application using a kustomization.yaml.  Resources Structure Resources have the following components.\nTypeMeta: Resource Type apiVersion and kind.\nObjectMeta: Resource name and namespace + other metadata (labels, annotations, etc).\nSpec: the desired state of the Resource - intended state the user provides to the cluster.\nStatus: the observed state of the object - recorded state the cluster provides to the user.\nResource Config written by the user omits the Status field.\nExample Deployment Resource Config\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentlabels:app:nginxspec:replicas:3selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.15.4 Spec and Status Resources such as ConfigMaps and Secrets do not have a Status, and as a result their Spec is implicit (i.e. they don\u0026rsquo;t have a spec field).  Controllers Controllers actuate Kubernetes APIs. They observe the state of the system and look for changes either to desired state of Resources (create, update, delete) or the system (Pod or Node dies).\nControllers then make changes to the cluster to fulfill the intent specified by the user (e.g. in Resource Config) or automation (e.g. changes from Autoscalers).\nExample: After a user creates a Deployment, the Deployment Controller will see that the Deployment exists and verify that the corresponding ReplicaSet it expects to find exists. The Controller will see that the ReplicaSet does not exist and will create one.\nAsynchronous Actuation Because Controllers run asynchronously, issues such as a bad Container Image or unschedulable Pods will not be present in the CRUD response. Tooling must facilitate processes for watching the state of the system until changes are completely actuated by Controllers. Once the changes have been fully actuated such that the desired state matches the observed state, the Resource is considered Settled.  Controller Structure Reconcile\nControllers actuate Resources by reading the Resource they are Reconciling + related Resources, such as those that they create and delete.\nControllers do not Reconcile events, rather they Reconcile the expected cluster state to the observed cluster state at the time Reconcile is run.\n Deployment Controller creates/deletes ReplicaSets ReplicaSet Controller creates/deletes Pods Scheduler (Controller) writes Nodes to Pods Node (Controller) runs Containers specified in Pods on the Node  Watch\nControllers actuate Resources after they are written by Watching Resource Types, and then triggering Reconciles from Events. After a Resource is created/updated/deleted, Controllers Watching the Resource Type will receive a notification that the Resource has been changed, and they will read the state of the system to see what has changed (instead of relying on the Event for this information).\n Deployment Controller watches Deployments + ReplicaSets (+ Pods) ReplicaSet Controller watches ReplicaSets + Pods Scheduler (Controller) watches Pods Node (Controller) watches Pods (+ Secrets + ConfigMaps)  Level vs Edge Based Reconciliation Because Controllers don\u0026rsquo;t respond to individual Events, but instead Reconcile the state of the system at the time that Reconcile is run, changes from several different events may be observed and Reconciled together. This is referred to as a Level Based system, whereas a system that responds to each event individually would be referred to as an Edge Based system.  Overview of Kubernetes Resource APIs Pods Containers are run in Pods which are scheduled to run on Nodes (i.e. worker machines) in a cluster.\nPods run a single replica of an Application and provide:\n Compute Resources (cpu, memory, disk) Environment Variables Readiness and Health Checking Network (IP address shared by containers in the Pod) Mounting Shared Configuration and Secrets Mounting Storage Volumes Initialization  Multi Container Pods Multiple replicas of an Application should be created using a Workload API to manage creation and deletion of Pod replicas using a PodTemplate.\nIn some cases a Pod may contain multiple Containers forming a single instance of an Application. These containers may coordinate with one another through shared network (IP) and storage.\n Workloads Pods are typically managed by higher level abstractions that handle concerns such as replication, identity, persistent storage, custom scheduling, rolling updates, etc.\nThe most common out-of-the-box Workload APIs (manage Pods) are:\n Deployments (Stateless Applications)  replication + rollouts   StatefulSets (Stateful Applications)  replication + rollouts + persistent storage + identity   Jobs (Batch Work)  run to completion   CronJobs (Scheduled Batch Work)  scheduled run to completion   DaemonSets (Per-Machine)  per-Node scheduling    API Abstraction Layers High-level Workload APIs may manage lower-level Workload APIs instead of directly managing Pods (e.g. Deployments manage ReplicaSets).  Service Discovery and Load Balancing Service discovery and Load Balancing may be managed by a Service object. Services provide a single virtual IP address and dns name load balanced to a collection of Pods matching Labels.\nInternal vs External Services  Services Resources (L4) may expose Pods internally within a cluster or externally through an HA proxy. Ingress Resources (L7) may expose URI endpoints and route them to Services.   Configuration and Secrets Shared Configuration and Secret data may be provided by ConfigMaps and Secrets. This allows Environment Variables, Command Line Arguments and Files to be loosely injected into the Pods and Containers that consume them.\nInternal vs External Services  ConfigMaps are for providing non-sensitive data to Pods. Secrets are for providing sensitive data to Pods.   ","excerpt":"TL;DR  A Kubernetes API has 2 parts - a Resource Type and a Controller Resources are objects …","ref":"/guides/introduction/resources_controllers/","title":"Resources + Controllers Overview"},{"body":" TL;DR  Generate Secrets from files and literals with secretGenerator Generate ConfigMaps from files and literals with configMapGenerator Rolling out changes to Secrets and ConfigMaps   Motivation The source of truth for Secret and ConfigMap Resources typically resides somewhere else, such as a .properties file. Apply offers native support for generating both Secrets and ConfigMaps from other sources such as files and literals.\nAdditionally, Secrets and ConfigMaps require rollouts to be performed differently than for most other Resources in order for the changes to be rolled out to Pods consuming them.\nGenerators Secret and ConfigMap Resources can be generated by adding secretGenerator or configMapGenerator entries to the kustomization.yaml file.\nThe generated Resources name\u0026rsquo;s will have suffixes that change when their data changes. See Rollouts for more on this.\nConfigMapsGenerator Command / Examples Check out the reference for commands and examples for ConfigMapsGenerator  Consider we have to generate ConfigMap from a preset values stored in .properties file. One can make use of the following kustomization.yaml file to do so.\n# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationconfigMapGenerator:- name:my-application-propertiesfiles:- application.properties# application.propertiesFOO=BarWe get a generated ConfigMap YAML as output:\napiVersion:v1data:application.properties:|-FOO=Barkind:ConfigMapmetadata:name:my-application-properties-f7mm6mhf59SecretGenerator Command / Examples Check out the reference for commands and examples for SecretGenerator  Rollouts ConfigMap values are consumed by Pods as: environment variables, command line arguments and files.\nThis is important because Updating a ConfigMap will:\n immediately update the files mounted by all Pods consuming them not update the environment variables or command line arguments until the Pod is restarted  Typically users want to perform a rolling update of the ConfigMap changes to Pods as soon as the ConfigMap changes are pushed.\nApply facilitates rolling updates for ConfigMaps by creating a new ConfigMap for each change to the data. Workloads (e.g. Deployments, StatefulSets, etc) are updated to point to a new ConfigMap instead of the old one. This allows the change to be gradually rolled the same way other Pod Template changes are rolled out.\nEach generated Resources name has a suffix appended by hashing the contents. This approach ensures a new ConfigMap is generated each time the contents is modified.\nNote: Because the Resource names will contain a suffix, when looking for them with kubectl get, their names will not match exactly what is specified in the kustomization.yaml file.\n","excerpt":"TL;DR  Generate Secrets from files and literals with secretGenerator Generate ConfigMaps from files …","ref":"/guides/config_management/secrets_configmaps/","title":"Secrets and ConfigMaps"},{"body":"适用于 Linux、MacOS 和 Windows 的各版本的二进制可执行文件可以在 releases 页面 上手动下载。\n下面的脚本会检测你的操作系统，并下载相应的 kustomize 二进制文件到你当前的工作目录中。\ncurl -s \u0026#34;https://raw.githubusercontent.com/\\ kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash ","excerpt":"适用于 Linux、MacOS 和 Windows 的各版本的二进制可执行文件可以在 releases 页面 上手动下载。\n下面的脚本会检测你的操作系统，并下载相应的 kustomize 二进制文件到 …","ref":"/zh/installation/binaries/","title":"可执行文件"},{"body":"","excerpt":"","ref":"/zh/api-reference/","title":"API 参考"},{"body":"choco install kustomize For support on the chocolatey package and prior releases, see:\n Choco Package Package Source  ","excerpt":"choco install kustomize For support on the chocolatey package and prior releases, see:\n Choco …","ref":"/installation/kustomize/chocolatey/","title":"Chocolatey"},{"body":"choco install kustomize 有关软件包管理器 chocolatey 的使用以及对之前版本的支持，请参考以下链接：\n Choco Package Package Source  ","excerpt":"choco install kustomize 有关软件包管理器 chocolatey 的使用以及对之前版本的支持，请参考以下链接：\n Choco Package Package Source  ","ref":"/zh/installation/chocolatey/","title":"Chocolatey"},{"body":"Please check out the existing components guide for explanation with examples. More examples are in progress\n","excerpt":"Please check out the existing components guide for explanation with examples. More examples are in …","ref":"/references/kustomize/components/","title":"components"},{"body":"","excerpt":"","ref":"/guides/container_debugging/","title":"Container Debugging"},{"body":" TL;DR  Override or set the Name and Tag for Container Images   Container Images Motivation It may be useful to define the tags or digests of container images which are used across many Workloads.\nContainer image tags and digests are used to refer to a specific version or instance of a container image - e.g. for the nginx container image you might use the tag 1.15.9 or 1.14.2.\n Update the container image name or tag for multiple Workloads at once Increase visibility of the versions of container images being used within the project Set the image tag from external sources - such as environment variables Copy or Fork an existing Project and change the Image Tag for a container Change the registry used for an image  Consider the following deployment.yaml file,\n# deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:template:spec:containers:- name:mypostgresdbimage:postgres:8- name:nginxappimage:nginx:1.7.9- name:myappimage:my-demo-app:latest- name:alpine-appimage:alpine:3.7the image tag under containers specified the image that has to be pulled from the container registry.\nSome of things that can be done with images:\n Setting a Name Setting a Tag Setting a Digest Setting a Tag from the latest commit SHA Setting a Tag from an Environment Variable  Command / Examples Check out the reference for commands and examples for images  ","excerpt":"TL;DR  Override or set the Name and Tag for Container Images   Container Images Motivation It may be …","ref":"/guides/config_management/container_images/","title":"Container Images"},{"body":" TL;DR  Print verbose debug information about a Resource   Describe Resources Motivation Describe is a higher level printing operation that may aggregate data from other sources in addition to the Resource being queried (e.g. Events).\nDescribe pulls out the most important information about a Resource from the Resource itself and related Resources, and formats and prints this information on multiple lines.\n Aggregates data from related Resources Formats Verbose Output for debugging  kubectl describe deployments Name: nginx Namespace: default CreationTimestamp: Thu, 15 Nov 2018 10:58:03 -0800 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=1 Selector: app=nginx Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-78f5d695bd (1/1 replicas created) Events: \u0026lt;none\u0026gt;  Get vs Describe When Describing a Resource, it may aggregate information from several other Resources. For instance Describing a Node will aggregate Pod Resources to print the Node utilization.\nWhen Getting a Resource, it will only print information available from reading that Resource. While Get may aggregate data from the fields of that Resource, it won\u0026rsquo;t look at fields from other Resources.\n ","excerpt":"TL;DR  Print verbose debug information about a Resource   Describe Resources Motivation Describe is …","ref":"/guides/resource_printing/describe/","title":"Describe"},{"body":"本教程只是一个快速开始的示例，完整的插件文档请看：kustomize 插件\n请务必阅读 Go 插件注意事项。\n该示例使用 Go 插件 SopsEncodedSecrets，该插件位于 sopsencodedsecrets repository中。这是一个进程内的 Go 插件，而不是恰巧用 Go 编写的 exec 插件（这是 Go 作者的另一种选择）。\n尝试本教程不会破坏你的当前设置。\n环境要求  linux git curl Go 1.13  用于加密\n gpg  或\n Google cloud (gcloud) 安装 具有 KMS 权限的 Google帐户  创建一个工作空间/目录 # 将这些目录分开，以免造成 DEMO 目录的混乱。 DEMO=$(mktemp -d) tmpGoPath=$(mktemp -d) 安装 kustomize 需要安装 kustomize v3.0.0，并且必须对其进行 编译（而不是从 release 页面下载二进制文件）：\nGOPATH=$tmpGoPath go install sigs.k8s.io/kustomize/kustomize 为插件创建目录 kustomize 插件完全由其配置文件和源代码确定。\nKustomize 插件的配置文件的格式与 kubernetes 资源对象相同，这就意味着在配置文件中 apiVersion，kind 和 metadata 都是必须的字段。\n因为配置文件名出现在 kustomization 文件的 generatorsor 或 transformers 字段中，kustomize 会读取配置文件，然后在以下位置找到 Go 插件的目标代码：\n $XDG_CONFIG_HOME/kustomize/plugin/$apiVersion/$lKind/$kind.so  lKind 必须是小写字母的，然后将插件加载并提供其配置，插件的输出将成为整个 kustomize build 程序的一部分 。\n同一插件在一个 kustomize 构建中可能会多次使用不同的配置文件。此外，kustomize 可能会先自定义 config 数据，然后再发送给插件。由于这些原因，插件不能自己去读取配置文件，而需要通过 kustomize 来读取配置。\n该示例将在如下临时目录中存放其使用的插件：\nPLUGIN_ROOT=$DEMO/kustomize/plugin 并在下面的命令行中临时设置 XDG_CONFIG_HOME。\n使用什么 apiVersion 和 kind 在 kustomize 插件的开发时，插件代码不关心也不知道配置文件中的 apiVersion 或 kind。\n插件会检查这些字段，但是剩下的字段提供了实际的配置数据，在这一点上，成功解析其他字段对于插件很重要。\n本示例使用一个名为 SopsEncodedSecrets 的插件，其位于 SopsEncodedSecrets repository 中。\n我们选择安装插件到\napiVersion=mygenerators kind=SopsEncodedSecrets 定义插件的主目录 按照惯例，存放插件代码和补充数据，测试，文档等的目录名称必须是 kind 的小写形式。\nlKind=$(echo $kind | awk \u0026#39;{print tolower($0)}\u0026#39;) 下载 SopsEncodedSecrets 插件 在这种情况下，存储库名称已经与小写字母的 kind 匹配，因此我们只需克隆存储库并自动获取正确的目录名称即可：\nmkdir -p $PLUGIN_ROOT/${apiVersion} cd $PLUGIN_ROOT/${apiVersion} git clone git@github.com:monopole/sopsencodedsecrets.git 记住这个目录：\nMY_PLUGIN_DIR=$PLUGIN_ROOT/${apiVersion}/${lKind} 尝试测试插件 插件可能会自己带有测试文件。因此可以通过如下方式：\ncd $MY_PLUGIN_DIR go test SopsEncodedSecrets_test.go 构建对象代码以供 kustomize 使用：\ncd $MY_PLUGIN_DIR GOPATH=$tmpGoPath go build -buildmode plugin -o ${kind}.so ${kind}.go 此步骤可能会成功，但是由于依赖关系 skew，kustomize 最终可能无法加载该插件。\n在加载失败时\n  确保使用相同版本的Go (go1.13)，在相同的 $GOOS(linux)和 $GOARCH(amd64) 上构建插件，用于构建本演示中使用的 kustomize。\n  修改插件中的依赖文件 go.mod 以匹配 kustomize 使用的版本。\n  缺乏工具和元数据来实现自动化，就不会有一个完整的 Go 插件生态。\nKustomize 采用了 Go 插件架构，可以轻松的接受新的生成器和转换器（只需编写一个插件），并确保本机操作（也已作为插件构建和测试）是分段的、可排序的和可重用的，而不是奇怪的插入在整体代码中。\n编写 kustomization 新建一个 kustomization 目录存放你的配置：\nMYAPP=$DEMO/myapp mkdir -p $MYAPP 为 SopsEncodedSecrets 插件编写一个配置文件。\n插件可以通过 apiVersion 和 kind 找到：\ncat \u0026lt;\u0026lt;EOF \u0026gt;$MYAPP/secGenerator.yaml apiVersion: ${apiVersion} kind: ${kind} metadata: name: mySecretGenerator name: forbiddenValues namespace: production file: myEncryptedData.yaml keys: - ROCKET - CAR EOF 插件可以在 myEncryptedData.yaml 中找到更多的数据。\n编写一个引用插件配置的 kustomization 文件：\ncat \u0026lt;\u0026lt;EOF \u0026gt;$MYAPP/kustomization.yaml commonLabels: app: hello generators: - secGenerator.yaml EOF 接下来生成真实的加密数据。\n确保您已安装加密工具 我们将使用 sops 对文件进行编码。选择 GPG 或 Google Cloud KMS 作为加密提供者以继续。\nGPG 尝试这个命令：\ngpg --list-keys 如果返回 list，则您已经成功创建了密钥。如果不是，请尝试从 sops 导入测试密钥。\ncurl https://raw.githubusercontent.com/mozilla/sops/master/pgp/sops_functional_tests_key.asc | gpg --import SOPS_PGP_FP=\u0026#34;1022470DE3F0BC54BC6AB62DE05550BC07FB1A0A\u0026#34; Google Cloude KMS 尝试这个命令：\ngcloud kms keys list --location global --keyring sops 如果成功了，想必你已经创建了密钥，并将其放置在一个名为 sops 的钥匙圈中。如果没有，那就这样做：\ngcloud kms keyrings create sops --location global gcloud kms keys create sops-key --location global \\  --keyring sops --purpose encryption 通过如下方法，获取你的 keyLocation：\nkeyLocation=$(\\  gcloud kms keys list --location global --keyring sops |\\  grep GOOGLE | cut -d \u0026#34; \u0026#34; -f1) echo $keyLocation 安装 sops GOPATH=$tmpGoPath go install go.mozilla.org/sops/cmd/sops 用你的私钥创建加密数据 创建需要加密的原始数据：\ncat \u0026lt;\u0026lt;EOF \u0026gt;$MYAPP/myClearData.yaml VEGETABLE: carrot ROCKET: saturn-v FRUIT: apple CAR: dymaxion EOF 将数据加密插入到插件要读取的文件中：\n使用 PGP\n$tmpGoPath/bin/sops --encrypt \\  --pgp $SOPS_PGP_FP \\  $MYAPP/myClearData.yaml \u0026gt;$MYAPP/myEncryptedData.yaml 或者使用 GCP KMS\n$tmpGoPath/bin/sops --encrypt \\  --gcp-kms $keyLocation \\  $MYAPP/myClearData.yaml \u0026gt;$MYAPP/myEncryptedData.yaml 查看文件\ntree $DEMO 结果如下：\n /tmp/tmp.0kIE9VclPt ├── kustomize │ └── plugin │ └── mygenerators │ └── sopsencodedsecrets │ ├── go.mod │ ├── go.sum │ ├── LICENSE │ ├── README.md │ ├── SopsEncodedSecrets.go │ ├── SopsEncodedSecrets.so │ └── SopsEncodedSecrets_test.go └── myapp ├── kustomization.yaml ├── myClearData.yaml ├── myEncryptedData.yaml └── secGenerator.yaml  使用插件构建您的应用 XDG_CONFIG_HOME=$DEMO $tmpGoPath/bin/kustomize build --enable_alpha_plugins $MYAPP 这将生成一个 kubernetes secret，并对名称 ROCKET 和 CAR 的数据进行加密。\n之前如果您已经设置了 PLUGIN_ROOT=$HOME/.config/kustomize/plugin，则无需在 kustomize 命令前使用 XDG_CONFIG_HOME。\n","excerpt":"本教程只是一个快速开始的示例，完整的插件文档请看：kustomize 插件\n请务必阅读 Go 插件注意事项。\n该示例使用 Go 插件 SopsEncodedSecrets， …","ref":"/zh/guides/plugins/gopluginguidedexample/","title":"Go 插件示例"},{"body":"","excerpt":"","ref":"/references/kubectl/get/options/","title":"options"},{"body":" TL;DR  Port Forward local connections to Pods running in a cluster   Port Forward Motivation Connect to ports of Pods running a cluster by port forwarding local ports.\nForward Multiple Ports Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in the pod\nkubectl port-forward pod/mypod 5000 6000  Operations One can also perfrom operations such as, Port Forward to:\n Pod in a Workload Different Local and Remote Ports Random Local Port   Command / Examples Check out the reference for commands and examples of port forwarding  ","excerpt":" TL;DR  Port Forward local connections to Pods running in a cluster   Port Forward Motivation …","ref":"/guides/container_debugging/port_forward_to_pods/","title":"Port forward to Pods"},{"body":"Print out all labels on each Resource in a single column (last).\nCommand kubectl get deployment --show-labels Output NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE LABELS nginx 1 1 1 1 7m app=nginx ","excerpt":"Print out all labels on each Resource in a single column (last).\nCommand kubectl get deployment …","ref":"/references/kubectl/get/options/show_labels/","title":"show labels"},{"body":"","excerpt":"","ref":"/guides/app_deployment/","title":"App Deployment"},{"body":"Each entry in this list results in the creation of one ConfigMap resource (it\u0026rsquo;s a generator of n maps).\nThe example below creates four ConfigMaps:\n first, with the names and contents of the given files second, with key/value as data using key/value pairs from files third, also with key/value as data, directly specified using literals and a fourth, which sets an annotation and label via options for that single ConfigMap  Each configMapGenerator item accepts a parameter of behavior: [create|replace|merge]. This allows an overlay to modify or replace an existing configMap from the parent.\nAlso, each entry has an options field, that has the same subfields as the kustomization file\u0026rsquo;s generatorOptions field.\nThis options field allows one to add labels and/or annotations to the generated instance, or to individually disable the name suffix hash for that instance. Labels and annotations added here will not be overwritten by the global options associated with the kustomization file generatorOptions field. However, due to how booleans behave, if the global generatorOptions field specifies disableNameSuffixHash: true, this will trump any attempt to locally override it.\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomization# These labels are added to all configmaps and secrets.generatorOptions:labels:fruit:appleconfigMapGenerator:- name:my-java-server-propsbehavior:mergefiles:- application.properties- more.properties- name:my-java-server-env-file-varsenvs:- my-server-env.properties- more-server-props.env- name:my-java-server-env-varsliterals:- JAVA_HOME=/opt/java/jdk- JAVA_TOOL_OPTIONS=-agentlib:hprofoptions:disableNameSuffixHash:truelabels:pet:dog- name:dashboardsfiles:- mydashboard.jsonoptions:annotations:dashboard:\u0026#34;1\u0026#34;labels:app.kubernetes.io/name:\u0026#34;app1\u0026#34;It is also possible to define a key to set a name different than the filename.\nThe example below creates a ConfigMap with the name of file as myFileName.ini while the actual filename from which the configmap is created is whatever.ini.\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationconfigMapGenerator:- name:app-whateverfiles:- myFileName.ini=whatever.iniConfigMap from File ConfigMap Resources may be generated from files - such as a java .properties file. To generate a ConfigMap Resource for a file, add an entry to configMapGenerator with the filename.\nExample: Generate a ConfigMap with a data item containing the contents of a file.\nThe ConfigMaps will have data values populated from the file contents. The contents of each file will appear as a single data item in the ConfigMap keyed by the filename.\nThe example illustrates how you can create ConfigMaps from File using Generators.\nFile Input # kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationconfigMapGenerator:- name:my-application-propertiesfiles:- application.properties# application.propertiesFOO=BarBuild Output apiVersion:v1data:application.properties:|-FOO=Barkind:ConfigMapmetadata:name:my-application-properties-f7mm6mhf59ConfigMap from Literals ConfigMap Resources may be generated from literal key-value pairs - such as JAVA_HOME=/opt/java/jdk. To generate a ConfigMap Resource from literal key-value pairs, add an entry to configMapGenerator with a list of literals.\nLiteral Syntax  The key/value are separated by a = sign (left side is the key) The value of each literal will appear as a data item in the ConfigMap keyed by its key.   Example: Create a ConfigMap with 2 data items generated from literals.\nThe example illustrates how you can create ConfigMaps from Literals using Generators.\nFile Input # kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationconfigMapGenerator:- name:my-java-server-env-varsliterals:- JAVA_HOME=/opt/java/jdk- JAVA_TOOL_OPTIONS=-agentlib:hprofBuild Output apiVersion:v1data:JAVA_HOME:/opt/java/jdkJAVA_TOOL_OPTIONS:-agentlib:hprofkind:ConfigMapmetadata:name:my-java-server-env-vars-44k658k8gkConfigMap from env file ConfigMap Resources may be generated from key-value pairs much the same as using the literals option but taking the key-value pairs from an environment file. These generally end in .env. To generate a ConfigMap Resource from an environment file, add an entry to configMapGenerator with a single env entry, e.g. env: config.env.\nEnvironment File Syntax  The key/value pairs inside of the environment file are separated by a = sign (left side is the key) The value of each line will appear as a data item in the ConfigMap keyed by its key.   Example: Create a ConfigMap with 3 data items generated from an environment file.\nFile Input # kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationconfigMapGenerator:- name:tracing-optionsenv:tracing.env# tracing.env ENABLE_TRACING=true SAMPLER_TYPE=probabilistic SAMPLER_PARAMETERS=0.1 Build Output apiVersion:v1kind:ConfigMapmetadata:# The name has had a suffix appliedname:tracing-options-6bh8gkdf7k# The data has been populated from each literal pairdata:ENABLE_TRACING:\u0026#34;true\u0026#34;SAMPLER_TYPE:\u0026#34;probabilistic\u0026#34;SAMPLER_PARAMETERS:\u0026#34;0.1\u0026#34; Overriding Base ConfigMap Values ConfigMaps Values from Bases may be overridden by adding another generator for the ConfigMap in the Variant and specifying the behavior field. behavior may be one of create (default value), replace (replace the base ConfigMap), or merge (add or update the values the ConfigMap). See Bases and Variantions for more on using Bases. e.g. behavior: \u0026quot;merge\u0026quot;  Propagating the Name Suffix Letting ConfigMap or Secret know the name of the generated Resource name suffix\nWorkloads that reference the ConfigMap or Secret will need to know the name of the generated Resource including the suffix, however Apply takes care of this automatically for users. Apply will identify references to generated ConfigMaps and Secrets, and update them.\nThe generated ConfigMap name will be my-java-server-env-vars with a suffix unique to its contents. Changes to the contents will change the name suffix, resulting in the creation of a new ConfigMap, and transform Workloads to point to this one.\nThe PodTemplate volume references the ConfigMap by the name specified in the generator (excluding the suffix). Apply will update the name to include the suffix applied to the ConfigMap name.\nInput: The kustomization.yaml and deployment.yaml files\n# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationconfigMapGenerator:- name:my-java-server-env-varsliterals:- JAVA_HOME=/opt/java/jdk- JAVA_TOOL_OPTIONS=-agentlib:hprofresources:- deployment.yaml# deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:test-deploymentlabels:app:testspec:selector:matchLabels:app:testtemplate:metadata:labels:app:testspec:containers:- name:containerimage:k8s.gcr.io/busyboxcommand:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;ls /etc/config/\u0026#34;]volumeMounts:- name:config-volumemountPath:/etc/configvolumes:- name:config-volumeconfigMap:name:my-java-server-env-varsApplied: The Resources that are Applied to the cluster.\napiVersion:v1kind:ConfigMapmetadata:# The name has been updated to include the suffixname:my-java-server-env-vars-k44mhd6h5fdata:JAVA_HOME:/opt/java/jdkJAVA_TOOL_OPTIONS:-agentlib:hprof---apiVersion:apps/v1kind:Deploymentmetadata:labels:app:testname:test-deploymentspec:selector:matchLabels:app:testtemplate:metadata:labels:app:testspec:containers:- command:- /bin/sh- -c- ls /etc/config/image:k8s.gcr.io/busyboxname:containervolumeMounts:- mountPath:/etc/configname:config-volumevolumes:- configMap:# The name has been updated to include the# suffix matching the ConfigMapname:my-java-server-env-vars-k44mhd6h5fname:config-volume","excerpt":"Each entry in this list results in the creation of one ConfigMap resource (it\u0026rsquo;s a generator of …","ref":"/references/kustomize/configmapgenerator/","title":"configMapGenerator"},{"body":" TL;DR  Set the Namespace for all Resources within a Project with namespace Prefix the Names of all Resources within a Project with namePrefix Suffix the Names of all Resources within a Project with nameSuffix   Setting Namespaces and Names Motivation It may be useful to enforce consistency across the namespace and names of all Resources within a Project.\n Ensure all Resources are in the correct Namespace Ensure all Resources share a common naming convention Copy or Fork an existing Project and change the Namespace / Names  Setting Namespace Reference:\nThe Namespace for all namespaced Resources declared in the Resource Config may be set with namespace. This sets the namespace for both generated Resources (e.g. ConfigMaps and Secrets) and non-generated Resources.\nCommand / Examples Check out the reference for commands and examples for setting namespace  Example: Set the namespace specified in the kustomization.yaml on the namespaced Resources.\nInput: The kustomization.yaml and deployment.yaml files\n# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationnamespace:my-namespaceresources:- deployment.yaml# deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentlabels:app:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginxApplied: The Resource that is Applied to the cluster\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginxname:nginx-deployment# The namespace has been addednamespace:my-namespacespec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- image:nginxname:nginxOverriding Namespaces: Setting the namespace will override the namespace on Resources if it is already set.\nCommand / Examples Check out the nameprefix / namesuffix for commands and examples for setting nameprefix / namesuffix to kubernetes resources  Propagation of the Name to Object References Resources such as Deployments and StatefulSets may reference other Resources such as ConfigMaps and Secrets in the Pod Spec.\nThis sets a name prefix or suffix for both generated Resources (e.g. ConfigMaps and Secrets) and non-generated Resources.\nThe namePrefix or nameSuffix that is applied is propagated to references to updated resources\n e.g. references to Secrets and ConfigMaps are updated with the namePrefix and nameSuffix.  References Apply will propagate the namePrefix to any place Resources within the project are referenced by other Resources including:\n Service references from StatefulSets ConfigMap references from PodSpecs Secret references from PodSpecs   ","excerpt":"TL;DR  Set the Namespace for all Resources within a Project with namespace Prefix the Names of all …","ref":"/guides/config_management/namespaces_names/","title":"Namespaces and Names"},{"body":" TL;DR  Proxy local connections to Services running in the cluster   Connecting to Services Motivation Not all Services running a Kubernetes cluster are exposed externally. However Services only exposed internally to a cluster with a clusterIp are accessible through an apiserver proxy.\nUsers may use Proxy to connect to Kubernetes Services in a cluster that are not externally exposed.\nNote: Services running a type LoadBalancer or type NodePort may be exposed externally and accessed without the need for a Proxy.\nConnecting to an internal Service Connect to a internal Service using the Proxy command, and the Service Proxy url.\nTo visit the nginx service go to the Proxy URL at http://127.0.0.1:8001/api/v1/namespaces/default/services/nginx/proxy/\nkubectl proxy Starting to serve on 127.0.0.1:8001 curl http://127.0.0.1:8001/api/v1/namespaces/default/services/nginx/proxy/  Literal Syntax To connect to a Service through a proxy the user must build the Proxy URL. The Proxy URL format is:\nhttp://\u0026lt;apiserver-address\u0026gt;/api/v1/namespaces/\u0026lt;service-namespace\u0026gt;/services/[https:]\u0026lt;service-name\u0026gt;[:\u0026lt;port-name\u0026gt;]/proxy  The apiserver-address should be the URL printed by the Proxy command The Port is optional if you haven’t specified a name for your port The Protocol is optional if you are using http   Builtin Cluster Services A common usecase is to connect to Services running as part of the cluster itself. A user can print out these Services and their Proxy Urls with kubectl cluster-info.\nkubectl cluster-info Kubernetes master is running at https://104.197.5.247 GLBCDefaultBackend is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy Heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/heapster/proxy KubeDNS is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy  More Info For more information on connecting to a cluster, see Accessing Clusters.  ","excerpt":"TL;DR  Proxy local connections to Services running in the cluster   Connecting to Services …","ref":"/guides/container_debugging/proxying_traffic_to_services/","title":"Proxying traffic to services"},{"body":" TL;DR  Queries for Getting or Describing Resources   Matching Objects from Get and Describing Motivation Match Resources with Queries when Getting or Describing them.\nResource Config By kustomization.yaml Get all Resources provided by the kustomization.yaml in project/.\nkubectl get -k project/ Resource Config By Dir Get all Resources present in the Resource Config for a directory.\nkubectl get -f configs/ Resource Types Get all Resources in a namespace for a given type.\nThe Group and Version for the Resource are determined by the apiserver discovery service.\nThe Singular, Plural, Short Name also apply to Types with Name and Types with Selectors.\n# Plural kubectl get deployments # Singular kubectl get deployment # Short name kubectl get deploy Resource Types with Group / Version Get all Resources in a namespace for a given type.\nThe Group and Version for the Resource are explicit.\nkubectl get deployments.apps kubectl get deployments.v1.apps Resource Types with Name Get named Resources in a namespace for a given type.\nkubectl get deployment nginx Label Selector Get all Resources in a namespace matching a label select for a given type.\nkubectl get deployments -l app=nginx Namespaces By default Get and Describe will fetch resource in the default namespace or the namespace specified with --namespace.\nThe --all-namespaces flag will fetch Resources from all namespaces.\nkubectl get deployments --all-namespaces List multiple Resource types Get and Describe can accept multiple Resource types, and it will print them both in separate sections.\nkubectl get deployments,services List multiple Resource types by name Get and Describe can accept multiple Resource types and names.\nkubectl get rc/web service/frontend pods/web-pod-13je7 Not Found By default, Get or Describe will return an error if an object is requested and doesn\u0026rsquo;t exist. The --ignore-not-found flag will cause kubectl to exit 0 if the Resource is not found\nkubectl get deployment nginx --ignore-not-found ","excerpt":"TL;DR  Queries for Getting or Describing Resources   Matching Objects from Get and Describing …","ref":"/guides/resource_printing/queries_and_options/","title":"Queries and Options"},{"body":"Print out the Group.Kind as part of the Name column.\nNote: This can be useful if the user did not specify the group in the command and they want to know which API is being used.\nCommand kubectl get deployments --show-kind Output NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.extensions/nginx 1 1 1 1 8m ","excerpt":"Print out the Group.Kind as part of the Name column.\nNote: This can be useful if the user did not …","ref":"/references/kubectl/get/options/show_kind/","title":"show kind"},{"body":"Each entry in this list should be a relative path to a file for custom resource definition (CRD).\nThe presence of this field is to allow kustomize be aware of CRDs and apply proper transformation for any objects in those types.\nTypical use case: A CRD object refers to a ConfigMap object. In a kustomization, the ConfigMap object name may change by adding namePrefix, nameSuffix, or hashing. The name reference for this ConfigMap object in CRD object need to be updated with namePrefix, nameSuffix, or hashing in the same way.\nThe annotations can be put into openAPI definitions are:\n \u0026ldquo;x-kubernetes-annotation\u0026rdquo;: \u0026quot;\u0026quot; \u0026ldquo;x-kubernetes-label-selector\u0026rdquo;: \u0026quot;\u0026quot; \u0026ldquo;x-kubernetes-identity\u0026rdquo;: \u0026quot;\u0026quot; \u0026ldquo;x-kubernetes-object-ref-api-version\u0026rdquo;: \u0026ldquo;v1\u0026rdquo;, \u0026ldquo;x-kubernetes-object-ref-kind\u0026rdquo;: \u0026ldquo;Secret\u0026rdquo;, \u0026ldquo;x-kubernetes-object-ref-name-key\u0026rdquo;: \u0026ldquo;name\u0026rdquo;,  apiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationcrds:- crds/typeA.yaml- crds/typeB.yaml","excerpt":"Each entry in this list should be a relative path to a file for custom resource definition (CRD). …","ref":"/references/kustomize/crds/","title":"crds"},{"body":"","excerpt":"","ref":"/guides/extending_kubectl/","title":"Extending Kubectl"},{"body":"Kustomize offers a plugin framework allowing people to write their own resource generators and transformers.\nWrite a plugin when changing generator options or transformer configs doesn\u0026rsquo;t meet your needs.\n  A generator plugin could be a helm chart inflator, or a plugin that emits all the components (deployment, service, scaler, ingress, etc.) needed by someone\u0026rsquo;s 12-factor application, based on a smaller number of free variables.\n  A transformer plugin might perform special container command line edits, or any other transformation beyond those provided by the builtin (namePrefix, commonLabels, etc.) transformers.\n  Specification in kustomization.yaml Start by adding a generators and/or transformers field to your kustomization.\nEach field accepts a string list:\n generators: - relative/path/to/some/file.yaml - relative/path/to/some/kustomization - /absolute/path/to/some/kustomization - https://github.com/org/repo/some/kustomization transformers: - {as above}  The value of each entry in a generators or transformers list must be a relative path to a YAML file, or a path or URL to a kustomization. This is the same format as demanded by the resources field.\nYAML files are read from disk directly. Paths or URLs leading to kustomizations trigger an in-process kustomization run. Each of the resulting objects is now further interpreted by kustomize as a plugin configuration object.\nConfiguration A kustomization file could have the following lines:\ngenerators: - chartInflator.yaml Given this, the kustomization process would expect to find a file called chartInflator.yaml in the kustomization root.\nThis is the plugin\u0026rsquo;s configuration file; it contains a YAML configuration object.\nThe file chartInflator.yaml could contain:\napiVersion: someteam.example.com/v1 kind: ChartInflator metadata: name: notImportantHere chartName: minecraft The apiVersion and kind fields are used to locate the plugin.\nThus, these fields are required. They are also required because a kustomize plugin configuration object is also a k8s object.\nTo get the plugin ready to generate or transform, it is given the entire contents of the configuration file.\nFor more examples of plugin configuration YAML, browse the unit tests below the plugins root, e.g. the tests for ChartInflator or NameTransformer.\nPlacement Each plugin gets its own dedicated directory named\n$XDG_CONFIG_HOME/kustomize/plugin /${apiVersion}/LOWERCASE(${kind}) The default value of XDG_CONFIG_HOME is $HOME/.config.\nThe one-plugin-per-directory requirement eases creation of a plugin bundle (source, tests, plugin data files, etc.) for sharing.\nIn the case of a Go plugin, it also allows one to provide a go.mod file for the single plugin, easing resolution of package version dependency skew.\nWhen loading, kustomize will first look for an executable file called\n$XDG_CONFIG_HOME/kustomize/plugin /${apiVersion}/LOWERCASE(${kind})/${kind} If this file is not found or is not executable, kustomize will look for a file called ${kind}.so in the same directory and attempt to load it as a Go plugin.\nIf both checks fail, the plugin load fails the overall kustomize build.\nExecution Plugins are only used during a run of the kustomize build command.\nGenerator plugins are run after processing the resources field (which itself can be viewed as a generator, simply reading objects from disk).\nThe full set of resources is then passed into the transformation pipeline, wherein builtin transformations like namePrefix and commonLabel are applied (if they were specified in the kustomization file), followed by the user-specified transformers in the transformers field.\nThe order specified in the transformers field is respected, as transformers cannot be expected to be commutative.\nNo Security Kustomize plugins do not run in any kind of kustomize-provided sandbox. There\u0026rsquo;s no notion of \u0026ldquo;plugin security\u0026rdquo;.\nA kustomize build that tries to use plugins but omits the flag\n --enable_alpha_plugins\n will not load plugins and will fail with a warning about plugin use.\nThe use of this flag is an opt-in acknowledging the unstable (alpha) plugin API, the absence of plugin provenance, and the fact that a plugin is not part of kustomize.\nTo be clear, some kustomize plugin downloaded from the internet might wonderfully transform k8s config in a desired manner, while also quietly doing anything the user could do to the system running kustomize build.\nAuthoring There are two kinds of plugins, exec and Go.\nExec plugins A exec plugin is any executable that accepts a single argument on its command line - the name of a YAML file containing its configuration (the file name provided in the kustomization file).\n TODO: restrictions on plugin to allow the same exec plugin to be targeted by both the generators and transformers fields.\n first arg could be the fixed string generate or transform, (the name of the configuration file moves to the 2nd arg), or or by default an exec plugin behaves as a tranformer unless a flag -g is provided, switching the exec plugin to behave as a generator.   Examples  helm chart inflator - A generator that inflates a helm chart. bashed config map - Super simple configMap generation from bash. sed transformer - Define your unstructured edits using a plugin like this one. hashicorp go-getter - Download kustomize layes and build it to generate resources  A generator plugin accepts nothing on stdin, but emits generated resources to stdout.\nA transformer plugin accepts resource YAML on stdin, and emits those resources, presumably transformed, to stdout.\nkustomize uses an exec plugin adapter to provide marshalled resources on stdin and capture stdout for further processing.\nGenerator Options A generator exec plugin can adjust the generator options for the resources it emits by setting one of the following internal annotations.\n NOTE: These annotations are local to kustomize and will not be included in the final output.\n kustomize.config.k8s.io/needs-hash\nResources can be marked as needing to be processed by the internal hash transformer by including the needs-hash annotation. When set valid values for the annotation are \u0026quot;true\u0026quot; and \u0026quot;false\u0026quot; which respectively enable or disable hash suffixing for the resource. Omitting the annotation is equivalent to setting the value \u0026quot;false\u0026quot;.\nHashes are determined as follows:\n For ConfigMap resources, hashes are based on the values of the name, data, and binaryData fields. For Secret resources, hashes are based on the values of the name, type, data, and stringData fields. For any other object type, hashes are based on the entire object content (i.e. all fields).  Example:\napiVersion:v1kind:ConfigMapmetadata:name:cm-testannotations:kustomize.config.k8s.io/needs-hash:\u0026#34;true\u0026#34;data:foo:barkustomize.config.k8s.io/behavior\nThe behavior annotation will influence how conflicts are handled for resources emitted by the plugin. Valid values include \u0026ldquo;create\u0026rdquo;, \u0026ldquo;merge\u0026rdquo;, and \u0026ldquo;replace\u0026rdquo; with \u0026ldquo;create\u0026rdquo; being the default.\nExample:\napiVersion:v1kind:ConfigMapmetadata:name:cm-testannotations:kustomize.config.k8s.io/behavior:\u0026#34;merge\u0026#34;data:foo:barGo plugins Be sure to read Go plugin caveats.\nA .go file can be a Go plugin if it declares \u0026lsquo;main\u0026rsquo; as it\u0026rsquo;s package, and exports a symbol to which useful functions are attached.\nIt can further be used as a kustomize plugin if the symbol is named \u0026lsquo;KustomizePlugin\u0026rsquo; and the attached functions implement the Configurable, Generator and Transformer interfaces.\nA Go plugin for kustomize looks like this:\n package main import ( \u0026#34;sigs.k8s.io/kustomize/api/resmap\u0026#34; ... ) type plugin struct {...} var KustomizePlugin plugin func (p *plugin) Config( h *resmap.PluginHelpers, c []byte) error {...} func (p *plugin) Generate() (resmap.ResMap, error) {...} func (p *plugin) Transform(m resmap.ResMap) error {...}  Use of the identifiers plugin, KustomizePlugin and implementation of the method signature Config is required.\nImplementing the Generator or Transformer method allows (respectively) the plugin\u0026rsquo;s config file to be added to the generators or transformers field in the kustomization file. Do one or the other or both as desired.\nExamples  service generator - generate a service from a name and port argument. string prefixer - uses the value in metadata/name as the prefix. This particular example exists to show how a plugin can transform the behavior of a plugin. See the TestTransformedTransformers test in the target package. date prefixer - prefix the current date to resource names, a simple example used to modify the string prefixer plugin just mentioned. secret generator - generate secrets from a toy database. sops encoded secrets - a more complex secret generator that converts SOPS files into Kubernetes Secrets SOPSGenerator - another generator that decrypts SOPS files into Secrets All the builtin plugins. User authored plugins are on the same footing as builtin operations.  A Go plugin can be both a generator and a transformer. The Generate method will run along with all the other generators before the Transform method runs.\nHere\u0026rsquo;s a build command that sensibly assumes the plugin source code sits in the directory where kustomize expects to find .so files:\nd=$XDG_CONFIG_HOME/kustomize/plugin\\ /${apiVersion}/LOWERCASE(${kind}) go build -buildmode plugin \\  -o $d/${kind}.so $d/${kind}.go ","excerpt":"Kustomize offers a plugin framework allowing people to write their own resource generators and …","ref":"/guides/extending_kustomize/","title":"Extending Kustomize"},{"body":" TL;DR  Set Labels for all Resources declared within a Project with commonLabels Set Annotations for all Resources declared within a Project with commonAnnotations   Setting Labels and Annotations Motivation Users may want to define a common set of labels or annotations for all the Resource in a project.\n Identify the Resources within a project by querying their labels. Set metadata for all Resources within a project (e.g. environment=test). Copy or Fork an existing Project and add or change labels and annotations.  Setting Labels Example: Add the labels declared in commonLabels to all Resources in the project.\nImportant: Once set, commonLabels should not be changed so as not to change the Selectors for Services or Workloads.\nInput: The kustomization.yaml and deployment.yaml files\n# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationcommonLabels:app:fooenvironment:testresources:- deployment.yaml# deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentlabels:app:nginxbar:bazspec:selector:matchLabels:app:nginxbar:baztemplate:metadata:labels:app:nginxbar:bazspec:containers:- name:nginximage:nginxApplied: The Resource that is Applied to the cluster\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:foo# Label was changedenvironment:test# Label was addedbar:baz# Label was ignoredname:nginx-deploymentspec:selector:matchLabels:app:foo# Selector was changedenvironment:test# Selector was addedbar:baz# Selector was ignoredtemplate:metadata:labels:app:foo# Label was changedenvironment:test# Label was addedbar:baz# Label was ignoredspec:containers:- image:nginxname:nginx Command / Examples Check out the reference for commands and examples for setting labels  Propagating Labels to Selectors In addition to updating the labels for each Resource, any selectors will also be updated to target the labels. e.g. the selectors for Services in the project will be updated to include the commonLabels in addition to the other labels.\nNote: Once set, commonLabels should not be changed so as not to change the Selectors for Services or Workloads.\nCommon Labels The k8s.io documentation defines a set of Common Labeling Conventions that may be applied to Applications.\nNote: commonLabels should only be set for immutable labels, since they will be applied to Selectors.\nLabeling Workload Resources makes it simpler to query Pods - e.g. for the purpose of getting their logs.\n Setting Annotations Setting Annotations is very similar to setting labels as seen above. Check out the reference for commands and examples.\nPropagating Annotations In addition to updating the annotations for each Resource, any fields that contain ObjectMeta (e.g. PodTemplate) will also have the annotations added.  ","excerpt":"TL;DR  Set Labels for all Resources declared within a Project with commonLabels Set Annotations for …","ref":"/guides/config_management/labels_annotations/","title":"Labels and Annotations"},{"body":"Continuously Watch and print Resources as they change\nPrint Resources as they are updated.\nIt is possible to have kubectl get continuously watch for changes to objects, and print the objects when they are changed or when the watch is reestablished.\nCommand kubectl get deployments --watch Output NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 1 1 1 1 6h nginx2 1 1 1 1 21m  Watch Timeouts Watch timesout after 5 minutes, after which kubectl will re-establish the watch and print the resources.  It is possible to have kubectl get continuously watch for changes to objects without fetching them first using the --watch-only flag.\nCommand kubectl get deployments --watch-only ","excerpt":"Continuously Watch and print Resources as they change\nPrint Resources as they are updated.\nIt is …","ref":"/references/kubectl/get/options/watch/","title":"watch"},{"body":" TL;DR  Print information about the Cluster and Client versions Print information about the Control Plane Print information about Nodes Print information about APIs   Cluster Info Motivation It may be necessary to learn about the Kubernetes cluster itself, rather than just the workloads running in it. This can be useful for debugging unexpected behavior.\nVersions The kubectl version prints the client and server versions. Note that the client version may not be present for clients built locally from source.\nkubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;9\u0026#34;, GitVersion:\u0026#34;v1.9.5\u0026#34;, GitCommit:\u0026#34;f01a2bf98249a4db383560443a59bed0c13575df\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2018-03-19T19:38:17Z\u0026#34;, GoVersion:\u0026#34;go1.9.4\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;11+\u0026#34;, GitVersion:\u0026#34;v1.11.6-gke.2\u0026#34;, GitCommit:\u0026#34;04ad69a117f331df6272a343b5d8f9e2aee5ab0c\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2019-01-04T16:19:46Z\u0026#34;, GoVersion:\u0026#34;go1.10.3b4\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;}  Version Skew Kubectl supports +/-1 version skew with the Kubernetes cluster. Kubectl versions that are more than 1 version ahead of or behind the cluster are not guaranteed to be compatible.  Control Plane and Addons The kubectl cluster-info prints information about the control plane and add-ons.\nkubectl cluster-info Kubernetes master is running at https://1.1.1.1 GLBCDefaultBackend is running at https://1.1.1.1/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy Heapster is running at https://1.1.1.1/api/v1/namespaces/kube-system/services/heapster/proxy KubeDNS is running at https://1.1.1.1/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://1.1.1.1/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy  Kube Proxy The URLs printed by cluster-info can be accessed at 127.0.0.1:8001 by running kubectl proxy.  Nodes The kubectl top node and kubectl top pod print information about the top nodes and pods.\nkubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% gke-dev-default-pool-e1e7bf6a-cc8b 37m 1% 571Mi 10% gke-dev-default-pool-e1e7bf6a-f0xh 103m 5% 1106Mi 19% gke-dev-default-pool-e1e7bf6a-jfq5 139m 7% 1252Mi 22% gke-dev-default-pool-e1e7bf6a-x37l 112m 5% 982Mi 17% APIs The kubectl api-versions and kubectl api-resources print information about the available Kubernetes APIs. This information is read from the Discovery Service.\nPrint the Resource Types available in the cluster.\nkubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace ... Print the API versions available in the cluster.\nkubectl api-versions admissionregistration.k8s.io/v1beta1 apiextensions.k8s.io/v1beta1 apiregistration.k8s.io/v1 apiregistration.k8s.io/v1beta1 apps/v1 apps/v1beta1 apps/v1beta2 ...  Discovery The discovery information can be viewed at 127.0.0.1:8001/ by running kubectl proxy. The Discovery for specific API can be found under either /api/v1 or /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;, depending on the API group - e.g. 127.0.0.1:8001/apis/apps/v1  The kubectl explain command can be used to print metadata about specific Resource types. This is useful for learning about the type.\nkubectl explain deployment --api-version apps/v1 KIND: Deployment VERSION: apps/v1 DESCRIPTION: Deployment enables declarative updates for Pods and ReplicaSets. FIELDS: apiVersion\t\u0026lt;string\u0026gt; APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources kind\t\u0026lt;string\u0026gt; Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds metadata\t\u0026lt;Object\u0026gt; Standard object metadata. spec\t\u0026lt;Object\u0026gt; Specification of the desired behavior of the Deployment. status\t\u0026lt;Object\u0026gt; Most recently observed status of the Deployment. ","excerpt":"TL;DR  Print information about the Cluster and Client versions Print information about the Control …","ref":"/guides/resource_printing/cluster_information/","title":"Cluster Information"},{"body":"","excerpt":"","ref":"/guides/example/","title":"Examples"},{"body":" TL;DR  Fields set and deleted from Resource Config are merged into Resources by Apply If a Resource already exists, Apply updates the Resources by merging the local Resource Config into the remote Resources Fields removed from the Resource Config will be deleted from the remote Resource   Merging Fields Advanced Section This chapter contains advanced material that readers may want to skip and come back to later.  When are fields merged? This page describes how Resource Config is merged with Resources or other Resource Config. This may occur when:\n Applying Resource Config updates to the live Resources in the cluster Defining Patches in the kustomization.yaml which are overlayed on resources and bases  Applying Resource Config Updates Rather than replacing the Resource with the new Resource Config, Apply will merge the new Resource Config into the live Resource. This retains values which may be set by the control plane - such as replicas values set by auto scalers\nDefining Patches patches are sparse Resource Config which contain a subset of fields that override values defined in other Resource Config with the same Group/Version/Kind/Namespace/Name. This is used to alter values defined on Resource Config without having to fork it.\nMotivation (Apply) This page describes the semantics for merging Resource Config.\nOwnership of Resource fields are shared between declarative Resource Config authored by human users, and values set by Controllers running in the cluster. Some fields, such as the status and clusterIp fields, are owned exclusively by Controllers. Fields, such as the name and namespace fields, are owned exclusively by the human user managing the Resource.\nOther fields, such as replicas, may be owned by either human users, the apiserver or Controllers. For example, replicas may be explicitly set by a user, implicitly set to a default value by the apiserver, or continuously adjusted by a Controller such as and HorizontalPodAutoscaler.\nLast Applied Resource Config When Apply creates or updates a Resource, it writes the Resource Config it Applied to an annotation on the Resource. This allows it to compare the last Resource Config it Applied to the current Resource Config and identify fields that have been deleted.\n# deployment.yaml (Resource Config)apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.7.9# Original ResourceDoesn\u0026#39;t Exist# Applied Resourcekind:Deploymentmetadata:annotations:# ...# This is the deployment.yaml Resource Config written as an annotation on the object# It was written by kubectl apply when the object was createdkubectl.kubernetes.io/last-applied-configuration:|{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;, \u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;nginx-deployment\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;}, \u0026#34;spec\u0026#34;:{\u0026#34;selector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;app\u0026#34;:nginx}},\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}}, \u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;image\u0026#34;:\u0026#34;nginx:1.7.9\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;}]}}}}# ...spec:# ...status:# ...Merging Resources Following are the merge semantics for Resources:\nAdding Fields:\n Fields present in the Resource Config that are missing from the Resource will be added to the Resource. Fields will be added to the Last Applied Resource Config  # deployment.yaml (Resource Config)apiVersion:apps/v1kind:Deploymentmetadata:# ...name:nginx-deploymentspec:# ...minReadySeconds:3# Original Resourcekind:Deploymentmetadata:# ...name:nginx-deploymentspec:# ...status:# ...# Applied Resourcekind:Deploymentmetadata:# ...name:nginx-deploymentspec:# ...minReadySeconds:3status:# ...Updating Fields\n Fields present in the Resource Config that are also present in the Resource will be merged recursively until a primitive field is updated, or a field is added / deleted. Fields will be updated in the Last Applied Resource Config  # deployment.yaml (Resource Config)apiVersion:apps/v1kind:Deploymentmetadata:# ...name:nginx-deploymentspec:# ...replicas:2# Original Resourcekind:Deploymentmetadata:# ...name:nginx-deploymentspec:# ...# could be defaulted or set by Resource Configreplicas:1status:# ...# Applied Resourcekind:Deploymentmetadata:# ...name:nginx-deploymentspec:# ...# updatedreplicas:2status:# ...Deleting Fields\n Fields present in the Last Applied Resource Config that have been removed from the Resource Config will be deleted from the Resource. Fields set to null in the Resource Config that are present in the Resource Config will be deleted from the Resource. Fields will be removed from the Last Applied Resource Config  # deployment.yaml (Resource Config)apiVersion:apps/v1kind:Deploymentmetadata:# ...name:nginx-deploymentspec:# ...# Original Resourcekind:Deploymentmetadata:# ...name:nginx-deployment# Containers replicas and minReadySecondskubectl.kubernetes.io/last-applied-configuration:|{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;, \u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;minReadySeconds\u0026#34;: \u0026#34;3\u0026#34;, ...}, \u0026#34;metadata\u0026#34;: {...}}spec:# ...minReadySeconds:3replicas:2status:# ...# Applied Resourcekind:Deploymentmetadata:# ...name:nginx-deploymentkubectl.kubernetes.io/last-applied-configuration:|{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;, \u0026#34;spec\u0026#34;:{...}, \u0026#34;metadata\u0026#34;: {...}}spec:# ...# deleted and then defaulted, but not in Last Appliedreplicas:1# minReadySeconds deletedstatus:# ... Removing Fields from Resource Config Simply removing a field from the Resource Config will not transfer the ownership to the cluster. Instead it will delete the field from the Resource. If a field is set in the Resource Config and the user wants to give up ownership (e.g. removing replicas from the Resource Config and using and autoscaler), the user must first remove it from the last Applied Resource Config stored by the cluster.\nThis can be performed using kubectl apply edit-last-applied to delete the replicas field from the Last Applied Resource Config, and then deleting it from the Resource Config.\n Field Merge Semantics Merging Primitives Primitive fields are merged by replacing the current value with the new value.\nField Creation: Add the primitive field\nField Update: Change the primitive field value\nField Deletion: Delete the primitive field\n   Field in Resource Config Field in Resource Field in Last Applied Action     Yes Yes - Set live to the Resource Config value.   Yes No - Set live to the Resource Config value.   No - Yes Remove from Resource.   No - No Do nothing.    Merging Objects Objects fields are updated by merging the sub-fields recursively (by field name) until a primitive field is found or the field is added / deleted.\nField Creation: Add the object field\nField Update: Recursively compare object sub-field values and merge them\nField Deletion: Delete the object field\nMerge Table: For each field merge Resource Config and Resource values with the same name\n   Field in Resource Config Field in Resource Field in Last Applied Action     Yes Yes - Recursively merge the Resource Config and Resource values.   Yes No - Set live to the Resource Config value.   No - Yes Remove field from Resource.   No - No Do nothing.    Merging Maps Map fields are updated by merging the elements (by key) until a primitive field is found or the value is added / deleted.\nField Creation: Add the map field\nField Update: Recursively compare map values by key and merge them\nField Deletion: Delete the map field\nMerge Table: For each map element merge Resource Config and Resource values with the same key\n   Key in Resource Config Key in Resource Key in Last Applied Action     Yes Yes - Recursively merge the Resource Config and Resource values.   Yes No - Set live to the Resource Config value.   No - Yes Remove map element from Resource.   No - No Do nothing.    Merging Lists of Primitives Lists of primitives will be merged if they have a patch strategy: merge on the field otherwise they will be replaced. Finalizer list example\nMerge Strategy:\n Merged primitive lists behave like ordered sets Replace primitive lists are replaced when merged  Ordering: Uses the ordering specified in the Resource Config. Elements not specified in the Resource Config do not have ordering guarantees with respect to the elements in the Resource Config.\nMerge Table: For each list element merge Resource Config and Resource element with the same value\n   Element in Resource Config Element in Resource Element in Last Applied Action     Yes Yes - Do nothing   Yes No - Add to list.   No - Yes Remove from list.   No - No Do nothing.    This merge strategy uses the patch merge key to identify container elements in a list and merge them. The patch merge key is defined in the Kubernetes API on the field.\n# Last Appliedargs:[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;]# Resource Config (Local)args:[\u0026#34;a\u0026#34;,\u0026#34;c\u0026#34;]# Resource (Live)args:[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;d\u0026#34;]# Applied Resourceargs:[\u0026#34;a\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;d\u0026#34;]Merging Lists of Objects Merge Strategy: Lists of primitives may be merged or replaced. Lists are merged if the list has a patch strategy of merge and a patch merge key on the list field. Container list example.\nMerge Key: The patch merge key is used to identify same elements in a list. Unlike map elements (keyed by key) and object fields (keyed by field name), lists don\u0026rsquo;t have a built-in merge identity for elements (index does not define identity). Instead an object field is used as a synthetic key/value for merging elements. This fields is the patch merge key. List elements with the same patch merge key will be merged when lists are merged.\nOrdering: Uses the ordering specified in the Resource Config. Elements not specified in the Resource Config do not have ordering guarantees.\nMerge Table: For each list element merge Resource Config and Resource element where the elements have the same value for the patch merge key\n   Element in Resource Config Element in Resource Element in Last Applied Action     Yes - - Recursively merge the Resource Config and Resource values.   Yes No - Add to list.   No - Yes Remove from list.   No - No Do nothing.    This merge strategy uses the patch merge key to identify container elements in a list and merge them. The patch merge key is defined in the Kubernetes API on the field.\n# Last Applied Resource Configcontainers:- name: nginx # key:nginximage:nginx:1.10- name: nginx-helper-a # key:nginx-helper-a; will be deleted in resultimage:helper:1.3- name: nginx-helper-b # key:nginx-helper-b; will be retainedimage:helper:1.3# Resource Config (Local)containers:- name:nginximage:nginx:1.10- name:nginx-helper-bimage:helper:1.3- name: nginx-helper-c # key:nginx-helper-c; will be added in resultimage:helper:1.3# Resource (Live)containers:- name:nginximage:nginx:1.10- name:nginx-helper-aimage:helper:1.3- name:nginx-helper-bimage:helper:1.3args:[\u0026#34;run\u0026#34;]# Field will be retained- name: nginx-helper-d # key:nginx-helper-d; will be retainedimage:helper:1.3# Applied Resourcecontainers:- name:nginximage:nginx:1.10# Element nginx-helper-a was Deleted- name:nginx-helper-bimage:helper:1.3# Field was Ignoredargs:[\u0026#34;run\u0026#34;]# Element was Added- name:nginx-helper-cimage:helper:1.3# Element was Ignored- name:nginx-helper-dimage:helper:1.3 Edit and Set While kubectl edit and kubectl set ignore the Last Applied Resource Config, Apply will change any values in the Resource Config set by either kubectl edit or kubectl set. To ignore values set by kubectl edit or kubectl set:\n Use kubectl apply edit-last-applied to remove the value from the Last Applied (if it is present) Remove the field from the Resource Config  This is the same technique for retaining values set by cluster components such as autoscalers.\n ","excerpt":"TL;DR  Fields set and deleted from Resource Config are merged into Resources by Apply If a Resource …","ref":"/references/architecture/field_merge_semantics/","title":"Field Merge Semantics"},{"body":"Additionally, generatorOptions can be set on a per resource level within each generator. For details on per-resource generatorOptions usage see field-name-configMapGenerator and See field-name-secretGenerator.\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationgeneratorOptions:# labels to add to all generated resourceslabels:kustomize.generated.resources:somevalue# annotations to add to all generated resourcesannotations:kustomize.generated.resource:somevalue# disableNameSuffixHash is true disables the default behavior of adding a# suffix to the names of generated resources that is a hash of# the resource contents.disableNameSuffixHash:trueExample I Using ConfigMap\nInput Files apiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationconfigMapGenerator:- name:my-application-propertiesfiles:- application.propertiesgeneratorOptions:labels:kustomize.generated.resources:config-labelannotations:kustomize.generated.resource:config-annotation# application.propertiesFOO=BarOutput File apiVersion:v1data:application.properties:|-# application.properties FOO=Barkind:ConfigMapmetadata:annotations:kustomize.generated.resource:config-annotationlabels:kustomize.generated.resources:config-labelname:my-application-properties-f7mm6mhf59Example II Using Secrets\nInput Files # kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationsecretGenerator:- name:app-tlsfiles:- \u0026#34;tls.cert\u0026#34;- \u0026#34;tls.key\u0026#34;type:\u0026#34;kubernetes.io/tls\u0026#34;generatorOptions:labels:kustomize.generated.resources:secret-labelannotations:kustomize.generated.resource:secret-annotationdisableNameSuffixHash:trueOutput File apiVersion:v1data:tls.cert:TFMwdExTMUNSVWQuLi50Q2c9PQ==tls.key:TFMwdExTMUNSVWQuLi4wdExRbz0=kind:Secretmetadata:annotations:kustomize.generated.resource:secret-annotationlabels:kustomize.generated.resources:secret-labelname:app-tlstype:kubernetes.io/tls","excerpt":"Additionally, generatorOptions can be set on a per resource level within each generator. For details …","ref":"/references/kustomize/generatoroptions/","title":"generatorOptions"},{"body":"In this workflow, all configuration (resource YAML) files are owned by the user. No content is incorporated from version control repositories owned by others.\nFollowing are the steps involved:\n  Create a directory in version control\nSpeculate some overall cluster application called ldap; we want to keep its configuration in its own repo.\n git init ~/ldap    Create a base\n mkdir -p ~/ldap/base  In this directory, create and commit a kustomization file and a set of resources.\n  Create overlays\n mkdir -p ~/ldap/overlays/staging mkdir -p ~/ldap/overlays/production  Each of these directories needs a kustomization file and one or more patches.\nThe staging directory might get a patch that turns on an experiment flag in a configmap.\nThe production directory might get a patch that increases the replica count in a deployment specified in the base.\n  Bring up variants\nRun kustomize, and pipe the output to apply.\n kustomize build ~/ldap/overlays/staging | kubectl apply -f - kustomize build ~/ldap/overlays/production | kubectl apply -f -  You can also use kubectl-v1.14.0 to apply your variants.\n kubectl apply -k ~/ldap/overlays/staging kubectl apply -k ~/ldap/overlays/production    ","excerpt":"In this workflow, all configuration (resource YAML) files are owned by the user. No content is …","ref":"/guides/config_management/bespoke/","title":"Bespoke Application"},{"body":"Images modify the name, tags and/or digest for images without creating patches.\nOne can change the image in the following ways (Refer the following example to know exactly how this is done):\n postgres:8 to my-registry/my-postgres:v1, nginx tag 1.7.9 to 1.8.0, image name my-demo-app to my-app, alpine\u0026rsquo;s tag 3.7 to a digest value  It is possible to set image tags for container images through the kustomization.yaml using the images field. When images are specified, Apply will override the images whose image name matches name with a new tag.\n   Field Description Example Field Example Result     name Match images with this image name name: nginx    newTag Override the image tag or digest for images whose image name matches name newTag: new nginx:old -\u0026gt; nginx:new   newName Override the image name for images whose image name matches name newName: nginx-special nginx:old -\u0026gt; nginx-special:old    Example File Input # deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:template:spec:containers:- name:mypostgresdbimage:postgres:8- name:nginxappimage:nginx:1.7.9- name:myappimage:my-demo-app:latest- name:alpine-appimage:alpine:3.7# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationimages:- name:postgresnewName:my-registry/my-postgresnewTag:v1- name:nginxnewTag:1.8.0- name:my-demo-appnewName:my-app- name:alpinedigest:sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3resources:- deployment.yamlBuild Output apiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:template:spec:containers:- image:my-registry/my-postgres:v1name:mypostgresdb- image:nginx:1.8.0name:nginxapp- image:my-app:latestname:myapp- image:alpine@sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3name:alpine-appSetting a Name The name for an image may be set by specifying newName and the name of the old container image.\n# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationimages:- name:mycontainerregistry/myimagenewName:differentregistry/myimageSetting a Tag The tag for an image may be set by specifying newTag and the name of the container image.\n# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationimages:- name:mycontainerregistry/myimagenewTag:v1Setting a Digest The digest for an image may be set by specifying digest and the name of the container image.\n# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationimages:- name:alpinedigest:sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3Setting a Tag from the latest commit SHA A common CI/CD pattern is to tag container images with the git commit SHA of source code. e.g. if the image name is foo and an image was built for the source code at commit 1bb359ccce344ca5d263cd257958ea035c978fd3 then the container image would be foo:1bb359ccce344ca5d263cd257958ea035c978fd3.\nA simple way to push an image that was just built without manually updating the image tags is to download the kustomize standalone tool and run kustomize edit set image command to update the tags for you.\nExample: Set the latest git commit SHA as the image tag for foo images.\nkustomize edit set image foo:$(git log -n 1 --pretty=format:\u0026#34;%H\u0026#34;) kubectl apply -f . Setting a Tag from an Environment Variable It is also possible to set a Tag from an environment variable using the same technique for setting from a commit SHA.\nExample: Set the tag for the foo image to the value in the environment variable FOO_IMAGE_TAG.\nkustomize edit set image foo:$FOO_IMAGE_TAG kubectl apply -f .  Committing Image Tag Updates The kustomization.yaml changes may be committed back to git so that they can be audited. When committing the image tag updates that have already been pushed by a CI/CD system, be careful not to trigger new builds + deployments for these changes.  ","excerpt":"Images modify the name, tags and/or digest for images without creating patches.\nOne can change the …","ref":"/references/kustomize/images/","title":"images"},{"body":"As namePrefix is self explanatory, it helps adding prefix to names in the defined yaml files.\nExample File Input # deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:5template:containers:- name:the-containerimage:registry/conatiner:latest# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationnamePrefix:overlook-resources:- deployment.yamlBuild Output apiVersion:apps/v1kind:Deploymentmetadata:name:alices-the-deploymentspec:replicas:5template:containers:- image:registry/conatiner:latestname:the-container References Apply will propagate the namePrefix to any place Resources within the project are referenced by other Resources including:\n Service references from StatefulSets ConfigMap references from PodSpecs Secret references from PodSpecs   ","excerpt":"As namePrefix is self explanatory, it helps adding prefix to names in the defined yaml files. …","ref":"/references/kustomize/nameprefix/","title":"namePrefix"},{"body":"In this workflow, all files are owned by the user and maintained in a repository under their control, but they are based on an off-the-shelf configuration that is periodically consulted for updates.\nFollowing are the steps involved:\n  Find and fork an OTS config\n  Clone it as your base\nThe base directory is maintained in a repo whose upstream is an OTS configuration, in this case some user\u0026rsquo;s ldap repo:\n mkdir ~/ldap git clone https://github.com/$USER/ldap ~/ldap/base cd ~/ldap/base git remote add upstream git@github.com:$USER/ldap    Create overlays\nAs in the bespoke case above, create and populate an overlays directory.\nThe overlays are siblings to each other and to the base they depend on.\n mkdir -p ~/ldap/overlays/staging mkdir -p ~/ldap/overlays/production  The user can maintain the overlays directory in a distinct repository.\n  Bring up variants\n kustomize build ~/ldap/overlays/staging | kubectl apply -f - kustomize build ~/ldap/overlays/production | kubectl apply -f -  You can also use kubectl-v1.14.0 to apply your variants.\n kubectl apply -k ~/ldap/overlays/staging kubectl apply -k ~/ldap/overlays/production    (Optionally) Capture changes from upstream\nThe user can periodically rebase their base to capture changes made in the upstream repository.\n cd ~/ldap/base git fetch upstream git rebase upstream/master    ","excerpt":"In this workflow, all files are owned by the user and maintained in a repository under their …","ref":"/guides/config_management/offtheshelf/","title":"Off The Shelf Application"},{"body":"File issues as desired, but if you\u0026rsquo;ve found a problem with how kustomize build works, please report\n the output of kustomize version, the input (the content of kustomization.yaml and any files it refers to), the expected YAML output.  If you have go installed kustomize has a simple test harness in the krusty package for specifying a kustomization\u0026rsquo;s input and the expected output.\nCopy one of those tests, e.g. this reusable custom transformer test, to a new test file in the krusty package.\nInsert the inputs you want to use, and run it as you\u0026rsquo;d run the reusable custom transformer test:\n(cd api; go test -run TestReusableCustomTransformers ./krusty) The output will demonstrate the bug or missing feature.\nRecord this output in the test file in a call to AssertActualEqualsExpected, per all the other tests in the krusty package. This makes the test pass, albeit with output demonstrating behavior you presumably want to change.\nSend the new test in a PR, along with commentary (in the test) on what you\u0026rsquo;d prefer to see.\nThe person who fixes the bug then has a clear bug reproduction and a test to modify when the bug is fixed.\nAny bug fix first requires a test demonstrating the bug (so we have permanent regression coverage), so if the bug reporter does this, it saves time and avoids misunderstandings.\n","excerpt":"File issues as desired, but if you\u0026rsquo;ve found a problem with how kustomize build works, please …","ref":"/contributing/kustomize/bugs/","title":"Filing Bugs"},{"body":"File issues as desired, but if you\u0026rsquo;ve found a problem with how kustomize build works, please report\n the output of kustomize version, the input (the content of kustomization.yaml and any files it refers to), the expected YAML output.  If you have go installed kustomize has a simple test harness in the krusty package for specifying a kustomization\u0026rsquo;s input and the expected output.\nCopy one of those tests, e.g. this reusable custom transformer test, to a new test file in the krusty package.\nInsert the inputs you want to use, and run it as you\u0026rsquo;d run the reusable custom transformer test:\n(cd api; go test -run TestReusableCustomTransformers ./krusty) The output will demonstrate the bug or missing feature.\nRecord this output in the test file in a call to AssertActualEqualsExpected, per all the other tests in the krusty package. This makes the test pass, albeit with output demonstrating behavior you presumably want to change.\nSend the new test in a PR, along with commentary (in the test) on what you\u0026rsquo;d prefer to see.\nThe person who fixes the bug then has a clear bug reproduction and a test to modify when the bug is fixed.\nAny bug fix first requires a test demonstrating the bug (so we have permanent regression coverage), so if the bug reporter does this, it saves time and avoids misunderstandings.\n","excerpt":"File issues as desired, but if you\u0026rsquo;ve found a problem with how kustomize build works, please …","ref":"/zh/contributing/bugs/","title":"Filing Bugs"},{"body":"","excerpt":"","ref":"/zh/api-reference/kustomization/","title":"kustomization.yaml"},{"body":"As of v3.7.0 Kustomize supports a special type of kustomization that allows one to define reusable pieces of configuration logic that can be included from multiple overlays.\nComponents come in handy when dealing with applications that support multiple optional features and you wish to enable only a subset of them in different overlays, i.e., different features for different environments or audiences.\nFor more details regarding this feature you can read the Kustomize Components KEP.\nUse case Suppose you\u0026rsquo;ve written a very simple Web application:\napiVersion:apps/v1kind:Deploymentmetadata:name:examplespec:template:spec:containers:- name:exampleimage:example:1.0You want to deploy a community edition of this application as SaaS, so you add support for persistence (e.g. an external database), and bot detection (e.g. Google reCAPTCHA).\nYou\u0026rsquo;ve now attracted enterprise customers who want to deploy it on-premises, so you add LDAP support, and disable Google reCAPTCHA. At the same time, the devs need to be able to test parts of the application, so they want to deploy it with some features enabled and others not.\nHere\u0026rsquo;s a matrix with the deployments of this application and the features enabled for each one:\n    External DB LDAP reCAPTCHA     Community ✔️  ✔️   Enterprise ✔️ ✔️    Dev ✅ ✅ ✅    (✔️ enabled, ✅: optional)\nSo, you want to make it easy to deploy your application in any of the above three environments. Here\u0026rsquo;s how you can do this with Kustomize components: each opt-in feature gets packaged as a component, so that it can be referred to from multiple higher-level overlays.\nFirst, define a place to work:\nDEMO_HOME=$(mktemp -d) Define a common base that has a Deployment and a simple ConfigMap, that is mounted on the application\u0026rsquo;s container.\nBASE=$DEMO_HOME/base mkdir $BASE # $BASE/kustomization.yaml resources: - deployment.yaml configMapGenerator: - name: conf literals: - main.conf=| color=cornflower_blue log_level=info # $BASE/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: example spec: template: spec: containers: - name: example image: example:1.0 volumeMounts: - name: conf mountPath: /etc/config volumes: - name: conf configMap: name: conf Define an external_db component, using kind: Component, that creates a Secret for the DB password and a new entry in the ConfigMap:\nEXT_DB=$DEMO_HOME/components/external_db mkdir -p $EXT_DB # $EXT_DB/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1alpha1 # \u0026lt;-- Component notation kind: Component secretGenerator: - name: dbpass files: - dbpass.txt patchesStrategicMerge: - configmap.yaml patchesJson6902: - target: group: apps version: v1 kind: Deployment name: example path: deployment.yaml # $EXT_DB/deployment.yaml - op: add path: /spec/template/spec/volumes/0 value: name: dbpass secret: secretName: dbpass - op: add path: /spec/template/spec/containers/0/volumeMounts/0 value: mountPath: /var/run/secrets/db/ name: dbpass # $EXT_DB/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: conf data: db.conf: | endpoint=127.0.0.1:1234 name=app user=admin pass=/var/run/secrets/db/dbpass.txt EOF Define an ldap component, that creates a Secret for the LDAP password and a new entry in the ConfigMap:\nLDAP=$DEMO_HOME/components/ldap mkdir -p $LDAP # $LDAP/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1alpha1 kind: Component secretGenerator: - name: ldappass files: - ldappass.txt patchesStrategicMerge: - configmap.yaml patchesJson6902: - target: group: apps version: v1 kind: Deployment name: example path: deployment.yaml # $LDAP/deployment.yaml - op: add path: /spec/template/spec/volumes/0 value: name: ldappass secret: secretName: ldappass - op: add path: /spec/template/spec/containers/0/volumeMounts/0 value: mountPath: /var/run/secrets/ldap/ name: ldappass # $LDAP/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: conf data: ldap.conf: | endpoint=ldap://ldap.example.com bindDN=cn=admin,dc=example,dc=com pass=/var/run/secrets/ldap/ldappass.txt EOF Define a recaptcha component, that creates a Secret for the reCAPTCHA site/secret keys and a new entry in the ConfigMap:\nRECAPTCHA=$DEMO_HOME/components/recaptcha mkdir -p $RECAPTCHA # $RECAPTCHA/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1alpha1 kind: Component secretGenerator: - name: recaptcha files: - site_key.txt - secret_key.txt # Updating the ConfigMap works with generators as well. configMapGenerator: - name: conf behavior: merge literals: - recaptcha.conf=| enabled=true site_key=/var/run/secrets/recaptcha/site_key.txt secret_key=/var/run/secrets/recaptcha/secret_key.txt patchesJson6902: - target: group: apps version: v1 kind: Deployment name: example path: deployment.yaml # $RECAPTCHA/deployment.yaml - op: add path: /spec/template/spec/volumes/0 value: name: recaptcha secret: secretName: recaptcha - op: add path: /spec/template/spec/containers/0/volumeMounts/0 value: mountPath: /var/run/secrets/recaptcha/ name: recaptcha EOF Define a community variant, that bundles the external DB and reCAPTCHA components:\nCOMMUNITY=$DEMO_HOME/overlays/community mkdir -p $COMMUNITY # $COMMUNITY/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../../base components: - ../../components/external_db - ../../components/recaptcha EOF Define an enterprise overlay, that bundles the external DB and LDAP components:\nENTERPRISE=$DEMO_HOME/overlays/enterprise mkdir -p $ENTERPRISE # $ENTERPRISE/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../../base components: - ../../components/external_db - ../../components/ldap EOF Define a dev overlay, that points to all the components and has LDAP disabled:\nDEV=$DEMO_HOME/overlays/dev mkdir -p $DEV # $DEV/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../../base components: - ../../components/external_db #- ../../components/ldap - ../../components/recaptcha EOF Now, the workspace has the following directories:\n├── base │ ├── deployment.yaml │ └── kustomization.yaml ├── components │ ├── external_db │ │ ├── configmap.yaml │ │ ├── dbpass.txt │ │ ├── deployment.yaml │ │ └── kustomization.yaml │ ├── ldap │ │ ├── configmap.yaml │ │ ├── deployment.yaml │ │ ├── kustomization.yaml │ │ └── ldappass.txt │ └── recaptcha │ ├── deployment.yaml │ ├── kustomization.yaml │ ├── secret_key.txt │ └── site_key.txt └── overlays ├── community │ └── kustomization.yaml ├── dev │ └── kustomization.yaml └── enterprise └── kustomization.yaml With this structure, you can generate the YAML manifests for each deployment using kustomize build:\nkustomize build overlays/community kustomize build overlays/enterprise kustomize build overlays/dev ","excerpt":"As of v3.7.0 Kustomize supports a special type of kustomization that allows one to define reusable …","ref":"/guides/config_management/components/","title":"Kustomize Components"},{"body":"Will override the existing namespace if it is set on a resource, or add it if it is not set on a resource.\nExample File Input # deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentnamespace:the-namespacespec:replicas:5template:containers:- name:the-containerimage:registry/conatiner:latest# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationnamespace:kustomize-namespaceresources:- deployment.yamlBuild Output apiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentnamespace:kustomize-namespacespec:replicas:5template:containers:- image:registry/conatiner:latestname:the-container","excerpt":"Will override the existing namespace if it is set on a resource, or add it if it is not set on a …","ref":"/references/kustomize/namespace/","title":"namespace"},{"body":"As nameSuffix is self explanatory, it helps adding suffix to names in the defined yaml files.\nNote: The suffix is appended before the content hash if the resource type is ConfigMap or Secret.\nExample File Input # deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:5template:containers:- name:the-containerimage:registry/conatiner:latest# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationnameSuffix:-custom-suffixresources:- deployment.yamlBuild Output apiVersion:apps/v1kind:Deploymentmetadata:name:the-deployment-custom-suffixspec:replicas:5template:containers:- image:registry/conatiner:latestname:the-container","excerpt":"As nameSuffix is self explanatory, it helps adding suffix to names in the defined yaml files.\nNote: …","ref":"/references/kustomize/namesuffix/","title":"nameSuffix"},{"body":"Patches (also call overlays) add or override fields on resources. They are provided using the patches Kustomization field.\nThe patches field contains a list of patches to be applied in the order they are specified.\nEach patch may:\n be either a strategic merge patch, or a JSON patch be either a file, or an inline string target a single resource or multiple resources  The patch target selects resources by group, version, kind, name, namespace, labelSelector and annotationSelector. Any resource which matches all the specified fields has the patch applied to it (regular expressions).\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationpatches:- path:patch.yamltarget:group:appsversion:v1kind:Deploymentname:deploy.*labelSelector:\u0026#34;env=dev\u0026#34;annotationSelector:\u0026#34;zone=west\u0026#34;- patch:|-- op: replace path: /some/existing/path value: new valuetarget:kind:MyKindlabelSelector:\u0026#34;env=dev\u0026#34;The name and namespace fields of the patch target selector are automatically anchored regular expressions. This means that the value myapp is equivalent to ^myapp$.\nConsider the following deployment.yaml common for both the examples:\napiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:5template:containers:- name:the-containerimage:registry/conatiner:latestExample I Intent To Make the container image point to a specific version and not to the latest container in the registry.\nFile Input # kustomization.yamlresources:- deployment.yamlpatches:- path:patch.yaml# patch.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:template:containers:- name:the-containerimage:registry/conatiner:1.0.0Build Output apiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:5template:containers:- image:registry/conatiner:1.0.0name:the-containerExample II Intent To Make the container image point to a specific version and not to the latest container in the registry.\nFile Input # kustomization.yamlresources:- deployment.yamlpatches:- target:kind:Deploymentname:the-deploymentpath:patch.json# patch.json[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;registry/conatiner:1.0.0\u0026#34;}]Build Output apiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:5template:containers:- image:registry/conatiner:1.0.0name:the-container","excerpt":"Patches (also call overlays) add or override fields on resources. They are provided using the …","ref":"/references/kustomize/patches/","title":"patches"},{"body":"Each entry in this list should resolve to a kubernetes object and a JSON patch that will be applied to the object. The JSON patch is documented at https://tools.ietf.org/html/rfc6902\ntarget field points to a kubernetes object within the same kustomization by the object\u0026rsquo;s group, version, kind, name and namespace. path field is a relative file path of a JSON patch file. The content in this patch file can be either in JSON format as\n[ {\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/some/new/path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;value\u0026#34;}, {\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/some/existing/path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;new value\u0026#34;} ] or in YAML format as\n- op:addpath:/some/new/pathvalue:value- op:replacepath:/some/existing/pathvalue:new valueapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationpatchesJson6902:- target:version:v1kind:Deploymentname:my-deploymentpath:add_init_container.yaml- target:version:v1kind:Servicename:my-servicepath:add_service_annotation.yamlThe patch content can be an inline string as well:\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationpatchesJson6902:- target:version:v1kind:Deploymentname:my-deploymentpatch:|-- op: add path: /some/new/path value: value - op: replace path: /some/existing/path value: \u0026#34;new value\u0026#34;","excerpt":"Each entry in this list should resolve to a kubernetes object and a JSON patch that will be applied …","ref":"/references/kustomize/patchesjson6902/","title":"patchesJson6902"},{"body":"Each entry in this list should be either a relative file path or an inline content resolving to a partial or complete resource definition.\nThe names in these (possibly partial) resource files must match names already loaded via the resources field. These entries are used to patch (modify) the known resources.\nSmall patches that do one thing are best, e.g. modify a memory request/limit, change an env var in a ConfigMap, etc. Small patches are easy to review and easy to mix together in overlays.\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationpatchesStrategicMerge:- service_port_8888.yaml- deployment_increase_replicas.yaml- deployment_increase_memory.yamlThe patch content can be a inline string as well.\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationpatchesStrategicMerge:- |-apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: template: spec: containers: - name: nginx image: nignx:latestNote that kustomize does not support more than one patch for the same object that contain a delete directive. To remove several fields / slice elements from an object create a single patch that performs all the needed deletions.\n","excerpt":"Each entry in this list should be either a relative file path or an inline content resolving to a …","ref":"/references/kustomize/patchesstrategicmerge/","title":"patchesStrategicMerge"},{"body":"Given this kubernetes Deployment fragment:\nkind:Deploymentmetadata:name:deployment-namespec:replicas:3one can change the number of replicas to 5 by adding the following to your kustomization:\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationreplicas:- name:deployment-namecount:5This field accepts a list, so many resources can be modified at the same time.\nAs this declaration does not take in a kind: nor a group: it will match any group and kind that has a matching name and that is one of:\n Deployment ReplicationController ReplicaSet StatefulSet  For more complex use cases, revert to using a patch.\nExample Input File # deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:5template:containers:- name:the-containerimage:registry/conatiner:latest# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationreplicas:- name:deployment-namecount:10resources:- deployment.yamlOutput # deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:the-deploymentspec:replicas:10template:containers:- name:the-containerimage:registry/conatiner:latest","excerpt":"Given this kubernetes Deployment fragment: …","ref":"/references/kustomize/replicas/","title":"replicas"},{"body":"Each entry in this list must be a path to a file, or a path (or URL) referring to another kustomization directory, e.g.\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationresources:- myNamespace.yaml- sub-dir/some-deployment.yaml- ../../commonbase- github.com/kubernetes-sigs/kustomize/examples/multibases?ref=v1.0.6- deployment.yaml- github.com/kubernets-sigs/kustomize/examples/helloWorld?ref=test-branchResources will be read and processed in depth-first order.\nFiles should contain k8s resources in YAML form. A file may contain multiple resources separated by the document marker ---. File paths should be specified relative to the directory holding the kustomization file containing the resources field.\nDirectory specification can be relative, absolute, or part of a URL. URL specifications should follow the hashicorp URL format. The directory must contain a kustomization.yaml file.\n","excerpt":"Each entry in this list must be a path to a file, or a path (or URL) referring to another …","ref":"/references/kustomize/resource/","title":"resources"},{"body":"Each entry in the argument list results in the creation of one Secret resource (it\u0026rsquo;s a generator of N secrets).\nThis works like the configMapGenerator.\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationsecretGenerator:- name:app-tlsfiles:- secret/tls.cert- secret/tls.keytype:\u0026#34;kubernetes.io/tls\u0026#34;- name:app-tls-namespaced# you can define a namespace to generate# a secret in, defaults to: \u0026#34;default\u0026#34;namespace:appsfiles:- tls.crt=catsecret/tls.cert- tls.key=secret/tls.keytype:\u0026#34;kubernetes.io/tls\u0026#34;- name:env_file_secretenvs:- env.txttype:Opaque- name:secret-with-annotationfiles:- app-config.yamltype:Opaqueoptions:annotations:app_config:\u0026#34;true\u0026#34;labels:app.kubernetes.io/name:\u0026#34;app2\u0026#34;Secret Resources may be generated much like ConfigMaps can. This includes generating them from literals, files or environment files.\nSecret Syntax Secret type is set using the type field.  Example File Input # kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationsecretGenerator:- name:app-tlsfiles:- \u0026#34;tls.cert\u0026#34;- \u0026#34;tls.key\u0026#34;type:\u0026#34;kubernetes.io/tls\u0026#34;# tls.certLS0tLS1CRUd...tCg==# tls.keyLS0tLS1CRUd...0tLQo=Build Output apiVersion:v1data:tls.cert:TFMwdExTMUNSVWQuLi50Q2c9PQ==tls.key:TFMwdExTMUNSVWQuLi4wdExRbz0=kind:Secretmetadata:name:app-tls-c888dfbhf8type:kubernetes.io/tls Important It is important to note that the secrets are base64 encoded  ","excerpt":"Each entry in the argument list results in the creation of one Secret resource (it\u0026rsquo;s a …","ref":"/references/kustomize/secretgenerator/","title":"secretGenerator"},{"body":"Vars are used to capture text from one resource\u0026rsquo;s field and insert that text elsewhere - a reflection feature.\nFor example, suppose one specifies the name of a k8s Service object in a container\u0026rsquo;s command line, and the name of a k8s Secret object in a container\u0026rsquo;s environment variable, so that the following would work:\ncontainers:- image:myimagecommand:[\u0026#34;start\u0026#34;,\u0026#34;--host\u0026#34;,\u0026#34;$(MY_SERVICE_NAME)\u0026#34;]env:- name:SECRET_TOKENvalue:$(SOME_SECRET_NAME)To do so, add an entry to vars: as follows:\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationvars:- name:SOME_SECRET_NAMEobjref:kind:Secretname:my-secretapiVersion:v1- name:MY_SERVICE_NAMEobjref:kind:Servicename:my-serviceapiVersion:v1fieldref:fieldpath:metadata.name- name:ANOTHER_DEPLOYMENTS_POD_RESTART_POLICYobjref:kind:Deploymentname:my-deploymentapiVersion:apps/v1fieldref:fieldpath:spec.template.spec.restartPolicyA var is a tuple of variable name, object reference and field reference within that object. That\u0026rsquo;s where the text is found.\nThe field reference is optional; it defaults to metadata.name, a normal default, since kustomize is used to generate or modify the names of resources.\nAt time of writing, only string type fields are supported. No ints, bools, arrays etc. It\u0026rsquo;s not possible to, say, extract the name of the image in container number 2 of some pod template.\nA variable reference, i.e. the string \u0026lsquo;$(FOO)\u0026rsquo;, can only be placed in particular fields of particular objects as specified by kustomize\u0026rsquo;s configuration data.\nThe default config data for vars is at /api/konfig/builtinpluginconsts/varreference.go Long story short, the default targets are all container command args and env value fields.\nVars should not be used for inserting names in places where kustomize is already handling that job. E.g., a Deployment may reference a ConfigMap by name, and if kustomize changes the name of a ConfigMap, it knows to change the name reference in the Deployment.\n","excerpt":"Vars are used to capture text from one resource\u0026rsquo;s field and insert that text elsewhere - a …","ref":"/references/kustomize/vars/","title":"vars"},{"body":"Kustomize is a sub project of the Kubernetes CLI special interest group and follows the Kubernetes project contributor roles.\nIf you are interested in contributing towards Kustomize or getting more involved with the community:\n join the mailing list and reach out join the slack channel and reach out attend one of the bi-weekly meetings (alternating Wednesdays at 9:00am Pacific Time)  ","excerpt":"Kustomize is a sub project of the Kubernetes CLI special interest group and follows the Kubernetes …","ref":"/contributing/kustomize/community/","title":"Community Engagment"},{"body":"Kustomize is a sub project of the Kubernetes CLI special interest group and follows the Kubernetes project contributor roles.\nIf you are interested in contributing towards Kustomize or getting more involved with the community:\n join the mailing list and reach out join the slack channel and reach out attend one of the bi-weekly meetings (alternating Wednesdays at 9:00am Pacific Time)  ","excerpt":"Kustomize is a sub project of the Kubernetes CLI special interest group and follows the Kubernetes …","ref":"/zh/contributing/community/","title":"Community Engagment"},{"body":"","excerpt":"","ref":"/blog/releases/","title":"New Releases"},{"body":"","excerpt":"","ref":"/zh/blog/releases/","title":"New Releases"},{"body":"","excerpt":"","ref":"/zh/installation/","title":"安装"},{"body":"Following is the process for proposing a new Kustomize feature:\n Check the eschewed feature list to see if the feature has already been proposed File an issue describing the desired feature  label it kind/feature the motivation for the feature example of how you would accomplish the motivating task without the feature example of how you would accomplish the motivating task with the feature   Email the sig-cli mailing list with the issue Present the issue at sig-cli bi-weekly meeting on Zoom  add it to the meeting agenda doc be present to discuss the feature response may be \u0026ndash; move forward with a PoC, not to move forward, defer and come back later, or more information is needed.   Address the feedback on the issue  Possibly write a KEP for tracking the feature   Implement the feature and send a PR  Add table-driven tests Expect comments on the PR within 2 weeks   Kustomize team will release the kustomize api and kustomize modules  ","excerpt":"Following is the process for proposing a new Kustomize feature:\n Check the eschewed feature list to …","ref":"/contributing/kustomize/features/","title":"Contributing Features"},{"body":"Following is the process for proposing a new Kustomize feature:\n Check the eschewed feature list to see if the feature has already been proposed File an issue describing the desired feature  label it kind/feature the motivation for the feature example of how you would accomplish the motivating task without the feature example of how you would accomplish the motivating task with the feature   Email the sig-cli mailing list with the issue Present the issue at sig-cli bi-weekly meeting on Zoom  add it to the meeting agenda doc be present to discuss the feature response may be \u0026ndash; move forward with a PoC, not to move forward, defer and come back later, or more information is needed.   Address the feedback on the issue  Possibly write a KEP for tracking the feature   Implement the feature and send a PR  Add table-driven tests Expect comments on the PR within 2 weeks   Kustomize team will release the kustomize api and kustomize modules  ","excerpt":"Following is the process for proposing a new Kustomize feature:\n Check the eschewed feature list to …","ref":"/zh/contributing/features/","title":"Contributing Features"},{"body":"Kustomize uses Docsy for the site, and was forked from the docsy-example\nPrerequisites  Install hugo Clone kustomize  git clone git@github.com:kubernetes-sigs/kustomize \u0026amp;\u0026amp; cd kustomize/    Development The doc input files are in the site directory. The site can be hosted locally using hugo serve.\ncd site/ hugo serve ... Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/kustomize/ (bind address 127.0.0.1) Publishing Hugo compiles the files under site Hugo into html which it puts in the docs folder:\ncd site/ hugo | EN -------------------+----- Pages | 99 Paginator pages | 0 Non-page files | 0 Static files | 47 Processed images | 0 Aliases | 2 Sitemaps | 1 Cleaned | 0 Add the site/ and docs/ folders to a commit, then create a PR.\nPublishing docs to your kustomize fork It is possible to have the kustomize docs published to your forks github pages.\nSetup GitHub Pages for the fork  Go to the forked repo\u0026rsquo;s Settings tab  e.g. https://github.com/pwittrock/kustomize   Go to the GitHub Pages section Set the source to master branch /docs folder  Publish to the fork\u0026rsquo;s GitHub Pages Changes must be pushed to the fork\u0026rsquo;s master branch to be served as the fork\u0026rsquo;s GitHub Page.\n  Make a change to a file under site/content Run hugo from the site/ directory Add the site and docs directories to the master branch Commit and push the changes to the remote fork\u0026rsquo;s master branch After a few minutes, the docs should be served from the fork\u0026rsquo;s GitHub Page  e.g. https://pwittrock.github.io/kustomize/    ","excerpt":"Kustomize uses Docsy for the site, and was forked from the docsy-example\nPrerequisites  Install hugo …","ref":"/zh/contributing/docs/","title":"Writing Docs"},{"body":" Info To build kustomize using the locally modified modules, replace statements must be added to the kustomize/go.mod.\ne.g. if code in api was modified, a replace statement would need to be added for the kustomize/api module.\n Call stack when running kustomize build, with links to code.\nRun build  RunBuild  MakeKustomizer Run: performs a kustomization. It uses its internal filesystem reference to read the file at the given path argument, interpret it as a kustomization.yaml file, perform the kustomization it represents, and return the resulting resources.  Create factories  tranformer.NewFactoryImpl resmap.NewFactory  resource.NewFactory  kustruct.NewKunstructuredFactoryImpl     loader.NewLoader validator.NewKustValidator   NewKustTarget Load MakeCustomizeResMap: details in next section   emitResources    Make resource map  makeCustomizeResMap  AccumulateTarget: returns a new ResAccumulator, holding customized resources and the data/rules used to do so. The name back references and vars are not yet fixed.  accummulateResources: fills the given resourceAccumulator with resources read from the given list of paths. Merge config from builtin and CRDs runGenerators  configureBuiltinGenerators  ConfigMapGenerator SecretGenerator   configureExternalGenerators Iterate all generators   runTransfomers  configureBuiltinTransformers  PatchStrategicMergeTransformer PatchTransformer NamespaceTransformer PrefixSuffixTransformer LabelTransformer AnnotationsTransformer PatchJson6902Transformer ReplicaCountTransformer ImageTagTransformer   configureExternalTransformers   MergeVars   The following steps must be done last, not as part of the recursion implicit in AccumulateTarget.  addHashesToNames FixBackReferences: Given that names have changed (prefixs/suffixes added), fix all the back references to those names. ResolveVars      ","excerpt":"Info To build kustomize using the locally modified modules, replace statements must be added to the …","ref":"/contributing/kustomize/howitworks/","title":"Writing Code"},{"body":" To build kustomize using the locally modified modules, replace statements must be added to the kustomize/go.mod.\ne.g. if code in api was modified, a replace statement would need to be added for the kustomize/api module.\n Call stack when running kustomize build, with links to code.\nRun build  RunBuild  MakeKustomizer Run: performs a kustomization. It uses its internal filesystem reference to read the file at the given path argument, interpret it as a kustomization.yaml file, perform the kustomization it represents, and return the resulting resources.  Create factories  tranformer.NewFactoryImpl resmap.NewFactory  resource.NewFactory  kustruct.NewKunstructuredFactoryImpl     loader.NewLoader validator.NewKustValidator   NewKustTarget Load MakeCustomizeResMap: details in next section   emitResources    Make resource map  makeCustomizeResMap  AccumulateTarget: returns a new ResAccumulator, holding customized resources and the data/rules used to do so. The name back references and vars are not yet fixed.  accummulateResources: fills the given resourceAccumulator with resources read from the given list of paths. Merge config from builtin and CRDs runGenerators  configureBuiltinGenerators  ConfigMapGenerator SecretGenerator   configureExternalGenerators Iterate all generators   runTransfomers  configureBuiltinTransformers  PatchStrategicMergeTransformer PatchTransformer NamespaceTransformer PrefixSuffixTransformer LabelTransformer AnnotationsTransformer PatchJson6902Transformer ReplicaCountTransformer ImageTagTransformer   configureExternalTransformers   MergeVars   The following steps must be done last, not as part of the recursion implicit in AccumulateTarget.  addHashesToNames FixBackReferences: Given that names have changed (prefixs/suffixes added), fix all the back references to those names. ResolveVars      ","excerpt":"To build kustomize using the locally modified modules, replace statements must be added to the …","ref":"/zh/contributing/howitworks/","title":"Writing Code"},{"body":"First install the tools to build and run tests\nInstall go 1.13 Instructions\nAdd go to your PATH\nInstall kubeval Instructions\ngo get github.com/instrumenta/kubeval Add kubeval to your PATH\nInstall gnu tools Instructions\nbrew install coreutils wget gnu-sed tree Add the new tools to your PATH\nMake everything Verify your install by running make:\nmake Be default, this runs all tests needed to qualify a pull request.\n","excerpt":"First install the tools to build and run tests\nInstall go 1.13 Instructions\nAdd go to your PATH …","ref":"/contributing/kustomize/mac/","title":"MacOS Dev Guide"},{"body":"First install the tools to build and run tests\nInstall go 1.13 Instructions\nAdd go to your PATH\nInstall kubeval Instructions\ngo get github.com/instrumenta/kubeval Add kubeval to your PATH\nInstall gnu tools Instructions\nbrew install coreutils wget gnu-sed tree Add the new tools to your PATH\nMake everything Verify your install by running make:\nmake Be default, this runs all tests needed to qualify a pull request.\n","excerpt":"First install the tools to build and run tests\nInstall go 1.13 Instructions\nAdd go to your PATH …","ref":"/zh/contributing/mac/","title":"MacOS Dev Guide"},{"body":"This is the PowerShell script to run all go tests for Kustomize on a windows based platform which mimics /build/pre-commit.sh\nPre-Reqs  PowerShell installed  PowerShell Core is supported   go installed golangci-lint installed mdrip installed  This script should output to the current console and return an exit code if all tests are successful(0) or any failed(1).\nIf you are tryin to run these tests locally you can follow these instructions Assume:\n Running a stock Windows 10 system Local Admin rights. You can open PowerShell as administrator You should be knowledgeable enough to pull source for packages into your GO src directory  Yes, this means you also need to know a bit about git usually    Step 1 - Install Go  Install Go - please use the msi  If you use chocolatey - it\u0026rsquo;s using the zip not msi and assumptions on where go is located are made for you.    Step 2 - Install Go Packages  Open new PowerShell Administrative window  Install golangci-lint  go get -u github.com/golangci/golangci-lint/cmd/golangci-lint   Install mdrip  go get github.com/monopole/mdrip      You should now be able to issue these commands and see comparable responses\nC:\\...\u0026gt; golangci-lint --help Smart, fast linters runner. Run it in cloud for every GitHub pull request on https://golangci.com ... C:\\...\u0026gt; mdrip --help Usage: C:\\_go\\bin\\mdrip.exe {fileName}... ... Step 3 - Get Source and Test  In your GoRoot src  Example: C:\\_go\\src   Navigate to the Kustomize travis directory  Example: C:\\_go\\src\\sigs.k8s.io\\kustomize\\travis   Now Execute:  .\\Invoke-PreCommit.ps1    This should run all pre-commit tests thus defined in the script.\n","excerpt":"This is the PowerShell script to run all go tests for Kustomize on a windows based platform which …","ref":"/contributing/kustomize/windows/","title":"Windows Dev Guide"},{"body":"This is the PowerShell script to run all go tests for Kustomize on a windows based platform which mimics /build/pre-commit.sh\nPre-Reqs  PowerShell installed  PowerShell Core is supported   go installed golangci-lint installed mdrip installed  This script should output to the current console and return an exit code if all tests are successful(0) or any failed(1).\nIf you are tryin to run these tests locally you can follow these instructions Assume:\n Running a stock Windows 10 system Local Admin rights. You can open PowerShell as administrator You should be knowledgeable enough to pull source for packages into your GO src directory  Yes, this means you also need to know a bit about git usually    Step 1 - Install Go  Install Go - please use the msi  If you use chocolatey - it\u0026rsquo;s using the zip not msi and assumptions on where go is located are made for you.    Step 2 - Install Go Packages  Open new PowerShell Administrative window  Install golangci-lint  go get -u github.com/golangci/golangci-lint/cmd/golangci-lint   Install mdrip  go get github.com/monopole/mdrip      You should now be able to issue these commands and see comparable responses\nC:\\...\u0026gt; golangci-lint --help Smart, fast linters runner. Run it in cloud for every GitHub pull request on https://golangci.com ... C:\\...\u0026gt; mdrip --help Usage: C:\\_go\\bin\\mdrip.exe {fileName}... ... Step 3 - Get Source and Test  In your GoRoot src  Example: C:\\_go\\src   Navigate to the Kustomize travis directory  Example: C:\\_go\\src\\sigs.k8s.io\\kustomize\\travis   Now Execute:  .\\Invoke-PreCommit.ps1    This should run all pre-commit tests thus defined in the script.\n","excerpt":"This is the PowerShell script to run all go tests for Kustomize on a windows based platform which …","ref":"/zh/contributing/windows/","title":"Windows Dev Guide"},{"body":"Kustomize 提供一个插件框架，允许用户开发自己的 生成器 和 转化器。\n通过插件，实现 [generatorOptions] 和 [transformerconfigs] 无法满足的需求。\n generator 插件生成 k8s 资源，比如 helm chart inflator 是一个 generator 插件，基于少量自由变量生成一个 12-factor 应用所包含的全部组件 deployment，service，scaler，ingress 等）也是一个 generator 插件。 transformer 插件转化（修改）k8s 资源，比如可能会执行对特殊容器命令行的编辑，或为其他内置转换器（namePrefix、commonLabels 等）无法转换的内容提供转换。  kustomization.yaml 的格式 从为添加 generators 或 transformers 字段开始。\n字段内容为一个 string list：\n generators:- relative/path/to/some/file.yaml- relative/path/to/some/kustomization- /absolute/path/to/some/kustomization- https://github.com/org/repo/some/kustomizationtransformers:- {as above} 格式要求与 resources 字段相同，generators 或 transformers 列表的每一列内容都必须是一个 YAML 文件的相对路径或者指向 kustomization 的 URL。\n从磁盘上读取 YAML 文件，kustomization 的路径或 URL 会触发 kustomization 的运行。由此产生的每个的对象都会被 kustomize 进一步解析为 plugin configuration 对象。\n配置 kustomization 文件可以包含如下内容：\ngenerators:- chartInflator.yaml像这样，kustomization 进程将在 kustomization root 下寻找到一个名为 chartInflator.yaml 的文件。\nchartInflator.yaml 为插件配置文件，该文件包含 YAML 配置对象，内容如下：\napiVersion:someteam.example.com/v1kind:ChartInflatormetadata:name:notImportantHerechartName:minecraftapiVersion 和 kind 字段用于定位插件。\n同时由于 kustomize 插件配置对象也是一个 k8s 对象，因此这些字段是必要的。\n为了让插件准备好生成或转换，它包含了配置文件的全部内容。\n更多关于插件配置 YAML 的例子，请浏览根目录下 plugins 中的单元测试，例如 ChartInflator 或 NameTransformer。\n植入 每个插件都有自己的专用目录，名为：\n$XDG_CONFIG_HOME/kustomize/plugin /${apiVersion}/LOWERCASE(${kind}) The default value of XDG_CONFIG_HOME is $HOME/.config.\nXDG_CONFIG_HOME 的默认值为 $HOME/.config。\n为了便于插件包（源码、测试、插件数据文件等）的共享，要求每个目录存放一个插件。\n在 Go 插件中，还可以为单个插件提供一个 go.mod 文件，可以缓解包版本依赖性偏移的问题。\n加载时，kustomize 首先会寻找一个 可执行 文件，名为：\n$XDG_CONFIG_HOME/kustomize/plugin /${apiVersion}/LOWERCASE(${kind})/${kind} 如果没有找到这个文件，或者这个文件不是可执行的，kustomize 会在同一目录下寻找一个名为 ${kind}.so 的文件，并尝试将其作为 Go插件 加载。\n如果这两项检查都失败，则插件加载失败，kustomize build 失败。\n执行情况 插件只有在运行 kustomize build 命令时使用。\n生成器插件是在处理完 resources 字段后运行的（resources 字段本身也可以看成是一个简单地从磁盘上读取对象的生成器）。\n之后所有资源将被传递到转换管道中，由其中内置的转换器，如 namePrefix 和 commonLabel 等先转换应用（如果 kustomization 文件中指定了他们），然后再转换用户指定的 transformers 字段。\n由于无法指定转化的顺序，所以需要遵守 transformers 字段中指定的顺序。\nNo Security Kustomize 插件不会在任何形式的 kustomize 提供的沙盒中运行。不存在 \u0026ldquo;plugin security\u0026rdquo; 的概念。\nkustomize build 会尝试使用插件，但如果省略了 --enable_alpha_plugins，将导致插件无法加载，并且会有一个关于插件使用的警告。\n使用这个 flag 就是承认使用不稳定的插件 API（alpha）、承认使用缺少出处插件，以及插件不属于 kustomize 的事实。\n简单的说，一些从网上下载的 kustomize 插件可能会奇妙地将 k8s 的配置以理想的方式进行改造，同时也会悄悄地对运行 kustomize build 的系统做任何用户可以做的事情。\n编写插件 插件有 exec 和 Go 两种.\nExec 插件 exec 插件 是一个可以在命令行中接收参数可执行文件，该参数指向包含 kustomization 配置的 YAML 文件。\n TODO: 对插件的限制，允许同一个 exec 插件 同时被 generators 和 transformers 字段所触发。\n 第一个参数可以是固定的字符串 generate 或 transform，（配置文件的名称移动到第2个参数） 默认情况下，exec plugin 会作为一个转化器,除非提供了标志 -g，将 exec 插件切换为生成器。   示例  helm chart inflator - helm chart inflates 生成器。 bashed config map - 使用 bash 生成十分简单的 configMap。 sed transformer - 使用插件来定义非结构化的编辑。 hashicorp go-getter - 下载 kustomize layes 并通过构建它来生成资源。  生成器插件无需在 stdin 上输入任何东西，就会将生成的资源输出到 stdout。\n转化器插件需要在 stdin 上输入资源的 YAML，并转化后的资源输出到 stdout。\nkustomize 会使用 exec 插件适配器，为 stdin 提供的资源，并获取 stdout 以进行下一步的处理。\nGenerator 选项 生成器 exec 插件可以通过设置以下内部注释中的一个来调整生成器选项。\n 注意：这些注释只会在本地的 kustomize 中，不会出现在最终输出中。\n kustomize.config.k8s.io/needs-hash\n通过包含 needs-hash 注释，可以将资源标记为需要由内部哈希转换器处理的资源。当设置注释的有效值为 \u0026quot;true\u0026quot; 和 \u0026quot;false\u0026quot; 时，分别启用或禁用资源的哈希后缀。忽略该注解相当于将值设置为 \u0026quot;false\u0026quot;。\n如果此注释被设置在不受哈希转换器支持的资源上，将导致构建将失败。\n示例：\napiVersion:v1kind:ConfigMapmetadata:name:cm-testannotations:kustomize.config.k8s.io/needs-hash:\u0026#34;true\u0026#34;data:foo:barkustomize.config.k8s.io/behavior\nbehavior 注释为当资源发生冲突时插件的处理方式，有效值包括：\u0026ldquo;create\u0026rdquo;、\u0026ldquo;merge \u0026ldquo;和 \u0026ldquo;replace\u0026rdquo;，默认为 \u0026ldquo;create\u0026rdquo;。\n示例：\napiVersion:v1kind:ConfigMapmetadata:name:cm-testannotations:kustomize.config.k8s.io/behavior:\u0026#34;merge\u0026#34;data:foo:barGo 插件 请务必阅读 Go 插件注意事项。\n如果一个 .go 文件声明 package main，并附加了有用的功能标志，那么它就可以成为一个 Go 插件。\n如果标志被命名为 “KustomizePlugin”，并且附加的函数实现了 Configurable、Generator 和 Transformer 接口，那么它可以进一步作为 kustomize 插件使用。\nkustomize 的一个 Go 插件看起来是这样的：\n package main import ( \u0026#34;sigs.k8s.io/kustomize/api/ifc\u0026#34; \u0026#34;sigs.k8s.io/kustomize/api/resmap\u0026#34; ... ) type plugin struct {...} var KustomizePlugin plugin func (p *plugin) Config( ldr ifc.Loader, rf *resmap.Factory, c []byte) error {...} func (p *plugin) Generate() (resmap.ResMap, error) {...} func (p *plugin) Transform(m resmap.ResMap) error {...}  需要使用标识符 plugin，KustomizePlugin 并且需要实现方法签名 Config。\n实现 Generatoror 或 Transformer 方法允许（分别）将插件的配置文件添加到 kustomization 文件的 generatorsor 或 transformers 字段中，并根据需要执行。\n示例  service generator - 使用 name 和 port 参数生成一个 service。 string prefixer - 使用 metadata/name 值作为前缀。这个特殊的示例是为了展示插件的转化行为。详见 target 包中的 TestTransformedTransformers 测试。 date prefixer - 将当前日期作为前缀添加到资源名称上，这是一个用于修改刚才提到的字符串前缀插件的简单示例。 secret generator - 从 toy 数据库生成 secret。 sops encoded secrets - 一个更复杂的 secret 生成器。 All the builtin plugins. 用户自制的插件与内置插件是一样的。  Go 插件既可以是生成器，也可以是转化器。Generate 方法将在 Transform 方法运行之前与所有其他生成器一起运行。\n如下的构建命令，假设插件源代码位于 kustomize 期望查找 .so 文件的目录中：\nd=$XDG_CONFIG_HOME/kustomize/plugin\\ /${apiVersion}/LOWERCASE(${kind}) go build -buildmode plugin \\  -o $d/${kind}.so $d/${kind}.go ","excerpt":"Kustomize 提供一个插件框架，允许用户开发自己的 生成器 和 转化器。\n通过插件，实现 [generatorOptions] 和 [transformerconfigs] 无法满足的需求。 …","ref":"/zh/guides/plugins/","title":"Kustomize 插件"},{"body":"The maintainers established this list to place bounds on the kustomize feature set. The bounds can be changed with a consensus on the risks.\nFor a bigger picture about why kustomize does some things and not others, see the glossary entry for DAM.\nRemoval directives kustomize supports configurations that can be reasoned about as compositions or mixins - concepts that are widely accepted as a best practice in various programming languages.\nTo this end, kustomize offers various addition directives. One may add labels, annotations, patches, resources, bases, etc. Corresponding removal directives are not offered.\nRemoval semantics would introduce many possibilities for inconsistency, and the need to add code to detect, report and reject it. It would also allow, and possibly encourage, unnecessarily complex configuration layouts.\nWhen faced with a situation where removal is desirable, it\u0026rsquo;s always possible to remove things from a base like labels and annotations, and/or split multi-resource manifests into individual resource files - then add things back as desired via the kustomization.\nIf the underlying base is outside of one\u0026rsquo;s control, an OTS workflow is the recommended best practice. Fork the base, remove what you don\u0026rsquo;t want and commit it to your private fork, then use kustomize on your fork. As often as desired, use git rebase to capture improvements from the upstream base.\nUnstructured edits Structured edits are changes controlled by knowledge of the k8s API, and YAML or JSON syntax.\nMost edits performed by kustomize can be expressed as JSON patches or SMP patches. Those can be verbose, so common patches, like adding labels or annotatations, get dedicated transformer plugins - LabelTransformer, AnnotationsTransformer, etc. These accept relatively simple YAML configuration allowing easy targeting of any number of resources.\nAnother class of edits take data from one specific object\u0026rsquo;s field and use it in another (e.g. a service object\u0026rsquo;s name found and copied into a container\u0026rsquo;s command line). These reflection-style edits are called replacements.\nThe above edits create valid output given valid input, and can provide syntactically and semantically informed error messages if inputs are invalid.\nUnstructured edits, edits that don\u0026rsquo;t limit themselves to a syntax or object structure, come in many forms. A common one in the configuration domain is the template or parameterization approach.\nIn this technique, the source material is sprinkled with strings of the form ${VAR}. A scanner replaces them with a value taken from a map using VAR as the map key. It\u0026rsquo;s trivial to implement.\nkustomize eschews parameterization, because\n The source yaml gets polluted with $VARs and can no longed be applied as is to the cluster (it must be processed). The source material is no longer structured, making it unusable with any YAML processor. It\u0026rsquo;s no longer data, it\u0026rsquo;s now logic that must be compiled. Errors in the output are disconnected from the edit that caused it. The input becomes unintelligible as the project scales in any number of dimensions (resource count, cluster count, environment count, etc.)  Kustomizations are meant to be sharable and stackable. Imagine tracing down a problem rooted in a clever set of stacked regexp replacements performed by various overlays on some remote base. We\u0026rsquo;ve used such systems, and never want to again.\nOther tools (sed, jinja, erb, envsubst, kafka, helm, ksonnet, etc.) provide varying degrees of unstructured editting and/or embedded languages, and can be used instead of, or in a pipe with, kustomize. If you want to go all-in on configuration as a language, consider cue.\nkustomize is going to stick to YAML in / YAML out.\nBuild-time side effects from CLI args or env variables kustomize supports the best practice of storing one\u0026rsquo;s entire configuration in a version control system.\nChanging kustomize build configuration output as a result of additional arguments or flags to build, or by consulting shell environment variable values in build code, would frustrate that goal.\nkustomize insteads offers kustomization file edit commands. Like any shell command, they can accept environment variable arguments.\nFor example, to set the tag used on an image to match an environment variable, run\nkustomize edit set image nginx:$MY_NGINX_VERSION as part of some encapsulating work flow executed before kustomize build.\nGlobs in kustomization files kustomize supports the best practice of storing one\u0026rsquo;s entire configuration in a version control system.\nGlobbing the local file system for files not explicitly declared in the kustomization file at kustomize build time would violate that goal.\nAllowing globbing in a kustomization file would also introduce the same problems as allowing globbing in java import declarations or BUILD/Makefile dependency rules.\nkustomize will instead provide kustomization file editting commands that accept globbed arguments, expand them at edit time relative to the local file system, and store the resulting explicit names into the kustomization file.\n","excerpt":"The maintainers established this list to place bounds on the kustomize feature set. The bounds can …","ref":"/faq/kustomize/eschewedfeatures/","title":"Eschewed Features"},{"body":"The maintainers established this list to place bounds on the kustomize feature set. The bounds can be changed with a consensus on the risks.\nFor a bigger picture about why kustomize does some things and not others, see the glossary entry for DAM.\nRemoval directives kustomize supports configurations that can be reasoned about as compositions or mixins - concepts that are widely accepted as a best practice in various programming languages.\nTo this end, kustomize offers various addition directives. One may add labels, annotations, patches, resources, bases, etc. Corresponding removal directives are not offered.\nRemoval semantics would introduce many possibilities for inconsistency, and the need to add code to detect, report and reject it. It would also allow, and possibly encourage, unnecessarily complex configuration layouts.\nWhen faced with a situation where removal is desirable, it\u0026rsquo;s always possible to remove things from a base like labels and annotations, and/or split multi-resource manifests into individual resource files - then add things back as desired via the kustomization.\nIf the underlying base is outside of one\u0026rsquo;s control, an OTS workflow is the recommended best practice. Fork the base, remove what you don\u0026rsquo;t want and commit it to your private fork, then use kustomize on your fork. As often as desired, use git rebase to capture improvements from the upstream base.\nUnstructured edits Structured edits are changes controlled by knowledge of the k8s API, and YAML or JSON syntax.\nMost edits performed by kustomize can be expressed as JSON patches or SMP patches. Those can be verbose, so common patches, like adding labels or annotatations, get dedicated transformer plugins - LabelTransformer, AnnotationsTransformer, etc. These accept relatively simple YAML configuration allowing easy targeting of any number of resources.\nAnother class of edits take data from one specific object\u0026rsquo;s field and use it in another (e.g. a service object\u0026rsquo;s name found and copied into a container\u0026rsquo;s command line). These reflection-style edits are called replacements.\nThe above edits create valid output given valid input, and can provide syntactically and semantically informed error messages if inputs are invalid.\nUnstructured edits, edits that don\u0026rsquo;t limit themselves to a syntax or object structure, come in many forms. A common one in the configuration domain is the template or parameterization approach.\nIn this technique, the source material is sprinkled with strings of the form ${VAR}. A scanner replaces them with a value taken from a map using VAR as the map key. It\u0026rsquo;s trivial to implement.\nkustomize eschews parameterization, because\n The source yaml gets polluted with $VARs and can no longed be applied as is to the cluster (it must be processed). The source material is no longer structured, making it unusable with any YAML processor. It\u0026rsquo;s no longer data, it\u0026rsquo;s now logic that must be compiled. Errors in the output are disconnected from the edit that caused it. The input becomes unintelligible as the project scales in any number of dimensions (resource count, cluster count, environment count, etc.)  Kustomizations are meant to be sharable and stackable. Imagine tracing down a problem rooted in a clever set of stacked regexp replacements performed by various overlays on some remote base. We\u0026rsquo;ve used such systems, and never want to again.\nOther tools (sed, jinja, erb, envsubst, kafka, helm, ksonnet, etc.) provide varying degrees of unstructured editting and/or embedded languages, and can be used instead of, or in a pipe with, kustomize. If you want to go all-in on configuration as a language, consider cue.\nkustomize is going to stick to YAML in / YAML out.\nBuild-time side effects from CLI args or env variables kustomize supports the best practice of storing one\u0026rsquo;s entire configuration in a version control system.\nChanging kustomize build configuration output as a result of additional arguments or flags to build, or by consulting shell environment variable values in build code, would frustrate that goal.\nkustomize insteads offers kustomization file edit commands. Like any shell command, they can accept environment variable arguments.\nFor example, to set the tag used on an image to match an environment variable, run\nkustomize edit set image nginx:$MY_NGINX_VERSION as part of some encapsulating work flow executed before kustomize build.\nGlobs in kustomization files kustomize supports the best practice of storing one\u0026rsquo;s entire configuration in a version control system.\nGlobbing the local file system for files not explicitly declared in the kustomization file at kustomize build time would violate that goal.\nAllowing globbing in a kustomization file would also introduce the same problems as allowing globbing in java import declarations or BUILD/Makefile dependency rules.\nkustomize will instead provide kustomization file editting commands that accept globbed arguments, expand them at edit time relative to the local file system, and store the resulting explicit names into the kustomization file.\n","excerpt":"The maintainers established this list to place bounds on the kustomize feature set. The bounds can …","ref":"/zh/faq/eschewedfeatures/","title":"Eschewed Features"},{"body":"Glossary application An application is a group of k8s resources related by some common purpose, e.g. a load balancer in front of a webserver backed by a database. Resource labelling, naming and metadata schemes have historically served to group resources together for collective operations like list and remove.\nThis proposal describes a new k8s resource called application to more formally describe this idea and provide support for application-level operations and dashboards.\nkustomize configures k8s resources, and the proposed application resource is just another resource.\napply The verb apply in the context of k8s refers to a kubectl command and an in-progress API endpoint for mutating a cluster.\nOne applies a statement of what one wants to a cluster in the form of a complete resource list.\nThe cluster merges this with the previously applied state and the actual state to arrive at a new desired state, which the cluster\u0026rsquo;s reconciliation loop attempts to create. This is the foundation of level-based state management in k8s.\nbase A base is a kustomization referred to by some other kustomization.\nAny kustomization, including an overlay, can be a base to another kustomization.\nA base has no knowledge of the overlays that refer to it.\nFor simple gitops management, a base configuration could be the sole content of a git repository dedicated to that purpose. Same with overlays. Changes in a repo could generate a build, test and deploy cycle.\nbespoke configuration A bespoke configuration is a kustomization and some resources created and maintained internally by some organization for their own purposes.\nThe workflow associated with a bespoke config is simpler than the workflow associated with an off-the-shelf config, because there\u0026rsquo;s no notion of periodically capturing someone else\u0026rsquo;s upgrades to the off-the-shelf config.\ncustom resource definition One can extend the k8s API by making a Custom Resource Definition (CRD spec).\nThis defines a custom resource (CD), an entirely new resource that can be used alongside native resources like ConfigMaps, Deployments, etc.\nKustomize can customize a CD, but to do so kustomize must also be given the corresponding CRD so that it can interpret the structure correctly.\ndeclarative application management Kustomize aspires to support Declarative Application Management, a set of best practices around managing k8s clusters.\nIn brief, kustomize should\n Work with any configuration, be it bespoke, off-the-shelf, stateless, stateful, etc. Support common customizations, and creation of variants (e.g. development vs. staging vs. production). Expose and teach native k8s APIs, rather than hide them. Add no friction to version control integration to support reviews and audit trails. Compose with other tools in a unix sense. Eschew crossing the line into templating, domain specific languages, etc., frustrating the other goals.  generator A generator makes resources that can be used as is, or fed into a transformer.\ngitops Devops or CICD workflows that use a git repository as a single source of truth and take action (e.g., build, test or deploy) when that truth changes.\nkustomization The term kustomization refers to a kustomization.yaml file, or more generally to a directory (the root) containing the kustomization.yaml file and all the relative file paths that it immediately references (all the local data that doesn\u0026rsquo;t require a URL specification).\nI.e. if someone gives you a kustomization for use with kustomize, it could be in the form of\n one file called kustomization.yaml, a tarball (containing that YAML file plus what it references), a git archive (ditto), a URL to a git repo (ditto), etc.  A kustomization file contains fields falling into four categories:\n  resources - what existing resources are to be customized. Example fields: resources, crds.\n  generators - what new resources should be created. Example fields: configMapGenerator (legacy), secretGenerator (legacy), generators (v2.1).\n  transformers - what to do to the aforementioned resources. Example fields: namePrefix, nameSuffix, images, commonLabels, patchesJson6902, etc. and the more general transformers (v2.1) field.\n  meta - fields which may influence all or some of the above. Example fields: vars, namespace, apiVersion, kind, etc.\n  kustomization root The directory that immediately contains a kustomization.yaml file.\nWhen a kustomization file is processed, it may or may not be able to access files outside its root.\nData files like resource YAML files, or text files containing name=value pairs intended for a ConfigMap or Secret, or files representing a patch to be used in a patch transformation, must live within or below the root, and as such are specified via relative paths exclusively.\nA special flag (in v2.1), --load_restrictions none, is provided to relax this security feature, to, say, allow a patch file to be shared by more than one kustomization.\nOther kustomizations (other directories containing a kustomization.yaml file) may be referred to by URL, by absolute path, or by relative path.\nIf kustomization A depends on kustomization B, then\n B may not contain A. B may not depend on A, even transitively.  A may contain B, but in this case it might be simplest to have A directly depend on B\u0026rsquo;s resources and eliminate B\u0026rsquo;s kustomization.yaml file (i.e. absorb B into A).\nConventionally, B is in a directory that\u0026rsquo;s sibling to A, or B is off in a completely independent git repository, referencable from any kustomization.\nA common layout is\n ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml └── overlays ├── dev │ ├── kustomization.yaml │ └── patch.yaml ├── prod │ ├── kustomization.yaml │ └── patch.yaml └── staging ├── kustomization.yaml └── patch.yaml  The three roots dev, prod and staging (presumably) all refer to the base root. One would have to inspect the kustomization.yaml files to be sure.\nkubernetes Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\nIt\u0026rsquo;s often abbreviated as k8s.\nkubernetes-style object An object, expressed in a YAML or JSON file, with the fields required by kubernetes. Basically just a kind field to identify the type, a metadata/name field to identify the particular instance, and an apiVersion field to identify the version (if there\u0026rsquo;s more than one version).\nkustomize kustomize is a command line tool supporting template-free, structured customization of declarative configuration targeted to k8s-style objects.\nTargeted to k8s means that kustomize has some understanding of API resources, k8s concepts like names, labels, namespaces, etc. and the semantics of resource patching.\nkustomize is an implementation of DAM.\noff-the-shelf configuration An off-the-shelf configuration is a kustomization and resources intentionally published somewhere for others to use.\nE.g. one might create a github repository like this:\n github.com/username/someapp/ kustomization.yaml deployment.yaml configmap.yaml README.md  Someone could then fork this repo (on github) and clone their fork to their local disk for customization.\nThis clone could act as a base for the user\u0026rsquo;s own overlays to do further customization.\noverlay An overlay is a kustomization that depends on another kustomization.\nThe kustomizations an overlay refers to (via file path, URI or other method) are called bases.\nAn overlay is unusable without its bases.\nAn overlay may act as a base to another overlay.\nOverlays make the most sense when there is more than one, because they create different variants of a common base - e.g. development, QA, staging and production environment variants.\nThese variants use the same overall resources, and vary in relatively simple ways, e.g. the number of replicas in a deployment, the CPU to a particular pod, the data source used in a ConfigMap, etc.\nOne configures a cluster like this:\n  kustomize build someapp/overlays/staging |\\ kubectl apply -f - kustomize build someapp/overlays/production |\\ kubectl apply -f -  Usage of the base is implicit - the overlay\u0026rsquo;s kustomization points to the base.\nSee also root.\npackage The word package has no meaning in kustomize, as kustomize is not to be confused with a package management tool in the tradition of, say, apt or rpm.\npatch General instructions to modify a resource.\nThere are two alternative techniques with similar power but different notation - the strategic merge patch and the JSON patch.\npatchStrategicMerge A patchStrategicMerge is strategic-merge-style patch (SMP).\nAn SMP looks like an incomplete YAML specification of a k8s resource. The SMP includes TypeMeta fields to establish the group/version/kind/name of the resource to patch, then just enough remaining fields to step into a nested structure to specify a new field value, e.g. an image tag.\nBy default, an SMP replaces values. This is usually desired when the target value is a simple string, but may not be desired when the target value is a list.\nTo change this default behavior, add a directive. Recognized directives in YAML patches are replace (the default) and delete (see these notes).\nNote that for custom resources, SMPs are treated as json merge patches.\nFun fact - any resource file can be used as an SMP, overwriting matching fields in another resource with the same group/version/kind/name, but leaving all other fields as they were.\nTODO(monopole): add ptr to example.\npatchJson6902 A patchJson6902 refers to a kubernetes resource and a JSONPatch specifying how to change the resource.\nA patchJson6902 can do almost everything a patchStrategicMerge can do, but with a briefer syntax. See this example.\nplugin A chunk of code used by kustomize, but not necessarily compiled into kustomize, to generate and/or transform a kubernetes resource as part of a kustomization.\nDetails here.\nresource A resource in the context of a REST-ful API is the target object of an HTTP operation like GET, PUT or POST. k8s offers a REST-ful API surface to interact with clients.\nA resource, in the context of a kustomization, is a root relative path to a YAML or JSON file describing a k8s API object, like a Deployment or a ConfigMap, or it\u0026rsquo;s a path to a kustomization, or a URL that resolves to a kustomization.\nMore generally, a resource can be any correct YAML file that defines an object with a kind and a metadata/name field.\nroot See kustomization root.\nsub-target / sub-application / sub-package A sub-whatever is not a thing. There are only bases and overlays.\ntarget The target is the argument to kustomize build, e.g.:\n  kustomize build $target  $target must be a path or a url to a kustomization.\nThe target contains, or refers to, all the information needed to create customized resources to send to the apply operation.\nA target can be a base or an overlay.\ntransformer A transformer can modify a resource, or merely visit it and collect information about it in the course of a kustomize build.\nvariant A variant is the outcome, in a cluster, of applying an overlay to a base.\nE.g., a staging and production overlay both modify some common base to create distinct variants.\nThe staging variant is the set of resources exposed to quality assurance testing, or to some external users who\u0026rsquo;d like to see what the next version of production will look like.\nThe production variant is the set of resources exposed to production traffic, and thus may employ deployments with a large number of replicas and higher cpu and memory requests.\n","excerpt":"Glossary application An application is a group of k8s resources related by some common purpose, e.g. …","ref":"/references/kustomize/glossary/","title":"Glossary"},{"body":"Running kustomize means one is running a particular version of a program (a CLI), using a particular version of underlying packages (a Go API), and reading a particular version of a kustomization file.\n If you\u0026rsquo;re having trouble with go get, please read Go API Versioning and be patient.\n CLI Program Versioning The command kustomize version prints a three field version tag (e.g. v3.0.0) that aspires to semantic versioning.\nThis notion of semver applies only to the CLI; the command names, their arguments and their flags.\nThe major version changes when some backward incompatibility appears in how the commands behave.\nInstallation See the installation docs.\nGo API Versioning The public methods in the public packages of module sigs.k8s.io/kustomize/api constitute the kustomize Go API.\nVersion sigs.k8s.io/kustomize/v3 and earlier In kustomize/v3 (and preceding major versions), the kustomize program and the API live the same Go module at sigs.k8s.io/kustomize, at import path sigs.k8s.io/kustomize/v3.\nThis has been fine for the CLI, but it presents a problem for the Go API.\nThe process around Go modules, in particular the notion of minimal version selection, demands that the module respect semver.\nAlmost all the code in module sigs.k8s.io/kustomize/v3 is exposed (not in a directory named internal). Even a minor refactor changing a method name or argument type in some deeply buried (but still public) method is a backward incompatible change. As a result, Go API semver hasn\u0026rsquo;t been followed. This was a mistake.\nSome options are\n  continue to ignore Go API semver and stick to CLI semver (eliminating the usefullness of minimal version selection),\n  obey semver, and increment the module\u0026rsquo;s major version number with every release (drastically reducing the usefullness of minimal version selection - since virtually all releases will be major),\n  slow down change in the huge API in favor of stability, yet somehow continue to deliver features,\n  drastically reduce the API surface, stabilize on semver there, and refactor as needed inside internal.\n  The last option seems the most appealing.\nThe first stable API version is coming The first stable API version will launch as the Go module\nsigs.k8s.io/kustomize/api The kustomize program itself (main.go and CLI specific code) will have moved out of sigs.k8s.io/kustomize and into the new module sigs.k8s.io/kustomize/kustomize. This is a submodule in the same repo, and it will retain its current notion of semver (e.g. a backward incompatible change in command behavior will trigger a major version bump). This module will not export packages; it\u0026rsquo;s just home to a main package.\nThe sigs.k8s.io/kustomize/api module will obey semver with a sustainable public surface, informed by current usage. Clients should import packages from this module, i.e. from import paths prefixed by sigs.k8s.io/kustomize/api/ at first, and later by sigs.k8s.io/kustomize/api/v2/. The kustomize binary itself is an API client requiring this module.\nThe clients and API will evolve independently.\nKustomization File Versioning The kustomization file is a struct that is part of the kustomize Go API (the sigs.k8s.io/kustomize module), but it also evolves as a k8s API object - it has an apiVersion field containing its own version number.\nField Change Policy  A field\u0026rsquo;s meaning cannot be changed. A field may be deprecated, then removed. Deprecation means triggering a minor (semver) version bump in the kustomize Go API, and defining a migration path in a non-fatal error message. Removal means triggering a major (semver) version bump in the kustomize Go API, and fatal error if field encountered (as with any unknown field). Likewise a change in apiVersion.  The edit fix Command This kustomize command reads a Kustomization file, converts deprecated fields to new fields, and writes it out again in the latest format.\nThis is a type version upgrade mechanism that works within major API revisions. There is no downgrade capability, as there\u0026rsquo;s no use case for it (see discussion below).\nExamples With the 2.0.0 release, there were three field removals:\n imageTag was deprecated when images was introduced, because the latter offers more general features for image data manipulation. imageTag was removed in v2.0.0. patches was deprecated and replaced by patchesStrategicMerge when patchesJson6902 was introduced, to make a clearer distinction between patch specification formats. patches was removed in v2.0.0. secretGenerator/commands was removed due to security concerns in v2.0.0 with no deprecation period.  The edit fix command in a v2.0.x binary will no longer recognize these fields.\nRelationship to the k8s API Review of k8s API versioning The k8s API has specific conventions and a process for making changes.\nThe presence of an apiVersion field in a k8s native type signals:\n its reliability level (alpha vs beta vs generally available), the existence of code to provide default values to fields not present in a serialization, the existence of code to provide both forward and backward conversion between different versions of types.  The k8s API promises a lossless conversion between versions over a specific range. This means that a recent client can write an object bearing the newest possible value for its version, the server will accept it and store it in \u0026ldquo;versionless\u0026rdquo; JSON form in storage, and can convert it to a range of older versions should an older client request data.\nFor native k8s types, this all requires writing Go code in the kubernetes core repo, to provide defaulting and conversions.\nFor CRDs, there\u0026rsquo;s a proposal on how to manage versioning (e.g. a remote service can offer type defaulting and conversions).\nDifferences  A k8s API server is able to go forward and backward in versioning, to work with older clients, over some range. The kustomize edit fix command only moves forward within a major API version.  At the time of writing, the YAML in a kustomization file does not represent a k8s API object, and the kustomize command and associated library is neither a server of, nor a client to, the k8s API.\nAdditional Kustomization file rules In addition to the field change policy described above, kustomization files conform to the following rules.\nEschew classic k8s fields Field names with dedicated meaning in k8s (metadata, spec, status, etc.) aren\u0026rsquo;t used. This is enforced via code review.\nDefault values for k8s kind and apiVersion In v3 or below, the two special k8s resource fields kind and apiVersion may be omitted from the kustomization file.\nIf either field is present, they both must be. If present, the value of kind must be:\n kind: Kustomization  If missing, the value of apiVersion defaults to\n apiVersion: kustomize.config.k8s.io/v1beta1  ","excerpt":"Running kustomize means one is running a particular version of a program (a CLI), using a particular …","ref":"/faq/kustomize/versioningpolicy/","title":"Versioning Policy"},{"body":"Running kustomize means one is running a particular version of a program (a CLI), using a particular version of underlying packages (a Go API), and reading a particular version of a kustomization file.\n If you\u0026rsquo;re having trouble with go get, please read Go API Versioning and be patient.\n CLI Program Versioning The command kustomize version prints a three field version tag (e.g. v3.0.0) that aspires to semantic versioning.\nThis notion of semver applies only to the CLI; the command names, their arguments and their flags.\nThe major version changes when some backward incompatibility appears in how the commands behave.\nInstallation See the installation docs.\nGo API Versioning The public methods in the public packages of module sigs.k8s.io/kustomize/api constitute the kustomize Go API.\nVersion sigs.k8s.io/kustomize/v3 and earlier In kustomize/v3 (and preceding major versions), the kustomize program and the API live the same Go module at sigs.k8s.io/kustomize, at import path sigs.k8s.io/kustomize/v3.\nThis has been fine for the CLI, but it presents a problem for the Go API.\nThe process around Go modules, in particular the notion of minimal version selection, demands that the module respect semver.\nAlmost all the code in module sigs.k8s.io/kustomize/v3 is exposed (not in a directory named internal). Even a minor refactor changing a method name or argument type in some deeply buried (but still public) method is a backward incompatible change. As a result, Go API semver hasn\u0026rsquo;t been followed. This was a mistake.\nSome options are\n  continue to ignore Go API semver and stick to CLI semver (eliminating the usefullness of minimal version selection),\n  obey semver, and increment the module\u0026rsquo;s major version number with every release (drastically reducing the usefullness of minimal version selection - since virtually all releases will be major),\n  slow down change in the huge API in favor of stability, yet somehow continue to deliver features,\n  drastically reduce the API surface, stabilize on semver there, and refactor as needed inside internal.\n  The last option seems the most appealing.\nThe first stable API version is coming The first stable API version will launch as the Go module\nsigs.k8s.io/kustomize/api The kustomize program itself (main.go and CLI specific code) will have moved out of sigs.k8s.io/kustomize and into the new module sigs.k8s.io/kustomize/kustomize. This is a submodule in the same repo, and it will retain its current notion of semver (e.g. a backward incompatible change in command behavior will trigger a major version bump). This module will not export packages; it\u0026rsquo;s just home to a main package.\nThe sigs.k8s.io/kustomize/api module will obey semver with a sustainable public surface, informed by current usage. Clients should import packages from this module, i.e. from import paths prefixed by sigs.k8s.io/kustomize/api/ at first, and later by sigs.k8s.io/kustomize/api/v2/. The kustomize binary itself is an API client requiring this module.\nThe clients and API will evolve independently.\nKustomization File Versioning The kustomization file is a struct that is part of the kustomize Go API (the sigs.k8s.io/kustomize module), but it also evolves as a k8s API object - it has an apiVersion field containing its own version number.\nField Change Policy  A field\u0026rsquo;s meaning cannot be changed. A field may be deprecated, then removed. Deprecation means triggering a minor (semver) version bump in the kustomize Go API, and defining a migration path in a non-fatal error message. Removal means triggering a major (semver) version bump in the kustomize Go API, and fatal error if field encountered (as with any unknown field). Likewise a change in apiVersion.  The edit fix Command This kustomize command reads a Kustomization file, converts deprecated fields to new fields, and writes it out again in the latest format.\nThis is a type version upgrade mechanism that works within major API revisions. There is no downgrade capability, as there\u0026rsquo;s no use case for it (see discussion below).\nExamples With the 2.0.0 release, there were three field removals:\n imageTag was deprecated when images was introduced, because the latter offers more general features for image data manipulation. imageTag was removed in v2.0.0. patches was deprecated and replaced by patchesStrategicMerge when patchesJson6902 was introduced, to make a clearer distinction between patch specification formats. patches was removed in v2.0.0. secretGenerator/commands was removed due to security concerns in v2.0.0 with no deprecation period.  The edit fix command in a v2.0.x binary will no longer recognize these fields.\nRelationship to the k8s API Review of k8s API versioning The k8s API has specific conventions and a process for making changes.\nThe presence of an apiVersion field in a k8s native type signals:\n its reliability level (alpha vs beta vs generally available), the existence of code to provide default values to fields not present in a serialization, the existence of code to provide both forward and backward conversion between different versions of types.  The k8s API promises a lossless conversion between versions over a specific range. This means that a recent client can write an object bearing the newest possible value for its version, the server will accept it and store it in \u0026ldquo;versionless\u0026rdquo; JSON form in storage, and can convert it to a range of older versions should an older client request data.\nFor native k8s types, this all requires writing Go code in the kubernetes core repo, to provide defaulting and conversions.\nFor CRDs, there\u0026rsquo;s a proposal on how to manage versioning (e.g. a remote service can offer type defaulting and conversions).\nDifferences  A k8s API server is able to go forward and backward in versioning, to work with older clients, over some range. The kustomize edit fix command only moves forward within a major API version.  At the time of writing, the YAML in a kustomization file does not represent a k8s API object, and the kustomize command and associated library is neither a server of, nor a client to, the k8s API.\nAdditional Kustomization file rules In addition to the field change policy described above, kustomization files conform to the following rules.\nEschew classic k8s fields Field names with dedicated meaning in k8s (metadata, spec, status, etc.) aren\u0026rsquo;t used. This is enforced via code review.\nDefault values for k8s kind and apiVersion In v3 or below, the two special k8s resource fields kind and apiVersion may be omitted from the kustomization file.\nIf either field is present, they both must be. If present, the value of kind must be:\n kind: Kustomization  If missing, the value of apiVersion defaults to\n apiVersion: kustomize.config.k8s.io/v1beta1  ","excerpt":"Running kustomize means one is running a particular version of a program (a CLI), using a particular …","ref":"/zh/faq/versioningpolicy/","title":"Versioning Policy"},{"body":"应用 应用是为某种目的关联起来的一组 Kubernetes 资源，例如一个前有负载均衡器，后有数据库的 Web 服务器。用标签、命名和元数据将[资源]组织起来，可以进行添加或删除等操作。\n有提案（Declarative Application Management）描述了一种称为应用的新的 Kubernetes 资源。更加正式的描述了这一思路，并在应用程序级别提供了运维和仪表盘的支持。\nKustomize 对 Kubernetes 资源进行配置，其中描述的应用程序资源只是另一种普通的资源。\napply Apply 这个动词在 Kubernetes 的上下文中，指的是一个 Kubernetes 命令以及能够对集群施加影响的进程内 API 端点。\n用户可以将对集群的运行要求用一组完整的资源列表的形式进行表达，通过 apply 命令进行提交。\n集群把新提交的资源和之前提交的状态以及当前的实际状态进行合并，形成新的状态。这就是 Kubernetes 的状态管理过程。\nbase Base 指的是会被其它 Kustomization 引用的 Kustomization。\n包括 Overlay 在内的任何 Kustomization，都可以作为其它 Kustomization 的 Base。\nBase 对引用自己的 Overlay 并无感知。\nBase 和 Overlay 可以作为 Git 仓库中的唯一内容，用于简单的 GitOps 管理。对仓库的变更可以触发构建、测试以及部署过程。\n定制配置 定制配置是由组织为满足自身需要，在内部创建和管理的 Kustomization 和[资源]。\n和定制配置关联的 Workflow 比关联到通用配置的 Workflow 要简单一些，原因是通用配置是共享的，需要周期性的跟踪他人作出的变更。\ncustom resource definition 可以通过定制 CRD (CRD spec) 的方式对 Kubernetes API 进行扩展。\nCRD 定义的[资源]是一种全新的资源，可以和 ConfigMap、Deployment 之类的原生资源以相同的方式来使用。\nKustomize 能够生成自定义资源，但是要完成这个目标，必须给出对应的 CRD，这样才能正确的对这种结构进行处理。\n声明式应用程序管理 Kustomize 鼓励对声明式应用程序管理（Declarative Application Management）的支持，这种方式是一系列 Kubernetes 集群管理的最佳实践。Kustomize 应该可以：\n 适用于任何配置，例如自有配置、共享配置、无状态、有状态等。 支持通用配置，以及创建变体（例如开发、预发布、生产）。 开放使用原生 Kubernetes API，而不是隐藏它们。 不会给版本控制系统和集成的评审和审计工作造成困难。 用 Unix 的风格和其它工具进行协作。 避免使用模板、领域特定的语言等额外的学习内容。  生成器 生成器生成的资源，可以直接使用，也可以输出给转换器（transformer）。\ngitops Kustomization 这个词可以指 kustomization.yaml 这个文件，更常见的情况是一个包含了 kustomization.yaml 及其所有直接引用文件的相对路径（所有不需要 URL 的本地数据）。\n也就是说，如果在 Kustomize 的上下文中说到 Kustomization，可能是以下的情况之一：\n 一个叫做 kustomization.yaml 的文件。 一个压缩包（包含 YAML 文件以及它的引用文件）。 一个 Git 压缩包。 一个 Git 仓库的 URL。  一个 Kustomization 文件包含的字段，分为四个类别：\n resources：待定制的现存[资源]，示例字段：resources、crds。 generator：将要创建的新资源，示例字段：configMapGenerator（传统）、secretGenerator（传统）、generators（v2.1） transformer：对前面提到的新旧资源进行处理的方式。示例字段：namePrefix、nameSuffix、images、commonLabels、patchesJson6902 等。在 v2.1 中还有更多的 transformer。 meta：会对上面几种字段产生影响。示例字段：vars、namespace、apiVersion、kind 等。  kustomization root 直接包含 kustomization.yaml 文件的目录。\n处理 Kustomization 文件时，可能访问到该目录以内或以外的文件。\n像 YAML 资源这样的数据文件，或者用于生成 ConfigMap 或 Secret 的包含 name=value 的文本文件，或者用于补丁转换的补丁文件，必须在这个目录的内部，需要显式的使用相对路径来表达。\nv2.1 中有一个特殊选项 --load_restrictions none 能够放宽这个限制，从而让不同的 Kustomization 可以共享补丁文件。\n可以用 URL、绝对路径或者相对路径引用其它的 Kustomization（包含 kustomization.yaml 文件的其它目录）。\n如果 A Kustomization 依赖 B Kustomization，那么：\n B 不能包含 A。 B 不能依赖 A，间接依赖也不可以。  A 可以包含 B，但是这样的话，最简单的方式可能是让 A 直接依赖 B 的资源，并去除 B 的 kustomization.yaml 文件（就是把 B 合并到 A）。\n通常情况下，B 和 A 处于同级目录，或者 B 放在一个完全独立的 Git 仓库里，可以从任意的 Kustomization 进行引用。\n常见布局大致如下：\n ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml └── overlays ├── dev │ ├── kustomization.yaml │ └── patch.yaml ├── prod │ ├── kustomization.yaml │ └── patch.yaml └── staging ├── kustomization.yaml └── patch.yaml  dev、prod 以及 staging 是否依赖于 base，要根据 kustomization.yaml 具体判断。\nkubernetes Kubernetes 是一个开源软件，为容器化应用提供了自动部署、伸缩和管理的能力。\n它经常会被简写为 k8s。\nKubernetes 风格的对象 用 YAML 或者 JSON 文件表达一个对象，其中包含一些必要字段。kind 字段用于标识对象类型，metadata/name 字段用于区分实例，apiVersion 表示的是版本（如果有多个版本的话）。\nkustomize kustomize 是一个面向 Kubernetes 的命令行工具，用一种无模板、结构化的的方式为为声明式配置提供定制支持。\n面向 Kubernetes 的意思是 Kustomize 对 API 资源、Kubernetes 概念（例如名称、标签、命名空间等）、以及资源补丁是有支持的。\nKustomize 是 DAM 的一个实现。\n通用配置 通用配置是一种用于共享的 Kustomization 以及资源。\n例如创建一个这样的 Github 仓库：\n github.com/username/someapp/ kustomization.yaml deployment.yaml configmap.yaml README.md  其他人可以 fork 这个仓库，并把它们的 Fork clone 到本地进行定制。\n用户可以用这个克隆回来的版本作为 Base，在此基础上定制 Overlay 来满足自身需求。\noverlay Overlay 是一个 依赖于其它 Kustomization 的 Kustomization。\nOverlay 引用（通过文件路径、URI 或者别的什么方式）的 Kustomization 被称为 Base。\nOverlay 无法脱离 Base 独立生效。\nOverlay 可以作为其它 Overlay 的 Base。\n通常 Overlay 都是不止一个的，因为实际情况中就需要为单一 Base 创建不同的[变体]，例如 development、QA、production 等。\n总的说来，这些变体使用的资源基本是一致的，只有一些简单的差异，例如 Deployment 的实例数量、特定 Pod 的 CPU 资源、ConfigMap 中的数据源定义等。\n可以这样把配置提交到集群：\n  kustomize build someapp/overlays/staging |\\ kubectl apply -f - kustomize build someapp/overlays/production |\\ kubectl apply -f -  对 Base 的使用是隐性的——Overlay 的依赖是指向 Base 的。\n请参考 root。\n包 在 Kustomize 中，包是没有意义的，Kustomize 并无意成为 apt、rpm 那样的传统包管理工具。\npatch 修改资源的通用指令。\n有两种功能类似但是实现不同的补丁方式：strategic merge patch 和 JSON patch。\npatchStrategicMerge patchStrategicMerge 是 strategic-merge 风格的补丁（SMP）。\nSMP 看上去像个不完整的 Kubernetes 资源 YAML。SMP 中包含 TypeMeta 字段，用于表明这个补丁的目标[资源]的 group/version/kind/name，剩余的字段是一个嵌套的结构，用于指定新的字段值，例如镜像标签。\n缺省情况下，SMP 会替换目标值。如果目标值是一个字符串，这种行为是合适的，但是如果目标值是个列表，可能就不合适了。\n可以加入 directive 来修改这种行为，，可以接受的 directive 包括 replace（缺省）、merge（不替换列表）、delete 等（相关说明）。\n注意对自定义资源来说，SMP 会被当作 json merge patches.\n有趣的事实：所有的资源文件都可以当作 SMP 使用，相同 group/version/kind/name 资源中的匹配字段会被替换，其它内容则保持不变。\npatchJson6902 patchJson6902 引用一个 Kubernetes 资源，并用 JSONPatch 指定了修改这一资源的方法。\npatchJson6902 几乎可以做到所有 patchStrategicMerge 的功能，但是语法更加简单，参考示例\n插件 Kustomize 可以使用的一段代码，但是无需编译到 Kustomize 内部，可以作为 Kustomization 的一部分，生成或转换 Kubernetes 资源。\n插件的细节。\n资源 在 REST-ful API 的上下文中，资源是 GET、PUT 或者 POST 等 HTTP 操作的目标。Kubernetes 提供了 REST-ful API 界面，用于和客户端进行交互。\n在 Kustomization 的上下文中，资源是一个相对于 root 的相对路径，指向 YAML 或者 JSON 文件，描述了一个 Kubernetes API 对象，例如 Deployment 或者 ConfigMap，或者一个 Kustomization、或者一个指向 Kustomization 的 URL。\n或者说任何定义了对象的格式正确的 YAML 文件，其中包含了 kind 和 metadata/name 字段，都是资源。\nroot 参看 kustomization root.\nsub-target / sub-application / sub-package 不存在 sub-xxx，只有 Base 和 Overlay。\ntarget target 是 kustomize build 的参数，例如：\n kustomize build $target  $target 必须是一个指向 Kustomization 的路径或者 URL。\n要创建用于进行 Apply 操作的资源，target 中必须包含或者引用所有相关信息。\nBase 或者 Overlay 都可以作为 target。\ntransformer 转换器能够修改资源，或者在 kustomize build 的过程中获取资源的信息。\n变体 在集群中把 Overlay 应用到 Base 上的产物称为变体。\n比如 staging 和 production 两个 Overlay，都修改了同样的 Base，来创建各自的变体。\nstaging 变体包含了一组用来保障测试过程的资源，或者一些想要看到生产环境下一个版本的外部用户。\nproduction 变体用于承载生产流量，可能使用大量的副本，分配更多的 CPU 和内存。\n","excerpt":"应用 应用是为某种目的关联起来的一组 Kubernetes 资源，例如一个前有负载均衡器，后有数据库的 Web 服务器。用标签、命名和元数据将[资源]组织起来，可以进行添加或删除等操作。\n有提 …","ref":"/zh/api-reference/glossary/","title":"术语表"},{"body":"Summary of changes First release of the Go API-only module Many of the PRs since the last vrelease were around restructuring the sigs.k8s.io/kustomize repository into three Go modules instead of just one.\nThe reasons for this are detailed in the versioning policy documentation, and what it means for releasing is explained in the release process documentation.\nThe tl;dr is that the top level module sigs.k8s.io/kustomize now defines the kustomize Go API, and the kustomize CLI sits below it in an independent module sigs.k8s.io/kustomize/kustomize.\nThe modules release independently, though in practice a new release of the kustomize Go API will likely be followed quickly by a new release of the kustomize executable.\nThis is a necessary step to creating a much smaller kustomize Go API surface that has some hope of conforming to semantic versioning and being of some use to clients.\nThe kustomize CLI will see the same kustomize Go API as any other client.\nThe new semver-able API will begin with v4.0.0 (not yet released) and be a clean break with v3 etc.\nChange log since v3.2.0 3c9d828f - Have kustomize CLI depend on kustomize Go API v3.3.0 (Jeffrey Regan) 5d800f0b - Merge pull request #1595 from monopole/threeReleases (Jeff Regan) 4eb2d5bc - Three builders. (Jeffrey Regan) 988af1ff - Update README.md (Jeff Regan) 1617183e - Merge pull request #1590 from monopole/releaseProcessUpdate (Kubernetes Prow Robot) ee727464 - update release process doc (jregan) c9e7dc3b - Merge pull request #1589 from monopole/moreTestsAroundKustFileName (Jeff Regan) 07e0e46a - improve tests for alternative kustomization file names (Jeffrey Regan) 404d2d63 - Merge pull request #1587 from monopole/reducePgmconfig (Jeff Regan) baa0296a - Reduce size of pgmconfig package (Jeffrey Regan) 0f665ac1 - Merge pull request #1544 from ptux/add-transformer-href (Jeff Regan) 14b0a650 - Merge pull request #1581 from monopole/refactorFs (Jeff Regan) 2d58f8b8 - Break the dep between fs and pgmconfig. (Jeffrey Regan) 9a43ca53 - Merge pull request #1578 from nlamirault/fix/build-plugins-doc (Jeff Regan) 5372fc6f - Merge pull request #1579 from monopole/fsPackageCleanup (Jeff Regan) 86bc3440 - Merge pull request #1513 from nimohunter/fix_empty_list_item (Kubernetes Prow Robot) a014f7d4 - Merge pull request #1561 from beautytiger/dev-190925 (Jeff Regan) 9a94bcb8 - Improve fs package and doc in prep to officially go public (Jeffrey Regan) 07634ef0 - Merge pull request #1575 from monopole/versioning (Jeff Regan) 995f88d6 - Update versioning notes. (jregan) 334a6467 - Fix: documentation link for plugins (Nicolas Lamirault) 08963ba5 - improve test code coverage in transformers (Guangming Wang) 326fb689 - Merge pull request #1570 from bzub/1234-go_plugin_doc (Jeff Regan) 970ce67c - Update goPluginCaveats.md (Jeff Regan) 98d18930 - Update INSTALL.md (Jeff Regan) d89b448c - Fix git tag recovery in cloud build. (Jeff Regan) 17bf9d32 - Update releasing README. (Jeff Regan) a99aff1d - Merge pull request #1571 from monopole/updateCloudBuildProcess (Kubernetes Prow Robot) a694ac7b - Update cloud build process for kustomize. (Jeffrey Regan) b5b11ef6 - Fix compile kustomize example. (bzub) fa1af6f5 - Merge pull request #1473 from richardmarshall/execpluginhash (Jeff Regan) 9288dec0 - Fix failing BashedConfigMapTest (Jeff Regan) 1a45dd0b - Merge pull request #1566 from monopole/releaseNotes3.2.1 (Kubernetes Prow Robot) 592c5acf - docs: Exec plugin generator options (Richard Marshall) ac9424fa - tests: Add unit tests for update resource options (Richard Marshall) 79fbe7c4 - Support resource generator options in exec plugins (Richard Marshall) f69d526f - v3.2.1 release notes (Jeff Regan) 07a95a60 - Merge pull request #1565 from monopole/tweakBinaryDepsBeforeTagging (Jeff Regan) 032b3857 - Pin the kustomize binary's dependence on kustomize libs. (jregan) 81062959 - Merge pull request #1564 from monopole/moveKustomizeBinaryToOwnModule (Kubernetes Prow Robot) b82a8fd3 - Move the kustomize binary to its own module. (Jeffrey Regan) 2d0c22d6 - Merge pull request #1562 from keleustes/tools (Kubernetes Prow Robot) aa342def - Pin tool versions using go modules (Ian Howell) 10786ec0 - Merge pull request #1554 from keleustes/readme (Kubernetes Prow Robot) 7c705687 - Update README.md to include Kubernetes 1.16 (Jerome Brette) e8933d97 - Merge pull request #1560 from monopole/precommitTuneup (Jeff Regan) 9d7b6544 - Make pre-commit more portable and less tricky. (jregan) 7a0946a9 - Merge pull request #1558 from monopole/dependOnNewPluginatorModule (Jeff Regan) def4f045 - Depend on new pluginator location. (Jeffrey Regan) 2f2408f1 - Merge pull request #1559 from monopole/compressCopyright (Jeff Regan) 3b9bcc48 - Compress copyright in the commands package. (Jeffrey Regan) d0429ff4 - Merge pull request #1557 from monopole/pluginatorModule (Jeff Regan) 33deefc3 - Copy pluginator to its own module. (Jeffrey Regan) 9b3de82b - Merge pull request #1506 from Liujingfang1/release (Jeff Regan) d217074f - Merge pull request #1550 from keleustes/apiversion (Kubernetes Prow Robot) 1d90ba7c - Fix typo in apiVersion yaml declaration (Jerome Brette) eeeb4c36 - Merge pull request #1547 from keleustes/extensions (Kubernetes Prow Robot) b1faa989 - Update Ingress apiVersion to networking.k8s.io/v1beta1 (Jerome Brette) d8250c9e - move test case (nimohunter) c9500466 - add transformer href (Wang(わん)) 0c32691e - Merge pull request #1537 from jaypipes/gomod-install-note (Kubernetes Prow Robot) 88b1d627 - Merge pull request #1541 from rtnpro/patch-1 (Jeff Regan) aec82066 - Update INSTALL.md (Jeff Regan) 20c2b53a - Merge pull request #1542 from monopole/tweakFilePathsInTest (Jeff Regan) 274b5c3b - Tweak file path handling and logging in test. (Jeffrey Regan) b1fdaa23 - Fix typo in transformerconfigs README (Ratnadeep Debnath) b5d5e70b - empty list or map item return error (nimohunter) 2e829853 - empty list or map item return error (nimohunter) 55941f57 - add note about GO111MODULE for source install (Jay Pipes) 9e226001 - empty list or map item return error (nimohunter) 77b63f96 - add release note for v3.2.0 (jingfangliu) ","excerpt":"Summary of changes First release of the Go API-only module Many of the PRs since the last vrelease …","ref":"/blog/2019/10/24/v3.3.0/","title":"v3.3.0"},{"body":"Summary of changes First release of the Go API-only module Many of the PRs since the last vrelease were around restructuring the sigs.k8s.io/kustomize repository into three Go modules instead of just one.\nThe reasons for this are detailed in the versioning policy documentation, and what it means for releasing is explained in the release process documentation.\nThe tl;dr is that the top level module sigs.k8s.io/kustomize now defines the kustomize Go API, and the kustomize CLI sits below it in an independent module sigs.k8s.io/kustomize/kustomize.\nThe modules release independently, though in practice a new release of the kustomize Go API will likely be followed quickly by a new release of the kustomize executable.\nThis is a necessary step to creating a much smaller kustomize Go API surface that has some hope of conforming to semantic versioning and being of some use to clients.\nThe kustomize CLI will see the same kustomize Go API as any other client.\nThe new semver-able API will begin with v4.0.0 (not yet released) and be a clean break with v3 etc.\nChange log since v3.2.0 3c9d828f - Have kustomize CLI depend on kustomize Go API v3.3.0 (Jeffrey Regan) 5d800f0b - Merge pull request #1595 from monopole/threeReleases (Jeff Regan) 4eb2d5bc - Three builders. (Jeffrey Regan) 988af1ff - Update README.md (Jeff Regan) 1617183e - Merge pull request #1590 from monopole/releaseProcessUpdate (Kubernetes Prow Robot) ee727464 - update release process doc (jregan) c9e7dc3b - Merge pull request #1589 from monopole/moreTestsAroundKustFileName (Jeff Regan) 07e0e46a - improve tests for alternative kustomization file names (Jeffrey Regan) 404d2d63 - Merge pull request #1587 from monopole/reducePgmconfig (Jeff Regan) baa0296a - Reduce size of pgmconfig package (Jeffrey Regan) 0f665ac1 - Merge pull request #1544 from ptux/add-transformer-href (Jeff Regan) 14b0a650 - Merge pull request #1581 from monopole/refactorFs (Jeff Regan) 2d58f8b8 - Break the dep between fs and pgmconfig. (Jeffrey Regan) 9a43ca53 - Merge pull request #1578 from nlamirault/fix/build-plugins-doc (Jeff Regan) 5372fc6f - Merge pull request #1579 from monopole/fsPackageCleanup (Jeff Regan) 86bc3440 - Merge pull request #1513 from nimohunter/fix_empty_list_item (Kubernetes Prow Robot) a014f7d4 - Merge pull request #1561 from beautytiger/dev-190925 (Jeff Regan) 9a94bcb8 - Improve fs package and doc in prep to officially go public (Jeffrey Regan) 07634ef0 - Merge pull request #1575 from monopole/versioning (Jeff Regan) 995f88d6 - Update versioning notes. (jregan) 334a6467 - Fix: documentation link for plugins (Nicolas Lamirault) 08963ba5 - improve test code coverage in transformers (Guangming Wang) 326fb689 - Merge pull request #1570 from bzub/1234-go_plugin_doc (Jeff Regan) 970ce67c - Update goPluginCaveats.md (Jeff Regan) 98d18930 - Update INSTALL.md (Jeff Regan) d89b448c - Fix git tag recovery in cloud build. (Jeff Regan) 17bf9d32 - Update releasing README. (Jeff Regan) a99aff1d - Merge pull request #1571 from monopole/updateCloudBuildProcess (Kubernetes Prow Robot) a694ac7b - Update cloud build process for kustomize. (Jeffrey Regan) b5b11ef6 - Fix compile kustomize example. (bzub) fa1af6f5 - Merge pull request #1473 from richardmarshall/execpluginhash (Jeff Regan) 9288dec0 - Fix failing BashedConfigMapTest (Jeff Regan) 1a45dd0b - Merge pull request #1566 from monopole/releaseNotes3.2.1 (Kubernetes Prow Robot) 592c5acf - docs: Exec plugin generator options (Richard Marshall) ac9424fa - tests: Add unit tests for update resource options (Richard Marshall) 79fbe7c4 - Support resource generator options in exec plugins (Richard Marshall) f69d526f - v3.2.1 release notes (Jeff Regan) 07a95a60 - Merge pull request #1565 from monopole/tweakBinaryDepsBeforeTagging (Jeff Regan) 032b3857 - Pin the kustomize binary's dependence on kustomize libs. (jregan) 81062959 - Merge pull request #1564 from monopole/moveKustomizeBinaryToOwnModule (Kubernetes Prow Robot) b82a8fd3 - Move the kustomize binary to its own module. (Jeffrey Regan) 2d0c22d6 - Merge pull request #1562 from keleustes/tools (Kubernetes Prow Robot) aa342def - Pin tool versions using go modules (Ian Howell) 10786ec0 - Merge pull request #1554 from keleustes/readme (Kubernetes Prow Robot) 7c705687 - Update README.md to include Kubernetes 1.16 (Jerome Brette) e8933d97 - Merge pull request #1560 from monopole/precommitTuneup (Jeff Regan) 9d7b6544 - Make pre-commit more portable and less tricky. (jregan) 7a0946a9 - Merge pull request #1558 from monopole/dependOnNewPluginatorModule (Jeff Regan) def4f045 - Depend on new pluginator location. (Jeffrey Regan) 2f2408f1 - Merge pull request #1559 from monopole/compressCopyright (Jeff Regan) 3b9bcc48 - Compress copyright in the commands package. (Jeffrey Regan) d0429ff4 - Merge pull request #1557 from monopole/pluginatorModule (Jeff Regan) 33deefc3 - Copy pluginator to its own module. (Jeffrey Regan) 9b3de82b - Merge pull request #1506 from Liujingfang1/release (Jeff Regan) d217074f - Merge pull request #1550 from keleustes/apiversion (Kubernetes Prow Robot) 1d90ba7c - Fix typo in apiVersion yaml declaration (Jerome Brette) eeeb4c36 - Merge pull request #1547 from keleustes/extensions (Kubernetes Prow Robot) b1faa989 - Update Ingress apiVersion to networking.k8s.io/v1beta1 (Jerome Brette) d8250c9e - move test case (nimohunter) c9500466 - add transformer href (Wang(わん)) 0c32691e - Merge pull request #1537 from jaypipes/gomod-install-note (Kubernetes Prow Robot) 88b1d627 - Merge pull request #1541 from rtnpro/patch-1 (Jeff Regan) aec82066 - Update INSTALL.md (Jeff Regan) 20c2b53a - Merge pull request #1542 from monopole/tweakFilePathsInTest (Jeff Regan) 274b5c3b - Tweak file path handling and logging in test. (Jeffrey Regan) b1fdaa23 - Fix typo in transformerconfigs README (Ratnadeep Debnath) b5d5e70b - empty list or map item return error (nimohunter) 2e829853 - empty list or map item return error (nimohunter) 55941f57 - add note about GO111MODULE for source install (Jay Pipes) 9e226001 - empty list or map item return error (nimohunter) 77b63f96 - add release note for v3.2.0 (jingfangliu) ","excerpt":"Summary of changes First release of the Go API-only module Many of the PRs since the last vrelease …","ref":"/zh/blog/2019/10/24/v3.3.0/","title":"v3.3.0"},{"body":"This is a patch release, with no new features from 3.2.0.\nIt reflects a change in dependence.\nThe kustomize binary is now built as a client, with no special consideration, of the set of public packages represented by the Go module at [https://github.com/kubernetes-sigs/kustomize].\nkustomize the binary is now a client of the kustomize API represented by the public package surface presented by https://github.com/kubernetes-sigs/kustomize/v{whatever}\n","excerpt":"This is a patch release, with no new features from 3.2.0.\nIt reflects a change in dependence.\nThe …","ref":"/blog/2019/09/26/v3.2.1/","title":"v3.2.1"},{"body":"This is a patch release, with no new features from 3.2.0.\nIt reflects a change in dependence.\nThe kustomize binary is now built as a client, with no special consideration, of the set of public packages represented by the Go module at [https://github.com/kubernetes-sigs/kustomize].\nkustomize the binary is now a client of the kustomize API represented by the public package surface presented by https://github.com/kubernetes-sigs/kustomize/v{whatever}\n","excerpt":"This is a patch release, with no new features from 3.2.0.\nIt reflects a change in dependence.\nThe …","ref":"/zh/blog/2019/09/26/v3.2.1/","title":"v3.2.1"},{"body":"Inline Patch Since this version, Kustomize allows inline patches in all three of patchesStrategicMerge, patchesJson6902 and patches. Take a look at inline patch.\nNew Subcommand Since this version, one can create a kustomization.yaml file in a directory through a create subcommand.\nCreate a new overlay from the base ../base\nkustomize create --resources ../base Create a new kustomization detecing resources in the current directory\nkustomize create --autodetect Once can also add all resources in the current directory recursively by\nkustomize create --autodetect --recursive New Example Generator A new example generator of using go-getter to download resources is added. Take a look at go-getter generator.\n","excerpt":"Inline Patch Since this version, Kustomize allows inline patches in all three of …","ref":"/blog/2019/09/17/v3.2.0/","title":"v3.2.0"},{"body":"Inline Patch Since this version, Kustomize allows inline patches in all three of patchesStrategicMerge, patchesJson6902 and patches. Take a look at inline patch.\nNew Subcommand Since this version, one can create a kustomization.yaml file in a directory through a create subcommand.\nCreate a new overlay from the base ../base\nkustomize create --resources ../base Create a new kustomization detecing resources in the current directory\nkustomize create --autodetect Once can also add all resources in the current directory recursively by\nkustomize create --autodetect --recursive New Example Generator A new example generator of using go-getter to download resources is added. Take a look at go-getter generator.\n","excerpt":"Inline Patch Since this version, Kustomize allows inline patches in all three of …","ref":"/zh/blog/2019/09/17/v3.2.0/","title":"v3.2.0"},{"body":"Extended patches Since this version, Kustomize allows applying one patch to multiple resources. This works for both Strategic Merge Patch and JSON Patch. Take a look at patch multiple objects.\nImproved Resource Matching Multiple improvements have been made to allow the user to leverage \u0026ldquo;namespace\u0026rdquo; instead/or with \u0026ldquo;name suffix/prefix\u0026rdquo; to segregate resources.\nPatch resolution improvement The following example demonstrates how using the namespace field in the patch definition, will let the user define two different patches against two different Deployment having the same \u0026ldquo;deploy1\u0026rdquo; name but in different namespaces in the same Kustomize context/folder. Unless the namespace: field has been specified in the kustomization.yaml, no namespace value will be handled as Kubernetes default namespace.\napiVersion:apps/v1kind:Deploymentmetadata:name:deploy1namespace:mainspec:template:spec:containers:- name:nginxenv:- name:ANOTHERENVvalue:TESTVALUE---apiVersion:apps/v1kind:Deploymentmetadata:name:deploy1namespace:productionspec:template:spec:containers:- name:mainenv:- name:ANOTHERENVvalue:PRODVALUEVariable resolution improvement It is possible to add namespace field to the variable declaration. In the following example, two Service objects with the same elasticsearch name have been declared. Specifying the namespace in the objRef of the corresponding varriables, allows Kustomize to resovlve thoses variables. If the namespace is not specified, Kustomize will handle it has a \u0026ldquo;wildcard\u0026rdquo; value.\nExtract of kustomization.yaml:\nvars:- name:elasticsearch-test-protocolobjref:kind:Servicename:elasticsearchnamespace:testapiVersion:v1fieldref:fieldpath:spec.ports[0].protocol- name:elasticsearch-dev-protocolobjref:kind:Servicename:elasticsearchnamespace:devapiVersion:v1fieldref:fieldpath:spec.ports[0].protocolSimultaneous change of names and namespaces Kustomize is now able to deal with simultaneous changes of name and namespace. Special attention has been paid the handling of:\n ClusterRoleBinding/RoleBinding \u0026ldquo;subjects\u0026rdquo; field, ValidatingWebhookConfiguration \u0026ldquo;webhooks\u0026rdquo; field.  The user should be able to use a kustomization.yaml as shown in the example bellow even if ClusterRoleBind,RoleBinding and ValidatingWebookConfiguration are part of the resources he needs to declare.\nExtract of kustomization.yaml:\nnamePrefix:pfx-nameSuffix:-sfxnamespace:testnamespaceresources:...Resource and Kustomize Context matching Kustomize is now able to support more aggregation patterns.\nIf for instance, the top level of kustomization.yaml, is simply combining sub-components, (as in the following example), Kustomize has improved resource matching capabilities. This removes some of the constraints which were present on the utilization of prefix/suffix and namespace transformers in the individual components.\nresources:- ../component1- ../component2- ../component3Other improvements  Image transformation has been improved. This allows the user to update the sha256 of an image with another sha256. Multiple default transformer configuration entries have been added, removing the need for the user to add them as part of the configurations: section of the kustomization.yaml. kustomize help command has been tidied up.  ","excerpt":"Extended patches Since this version, Kustomize allows applying one patch to multiple resources. This …","ref":"/blog/2019/07/26/v3.1.0/","title":"v3.1.0"},{"body":"Extended patches Since this version, Kustomize allows applying one patch to multiple resources. This works for both Strategic Merge Patch and JSON Patch. Take a look at patch multiple objects.\nImproved Resource Matching Multiple improvements have been made to allow the user to leverage \u0026ldquo;namespace\u0026rdquo; instead/or with \u0026ldquo;name suffix/prefix\u0026rdquo; to segregate resources.\nPatch resolution improvement The following example demonstrates how using the namespace field in the patch definition, will let the user define two different patches against two different Deployment having the same \u0026ldquo;deploy1\u0026rdquo; name but in different namespaces in the same Kustomize context/folder. Unless the namespace: field has been specified in the kustomization.yaml, no namespace value will be handled as Kubernetes default namespace.\napiVersion:apps/v1kind:Deploymentmetadata:name:deploy1namespace:mainspec:template:spec:containers:- name:nginxenv:- name:ANOTHERENVvalue:TESTVALUE---apiVersion:apps/v1kind:Deploymentmetadata:name:deploy1namespace:productionspec:template:spec:containers:- name:mainenv:- name:ANOTHERENVvalue:PRODVALUEVariable resolution improvement It is possible to add namespace field to the variable declaration. In the following example, two Service objects with the same elasticsearch name have been declared. Specifying the namespace in the objRef of the corresponding varriables, allows Kustomize to resovlve thoses variables. If the namespace is not specified, Kustomize will handle it has a \u0026ldquo;wildcard\u0026rdquo; value.\nExtract of kustomization.yaml:\nvars:- name:elasticsearch-test-protocolobjref:kind:Servicename:elasticsearchnamespace:testapiVersion:v1fieldref:fieldpath:spec.ports[0].protocol- name:elasticsearch-dev-protocolobjref:kind:Servicename:elasticsearchnamespace:devapiVersion:v1fieldref:fieldpath:spec.ports[0].protocolSimultaneous change of names and namespaces Kustomize is now able to deal with simultaneous changes of name and namespace. Special attention has been paid the handling of:\n ClusterRoleBinding/RoleBinding \u0026ldquo;subjects\u0026rdquo; field, ValidatingWebhookConfiguration \u0026ldquo;webhooks\u0026rdquo; field.  The user should be able to use a kustomization.yaml as shown in the example bellow even if ClusterRoleBind,RoleBinding and ValidatingWebookConfiguration are part of the resources he needs to declare.\nExtract of kustomization.yaml:\nnamePrefix:pfx-nameSuffix:-sfxnamespace:testnamespaceresources:...Resource and Kustomize Context matching Kustomize is now able to support more aggregation patterns.\nIf for instance, the top level of kustomization.yaml, is simply combining sub-components, (as in the following example), Kustomize has improved resource matching capabilities. This removes some of the constraints which were present on the utilization of prefix/suffix and namespace transformers in the individual components.\nresources:- ../component1- ../component2- ../component3Other improvements  Image transformation has been improved. This allows the user to update the sha256 of an image with another sha256. Multiple default transformer configuration entries have been added, removing the need for the user to add them as part of the configurations: section of the kustomization.yaml. kustomize help command has been tidied up.  ","excerpt":"Extended patches Since this version, Kustomize allows applying one patch to multiple resources. This …","ref":"/zh/blog/2019/07/26/v3.1.0/","title":"v3.1.0"},{"body":"This release is basically v2.1.0, with many post-v2.1.0 bugs fixed (in about 150 commits) and a v3 in Go package paths.\nThe major version increment to v3 puts a new floor on a stable API for plugin developers (both Go plugin developers and exec plugin developers who happen to use Go).\nWhy so soon after v2.1.0 We made a mistake - v2.1.0 should have been v3.0.0. Per the Go modules doc (which have improved a great deal recently), a release that\u0026rsquo;s already tagged v2 or higher should increment the major version when performing their first Go module-based release.\nThis advice applies to kustomize, since it was already at major version 2 when it began using Go modules to state its own dependencies in v2.1.0.\nBut the more important reason for v3 is a change to the kustomize versioning policy, forced by the introduction of plugins.\nHistorically, kustomize\u0026rsquo;s versioning policy didn\u0026rsquo;t involve Go modules and addressed only the command line tool\u0026rsquo;s behavior and the fields in a kustomization file. The underlying packages were an implementation detail, not under semantic versioning, because they weren\u0026rsquo;t intended for export (and should have all been under internal). Thus although the v2.1.0 CLI is backward compatible with v2.0.3, the underlying package APIs are not.\nWith Go modules, the go tool must assume that Go packages respect semantic versioning, so it can perform minimal version selection.\nWith the introduction of alpha plugins, kustomize sub-packages - in particular loader and resmap - become part of an API formally exposed to plugin authors, and so must be semantically versioned. This allows plugins defined in other repositories to clarify that they depend on kustomize v3.0.0, and not see confusing errors arising from incompatibilities between v2.1.0 and v2.0.3. Hence, the jump to v3.\nAside - the set of kustomize packages outside internal is too large, and over time, informed by package use, this API surface must shrink. Such shrinkage will trigger a major version increment.\n","excerpt":"This release is basically v2.1.0, with many post-v2.1.0 bugs fixed (in about 150 commits) and a v3 …","ref":"/blog/2019/07/03/v3.0.0/","title":"v3.0.0"},{"body":"This release is basically v2.1.0, with many post-v2.1.0 bugs fixed (in about 150 commits) and a v3 in Go package paths.\nThe major version increment to v3 puts a new floor on a stable API for plugin developers (both Go plugin developers and exec plugin developers who happen to use Go).\nWhy so soon after v2.1.0 We made a mistake - v2.1.0 should have been v3.0.0. Per the Go modules doc (which have improved a great deal recently), a release that\u0026rsquo;s already tagged v2 or higher should increment the major version when performing their first Go module-based release.\nThis advice applies to kustomize, since it was already at major version 2 when it began using Go modules to state its own dependencies in v2.1.0.\nBut the more important reason for v3 is a change to the kustomize versioning policy, forced by the introduction of plugins.\nHistorically, kustomize\u0026rsquo;s versioning policy didn\u0026rsquo;t involve Go modules and addressed only the command line tool\u0026rsquo;s behavior and the fields in a kustomization file. The underlying packages were an implementation detail, not under semantic versioning, because they weren\u0026rsquo;t intended for export (and should have all been under internal). Thus although the v2.1.0 CLI is backward compatible with v2.0.3, the underlying package APIs are not.\nWith Go modules, the go tool must assume that Go packages respect semantic versioning, so it can perform minimal version selection.\nWith the introduction of alpha plugins, kustomize sub-packages - in particular loader and resmap - become part of an API formally exposed to plugin authors, and so must be semantically versioned. This allows plugins defined in other repositories to clarify that they depend on kustomize v3.0.0, and not see confusing errors arising from incompatibilities between v2.1.0 and v2.0.3. Hence, the jump to v3.\nAside - the set of kustomize packages outside internal is too large, and over time, informed by package use, this API surface must shrink. Such shrinkage will trigger a major version increment.\n","excerpt":"This release is basically v2.1.0, with many post-v2.1.0 bugs fixed (in about 150 commits) and a v3 …","ref":"/zh/blog/2019/07/03/v3.0.0/","title":"v3.0.0"},{"body":"Go modules, resource ordering respected, generator and transformer plugins, eased loading restrictions, the notion of inventory, eased replica count modification. About ~90 issues closed since v2.0.3 in ~400 commits.\nDownload here.\nGo modules Kustomize now defines its dependencies in a top level go.mod file. This is the first step towards a package structure intentially exported as one or more Go modules for use in other programs (kubectl, kubebuilder, etc.) and in kustomize plugins (see below).\nResource ordering Kustomize now retains the depth-first order of resources as read, a frequently requested feature.\nThis means resource order can be controlled by editting kustomization files. This is also vital to applying user-defined transformations (plugins) in a particular order.\nNothing needs to be done to activate this; it happens automatically.\nThe build command now accepts a --reorder flag with values legacy and none, with a default value of legacy.\nlegacy means apply an ordering based on GVK, that currently emits Namespace objects first, and ValidatingWebhookConfiguration objects last. This means that despite automatic retention of load order, your build output won\u0026rsquo;t change by default.\nnone means don\u0026rsquo;t reorder the resources before output. Specify this to see output order respect input order.\nGenerator and transformer plugins Since the beginning (as kinflate back in Sep 2017), kustomize has read or generated resources, applied a series of pipelined transformation to them, and emitted the result to stdout.\nAt that time, the only way to change the behavior of a generator (e.g. a secret generator), or change the behavior of a transformer (e.g. a name changer, or json patcher), was to modify source code and put out a release.\nv1.0.9 introduced generator options as a means to change the behavior of the only two generators available at the time - Secret and ConfigMap generators. It also introduced transformer configs as a way to fine tune the targets of transformations (e.g. to which fields selectors should be added). Most of the feature requests for kustomize revolve around changing the behavior of the builtin generators and transformers.\nv2.1.0 adds an alpha plugin framework, that encourages users to write their own generators or transformers, declaring them as kubernetes objects just like everything else, and apply them as part of the kustomize build process.\nTo inform the API exposed to plugins, and to confirm that the plugin framework can offer plugin authors the same capabilities as builtin operations, all the builtin generators and tranformers have been converted to plugin form (with one exceptions awaiting Go module refinements). This means that adding, say, a secretGenerator or commonAnnotations directive to your kustomization will (in v2.1.0) trigger execution of code committed as a plugin.\nFor more information, see the kustomize plugin documentation.\nRemove load restrictions The following usage:\nkustomize build --load_restrictor none $target allows a kustomization.yaml file used in this build to refer to files outside its own directory (i.e. outside its root).\nThis is an opt-in to suppress a security feature that denies this precise behavior.\nThis feature should only be used to allow multiple overlays (e.g. prod, staging and dev) to share a patch file. To share resources, use a relative path or URL to a kustomization directory in the resources directive.\nInventory generation for pruning Alpha\nUsers can add an inventory stanza to their kustomization file, to add a special inventory object to the build result.\nThis object applies to the cluster along with everything else in the build result and can be used by other clients to intelligently prune orphaned cluster resources.\nFor more information see the kustomize inventory object documentation.\nField changes / deprecations resources expanded, bases deprecated The resources field has been generalized; it now accepts what formerly could only be specified in the bases field.\nThis change was made to allow users fine control over resource processing order. With a distinct bases field, bases had to be loaded separately from resources as a group. Now, base loading may be interleaved as desired with the loading of resource files from the current directory. Resource ordering had to be respected before this feature could be introduced.\nThe bases field is now deprecated, and will be deleted in some future major release. Manage the deprecation simply moving the arguments of the bases field to the resources field in the desired order, e.g.\n resources: - someResouceFile.yaml - someOtherResourceFile.yaml bases: - ../../someBaseDir  could become\n resources: - someResouceFile.yaml - ../../someBaseDir - someOtherResourceFile.yaml  The kustomized edit fix command will do this for you, though it will always put the bases at the end.\nAs an aside, the resources, generators and transformers fields now all accept the same argument format.\n Each field\u0026rsquo;s argument is a string list, where each entry is either a resource (a relative path to a YAML file) or a kustomization (a path or URL pointing to a directory with a kustomization file). A kustomization directory used in this context is called a base.\n The fact that the generators and transformers field accept bases and the fact that generator and transformer configuration objects are just normal k8s resources means that one can generate or transform a generator or a transformer (see TestTransformerTransformers).\nreplicas field The common task of patching a deployment to edit the number of replicas is now made easier with the new replicas field.\nenvs field An envs sub-field has been added to both configMapGenerator and secretGenerator, replacing the now deprecated (and singular) env field. The new field accepts lists, just like its sibling fields files and literals.\nOptionally use kustomize edit fix to merge singular env field into a plural field.\n","excerpt":"Go modules, resource ordering respected, generator and transformer plugins, eased loading …","ref":"/blog/2019/06/18/v2.1.0/","title":"v2.1.0"},{"body":"Go modules, resource ordering respected, generator and transformer plugins, eased loading restrictions, the notion of inventory, eased replica count modification. About ~90 issues closed since v2.0.3 in ~400 commits.\nDownload here.\nGo modules Kustomize now defines its dependencies in a top level go.mod file. This is the first step towards a package structure intentially exported as one or more Go modules for use in other programs (kubectl, kubebuilder, etc.) and in kustomize plugins (see below).\nResource ordering Kustomize now retains the depth-first order of resources as read, a frequently requested feature.\nThis means resource order can be controlled by editting kustomization files. This is also vital to applying user-defined transformations (plugins) in a particular order.\nNothing needs to be done to activate this; it happens automatically.\nThe build command now accepts a --reorder flag with values legacy and none, with a default value of legacy.\nlegacy means apply an ordering based on GVK, that currently emits Namespace objects first, and ValidatingWebhookConfiguration objects last. This means that despite automatic retention of load order, your build output won\u0026rsquo;t change by default.\nnone means don\u0026rsquo;t reorder the resources before output. Specify this to see output order respect input order.\nGenerator and transformer plugins Since the beginning (as kinflate back in Sep 2017), kustomize has read or generated resources, applied a series of pipelined transformation to them, and emitted the result to stdout.\nAt that time, the only way to change the behavior of a generator (e.g. a secret generator), or change the behavior of a transformer (e.g. a name changer, or json patcher), was to modify source code and put out a release.\nv1.0.9 introduced generator options as a means to change the behavior of the only two generators available at the time - Secret and ConfigMap generators. It also introduced transformer configs as a way to fine tune the targets of transformations (e.g. to which fields selectors should be added). Most of the feature requests for kustomize revolve around changing the behavior of the builtin generators and transformers.\nv2.1.0 adds an alpha plugin framework, that encourages users to write their own generators or transformers, declaring them as kubernetes objects just like everything else, and apply them as part of the kustomize build process.\nTo inform the API exposed to plugins, and to confirm that the plugin framework can offer plugin authors the same capabilities as builtin operations, all the builtin generators and tranformers have been converted to plugin form (with one exceptions awaiting Go module refinements). This means that adding, say, a secretGenerator or commonAnnotations directive to your kustomization will (in v2.1.0) trigger execution of code committed as a plugin.\nFor more information, see the kustomize plugin documentation.\nRemove load restrictions The following usage:\nkustomize build --load_restrictor none $target allows a kustomization.yaml file used in this build to refer to files outside its own directory (i.e. outside its root).\nThis is an opt-in to suppress a security feature that denies this precise behavior.\nThis feature should only be used to allow multiple overlays (e.g. prod, staging and dev) to share a patch file. To share resources, use a relative path or URL to a kustomization directory in the resources directive.\nInventory generation for pruning Alpha\nUsers can add an inventory stanza to their kustomization file, to add a special inventory object to the build result.\nThis object applies to the cluster along with everything else in the build result and can be used by other clients to intelligently prune orphaned cluster resources.\nFor more information see the kustomize inventory object documentation.\nField changes / deprecations resources expanded, bases deprecated The resources field has been generalized; it now accepts what formerly could only be specified in the bases field.\nThis change was made to allow users fine control over resource processing order. With a distinct bases field, bases had to be loaded separately from resources as a group. Now, base loading may be interleaved as desired with the loading of resource files from the current directory. Resource ordering had to be respected before this feature could be introduced.\nThe bases field is now deprecated, and will be deleted in some future major release. Manage the deprecation simply moving the arguments of the bases field to the resources field in the desired order, e.g.\n resources: - someResouceFile.yaml - someOtherResourceFile.yaml bases: - ../../someBaseDir  could become\n resources: - someResouceFile.yaml - ../../someBaseDir - someOtherResourceFile.yaml  The kustomized edit fix command will do this for you, though it will always put the bases at the end.\nAs an aside, the resources, generators and transformers fields now all accept the same argument format.\n Each field\u0026rsquo;s argument is a string list, where each entry is either a resource (a relative path to a YAML file) or a kustomization (a path or URL pointing to a directory with a kustomization file). A kustomization directory used in this context is called a base.\n The fact that the generators and transformers field accept bases and the fact that generator and transformer configuration objects are just normal k8s resources means that one can generate or transform a generator or a transformer (see TestTransformerTransformers).\nreplicas field The common task of patching a deployment to edit the number of replicas is now made easier with the new replicas field.\nenvs field An envs sub-field has been added to both configMapGenerator and secretGenerator, replacing the now deprecated (and singular) env field. The new field accepts lists, just like its sibling fields files and literals.\nOptionally use kustomize edit fix to merge singular env field into a plural field.\n","excerpt":"Go modules, resource ordering respected, generator and transformer plugins, eased loading …","ref":"/zh/blog/2019/06/18/v2.1.0/","title":"v2.1.0"},{"body":"After security review, a field used in secret generation (see below) was removed from the definition of a kustomization file with no mechanism to convert it to a new form. Also, the set of files accessible from a kustomization file has been further constrained.\nPer the versioning policy, backward incompatible changes trigger an increment of the major version number, hence we go from 1.0.11 to 2.0.0. We\u0026rsquo;re taking this major version increment opportunity to remove some already deprecated fields, and the code paths associated with them.\nBackward Incompatible Changes Kustomization Path Constraints A kustomization file can specify paths to other files, including resources, patches, configmap generation data, secret generation data and bases. In the case of a base, the path can be a git URL instead.\nIn 1.x, these paths had to be relative to the current kustomization directory (the location of the kustomization file used in the build command).\nIn 2.0, bases can continue to specify, via relative paths, kustomizations outside the current kustomization directory. But non-base paths are constrained to terminate in or below the current kustomization directory. Further, bases specified via a git URL may not reference files outside of the directory used to clone the repository.\nKustomization Field Removals patches patches was deprecated and replaced by patchesStrategicMerge when patchesJson6902 was introduced. In Kustomize 2.0.0, patches is removed. Please use patchesStrategicMerge instead.\nimageTags imageTags is replaced by images since images can provide more features to change image names, registries, tags and digests.\nsecretGenerator/commands commands is removed from SecretGenerator due to a security concern. One can use files or literals, similar to ConfigMapGenerator, to generate a secret.\nsecretGenerator: - name: app-tls files: - secret/tls.cert - secret/tls.key type: \u0026quot;kubernetes.io/tls\u0026quot; Compatible Changes (New Features) As this release is triggered by a security change, there are no major new features to announce. A few things that are worth mentioning in this release are:\n  More than 40 issues closed since 1.0.11 release (including many extensions to transformation rules).\n  Users can run kustomize edit fix to migrate a kustomization file working with previous versions to one working with 2.0.0. For example, a kustomization.yaml with following content\npatches: - deployment-patch.yaml imageTags: - name: postgres newTag: v1 will be converted to\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization patchesStrategicMerge: - deployment-patch.yaml images: - name: postgres newTag: v1   Kustomization filename\nIn previous versions, the name of a kustomization file had to be kustomization.yaml. Kustomize allows kustomization.yaml, kustomization.yml and Kustomization. In a directory, only one of those filenames is allowed. If there are more than one found, Kustomize will exit with an error. Please select the best filename for your use cases.\n  Cancelled plans to deprecate applying prefix/suffix to namespace. The deprecation warning\nAdding nameprefix and namesuffix to Namespace resource will be deprecated in next release. was removed.\n  ","excerpt":"After security review, a field used in secret generation (see below) was removed from the definition …","ref":"/blog/2019/02/05/v2.0.0/","title":"v2.0.0"},{"body":"After security review, a field used in secret generation (see below) was removed from the definition of a kustomization file with no mechanism to convert it to a new form. Also, the set of files accessible from a kustomization file has been further constrained.\nPer the versioning policy, backward incompatible changes trigger an increment of the major version number, hence we go from 1.0.11 to 2.0.0. We\u0026rsquo;re taking this major version increment opportunity to remove some already deprecated fields, and the code paths associated with them.\nBackward Incompatible Changes Kustomization Path Constraints A kustomization file can specify paths to other files, including resources, patches, configmap generation data, secret generation data and bases. In the case of a base, the path can be a git URL instead.\nIn 1.x, these paths had to be relative to the current kustomization directory (the location of the kustomization file used in the build command).\nIn 2.0, bases can continue to specify, via relative paths, kustomizations outside the current kustomization directory. But non-base paths are constrained to terminate in or below the current kustomization directory. Further, bases specified via a git URL may not reference files outside of the directory used to clone the repository.\nKustomization Field Removals patches patches was deprecated and replaced by patchesStrategicMerge when patchesJson6902 was introduced. In Kustomize 2.0.0, patches is removed. Please use patchesStrategicMerge instead.\nimageTags imageTags is replaced by images since images can provide more features to change image names, registries, tags and digests.\nsecretGenerator/commands commands is removed from SecretGenerator due to a security concern. One can use files or literals, similar to ConfigMapGenerator, to generate a secret.\nsecretGenerator: - name: app-tls files: - secret/tls.cert - secret/tls.key type: \u0026quot;kubernetes.io/tls\u0026quot; Compatible Changes (New Features) As this release is triggered by a security change, there are no major new features to announce. A few things that are worth mentioning in this release are:\n  More than 40 issues closed since 1.0.11 release (including many extensions to transformation rules).\n  Users can run kustomize edit fix to migrate a kustomization file working with previous versions to one working with 2.0.0. For example, a kustomization.yaml with following content\npatches: - deployment-patch.yaml imageTags: - name: postgres newTag: v1 will be converted to\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization patchesStrategicMerge: - deployment-patch.yaml images: - name: postgres newTag: v1   Kustomization filename\nIn previous versions, the name of a kustomization file had to be kustomization.yaml. Kustomize allows kustomization.yaml, kustomization.yml and Kustomization. In a directory, only one of those filenames is allowed. If there are more than one found, Kustomize will exit with an error. Please select the best filename for your use cases.\n  Cancelled plans to deprecate applying prefix/suffix to namespace. The deprecation warning\nAdding nameprefix and namesuffix to Namespace resource will be deprecated in next release. was removed.\n  ","excerpt":"After security review, a field used in secret generation (see below) was removed from the definition …","ref":"/zh/blog/2019/02/05/v2.0.0/","title":"v2.0.0"},{"body":"Initial release after move from github.com/kubernetes/kubectl to github.com/kubernetes-sigs/kustomize.\nHistory\n May 2018: v1.0 after move to github.com/kubernetes-sigs/kubectl from github.com/kubernetes/kubectl. Has kustomization file, bases, overlays, basic transforms. Apr 2018: s/kinflate/kustomize/, s/manifest/kustomization/ Oct 2017: s/kexpand/kinflate/ Sep 2017: kexpand starts in github.com/kubernetes/kubectl Aug 2017: DAM authored by Brian Grant  ","excerpt":"Initial release after move from github.com/kubernetes/kubectl to …","ref":"/blog/2018/05/21/v1.0.1/","title":"v1.0.1"},{"body":"Initial release after move from github.com/kubernetes/kubectl to github.com/kubernetes-sigs/kustomize.\nHistory\n May 2018: v1.0 after move to github.com/kubernetes-sigs/kubectl from github.com/kubernetes/kubectl. Has kustomization file, bases, overlays, basic transforms. Apr 2018: s/kinflate/kustomize/, s/manifest/kustomization/ Oct 2017: s/kexpand/kinflate/ Sep 2017: kexpand starts in github.com/kubernetes/kubectl Aug 2017: DAM authored by Brian Grant  ","excerpt":"Initial release after move from github.com/kubernetes/kubectl to …","ref":"/zh/blog/2018/05/21/v1.0.1/","title":"v1.0.1"},{"body":"","excerpt":"","ref":"/zh/index.json","title":""},{"body":" bases 字段在 v2.1.0 中已被弃用。\n 该条目已被移动到 resources 字段中，其仍是核心概念，这使得相对于其他输入资源 base 可以进行排序。\n","excerpt":"bases 字段在 v2.1.0 中已被弃用。\n 该条目已被移动到 resources 字段中，其仍是核心概念，这使得相对于其他输入资源 base 可以进行排序。","ref":"/zh/api-reference/kustomization/bases/","title":"bases"},{"body":"","excerpt":"","ref":"/blog/","title":"Kustomize Blog"},{"body":"Builtin Plugins A list of kustomize\u0026rsquo;s builtin plugins - both generators and transformers.\nFor each plugin, an example is given for\n  implicitly triggering the plugin via a dedicated kustomization file field (e.g. the AnnotationsTransformer is triggered by the commonAnnotations field).\n  explicitly triggering the plugin via the generators or transformers field (by providing a config file specifying the plugin).\n  The former method is convenient but limited in power as most of the plugins arguments must be defaulted. The latter method allows for complete plugin argument specification.\nAnnotationTransformer Usage via kustomization.yaml field name: commonAnnotations Adds annotions (non-identifying metadata) to add all resources. Like labels, these are key value pairs.\ncommonAnnotations: oncallPager: 800-555-1212 Usage via plugin Arguments  Annotations map[string]string\nFieldSpecs []config.FieldSpec\n Example  apiVersion: builtin kind: AnnotationsTransformer metadata: name: not-important-to-example annotations: app: myApp greeting/morning: a string with blanks fieldSpecs: - path: metadata/annotations create: true  ConfigMapGenerator Usage via kustomization.yaml field name: configMapGenerator Each entry in this list results in the creation of one ConfigMap resource (it\u0026rsquo;s a generator of n maps).\nThe example below creates three ConfigMaps. One with the names and contents of the given files, one with key/value as data, and a third which sets an annotation and label via options for that single ConfigMap.\nEach configMapGenerator item accepts a parameter of behavior: [create|replace|merge]. This allows an overlay to modify or replace an existing configMap from the parent.\nAlso, each entry has an options field, that has the same subfields as the kustomization file\u0026rsquo;s generatorOptions field.\nThis options field allows one to add labels and/or annotations to the generated instance, or to individually disable the name suffix hash for that instance. Labels and annotations added here will not be overwritten by the global options associated with the kustomization file generatorOptions field. However, due to how booleans behave, if the global generatorOptions field specifies disableNameSuffixHash: true, this will trump any attempt to locally override it.\n# These labels are added to all configmaps and secrets. generatorOptions: labels: fruit: apple configMapGenerator: - name: my-java-server-props behavior: merge files: - application.properties - more.properties - name: my-java-server-env-vars literals: - JAVA_HOME=/opt/java/jdk - JAVA_TOOL_OPTIONS=-agentlib:hprof options: disableNameSuffixHash: true labels: pet: dog - name: dashboards files: - mydashboard.json options: annotations: dashboard: \u0026quot;1\u0026quot; labels: app.kubernetes.io/name: \u0026quot;app1\u0026quot; It is also possible to define a key to set a name different than the filename.\nThe example below creates a ConfigMap with the name of file as myFileName.ini while the actual filename from which the configmap is created is whatever.ini.\nconfigMapGenerator: - name: app-whatever files: - myFileName.ini=whatever.ini Usage via plugin Arguments  types.ConfigMapArgs\n Example  apiVersion: builtin kind: ConfigMapGenerator metadata: name: mymap envs: - devops.env - uxteam.env literals: - FRUIT=apple - VEGETABLE=carrot  ImageTagTransformer Usage via kustomization.yaml field name: images Images modify the name, tags and/or digest for images without creating patches. E.g. Given this kubernetes Deployment fragment:\ncontainers: - name: mypostgresdb image: postgres:8 - name: nginxapp image: nginx:1.7.9 - name: myapp image: my-demo-app:latest - name: alpine-app image: alpine:3.7 one can change the image in the following ways:\n postgres:8 to my-registry/my-postgres:v1, nginx tag 1.7.9 to 1.8.0, image name my-demo-app to my-app, alpine\u0026rsquo;s tag 3.7 to a digest value  all with the following kustomization:\nimages: - name: postgres newName: my-registry/my-postgres newTag: v1 - name: nginx newTag: 1.8.0 - name: my-demo-app newName: my-app - name: alpine digest: sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 Usage via plugin Arguments  ImageTag image.Image\nFieldSpecs []config.FieldSpec\n Example  apiVersion: builtin kind: ImageTagTransformer metadata: name: not-important-to-example imageTag: name: nginx newTag: v2  LabelTransformer Usage via kustomization.yaml field name: commonLabels Adds labels to all resources and selectors\ncommonLabels: someName: someValue owner: alice app: bingo Usage via plugin Arguments  Labels map[string]string\nFieldSpecs []config.FieldSpec\n Example  apiVersion: builtin kind: LabelTransformer metadata: name: not-important-to-example labels: app: myApp env: production fieldSpecs: - path: metadata/labels create: true  NamespaceTransformer Usage via kustomization.yaml field name: namespace Adds namespace to all resources\nnamespace: my-namespace Usage via plugin Arguments  types.ObjectMeta\nFieldSpecs []config.FieldSpec\n Example  apiVersion: builtin kind: NamespaceTransformer metadata: name: not-important-to-example namespace: test fieldSpecs: - path: metadata/namespace create: true - path: subjects kind: RoleBinding group: rbac.authorization.k8s.io - path: subjects kind: ClusterRoleBinding group: rbac.authorization.k8s.io  PatchesJson6902 Usage via kustomization.yaml field name: patchesJson6902 Each entry in this list should resolve to a kubernetes object and a JSON patch that will be applied to the object. The JSON patch is documented at https://tools.ietf.org/html/rfc6902\ntarget field points to a kubernetes object within the same kustomization by the object\u0026rsquo;s group, version, kind, name and namespace. path field is a relative file path of a JSON patch file. The content in this patch file can be either in JSON format as\n[ {\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/some/new/path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;value\u0026#34;}, {\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/some/existing/path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;new value\u0026#34;} ] or in YAML format as\n- op: add path: /some/new/path value: value - op: replace path: /some/existing/path value: new value patchesJson6902: - target: version: v1 kind: Deployment name: my-deployment path: add_init_container.yaml - target: version: v1 kind: Service name: my-service path: add_service_annotation.yaml The patch content can be an inline string as well:\npatchesJson6902: - target: version: v1 kind: Deployment name: my-deployment patch: |- - op: add path: /some/new/path value: value - op: replace path: /some/existing/path value: \u0026#34;new value\u0026#34; Usage via plugin Arguments  Target types.PatchTarget\nPath string\nJsonOp string\n Example  apiVersion: builtin kind: PatchJson6902Transformer metadata: name: not-important-to-example target: group: apps version: v1 kind: Deployment name: my-deploy path: jsonpatch.json  PatchesStrategicMerge Usage via kustomization.yaml field name: patchesStrategicMerge Each entry in this list should be either a relative file path or an inline content resolving to a partial or complete resource definition.\nThe names in these (possibly partial) resource files must match names already loaded via the resources field. These entries are used to patch (modify) the known resources.\nSmall patches that do one thing are best, e.g. modify a memory request/limit, change an env var in a ConfigMap, etc. Small patches are easy to review and easy to mix together in overlays.\npatchesStrategicMerge: - service_port_8888.yaml - deployment_increase_replicas.yaml - deployment_increase_memory.yaml The patch content can be a inline string as well.\npatchesStrategicMerge: - |- apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: template: spec: containers: - name: nginx image: nignx:latest Note that kustomize does not support more than one patch for the same object that contain a delete directive. To remove several fields / slice elements from an object create a single patch that performs all the needed deletions.\nUsage via plugin Arguments  Paths []types.PatchStrategicMerge\nPatches string\n Example  apiVersion: builtin kind: PatchStrategicMergeTransformer metadata: name: not-important-to-example paths: - patch.yaml  PatchTransformer Usage via kustomization.yaml field name: patches Each entry in this list should resolve to an Patch object, which includes a patch and a target selector. The patch can be either a strategic merge patch or a JSON patch. it can be either a patch file or an inline string. The target selects resources by group, version, kind, name, namespace, labelSelector and annotationSelector. A resource which matches all the specified fields is selected to apply the patch.\npatches: - path: patch.yaml target: group: apps version: v1 kind: Deployment name: deploy.* labelSelector: \u0026#34;env=dev\u0026#34; annotationSelector: \u0026#34;zone=west\u0026#34; - patch: |- - op: replace path: /some/existing/path value: new value target: kind: MyKind labelSelector: \u0026#34;env=dev\u0026#34; The name and namespace fields of the patch target selector are automatically anchored regular expressions. This means that the value myapp is equivalent to ^myapp$.\nUsage via plugin Arguments  Path string\nPatch string\nTarget *types.Selector\n Example  apiVersion: builtin kind: PatchTransformer metadata: name: not-important-to-example patch: \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;nginx:latest\u0026#34;}]\u0026#39; target: name: .*Deploy kind: Deployment  PrefixSuffixTransformer Usage via kustomization.yaml field names: namePrefix, nameSuffix Prepends or postfixes the value to the names of all resources.\nE.g. a deployment named wordpress could become alices-wordpress or wordpress-v2 or alices-wordpress-v2.\nnamePrefix: alices- nameSuffix: -v2 The suffix is appended before the content hash if the resource type is ConfigMap or Secret.\nUsage via plugin Arguments  Prefix string\nSuffix string\nFieldSpecs []config.FieldSpec\n Example  apiVersion: builtin kind: PrefixSuffixTransformer metadata: name: not-important-to-example prefix: baked- suffix: -pie fieldSpecs: - path: metadata/name  ReplicaCountTransformer Usage via kustomization.yaml field name: replicas Replicas modified the number of replicas for a resource.\nE.g. Given this kubernetes Deployment fragment:\nkind: Deployment metadata: name: deployment-name spec: replicas: 3 one can change the number of replicas to 5 by adding the following to your kustomization:\nreplicas: - name: deployment-name count: 5 This field accepts a list, so many resources can be modified at the same time.\nAs this declaration does not take in a kind: nor a group: it will match any group and kind that has a matching name and that is one of:\n Deployment ReplicationController ReplicaSet StatefulSet  For more complex use cases, revert to using a patch.\nUsage via plugin Arguments  Replica types.Replica\nFieldSpecs []config.FieldSpec\n Example  apiVersion: builtin kind: ReplicaCountTransformer metadata: name: not-important-to-example replica: name: myapp count: 23 fieldSpecs: - path: spec/replicas create: true kind: Deployment - path: spec/replicas create: true kind: ReplicationController  SecretGenerator Usage via kustomization.yaml field name: secretGenerator Each entry in the argument list results in the creation of one Secret resource (it\u0026rsquo;s a generator of n secrets).\nThis works like the configMapGenerator field described above.\nsecretGenerator: - name: app-tls files: - secret/tls.cert - secret/tls.key type: \u0026#34;kubernetes.io/tls\u0026#34; - name: app-tls-namespaced # you can define a namespace to generate # a secret in, defaults to: \u0026#34;default\u0026#34; namespace: apps files: - tls.crt=catsecret/tls.cert - tls.key=secret/tls.key type: \u0026#34;kubernetes.io/tls\u0026#34; - name: env_file_secret envs: - env.txt type: Opaque - name: secret-with-annotation files: - app-config.yaml type: Opaque options: annotations: app_config: \u0026#34;true\u0026#34; labels: app.kubernetes.io/name: \u0026#34;app2\u0026#34; Usage via plugin Arguments  types.ObjectMeta\ntypes.SecretArgs\n Example  apiVersion: builtin kind: SecretGenerator metadata: name: my-secret namespace: whatever behavior: merge envs: - a.env - b.env files: - obscure=longsecret.txt literals: - FRUIT=apple - VEGETABLE=carrot  HelmChartInflationGenerator Usage via kustomization.yaml field name: helmChartInflationGenerator Each entry in the argument list results in the pulling and rendering of a helm chart.\nEach entry can have following fields:\n chartName: The name of the chart that you want to use. chartRepoUrl: [Optional] The URL of the repository which contains the chart. If this is provided, the plugin will try to fetch remote charts. Otherwise it will try to load local chart in chartHome. chartVersion: [Optional] Version of the chart. Will use latest version if this is omitted. chartHome: [Optional] Provide the path to the parent directory for local chart. chartRelease: [Optional] The name of the repo where to find the chart. values: [Optional] A path to the values file. releaseName: [Optional] The release name that will be set in the chart. releaseNamespace: [Optional] The namespace which will be used by --namespace flag in helm template command. helmBin: [Optional] Path to helm binary. Default is helm. helmHome: [Optional] Path to helm home directory. extraArgs: [Optional] A list of additional argumetns that will be passed into helm template command.  helmChartInflationGenerator: - chartName: minecraft chartRepoUrl: https://kubernetes-charts.storage.googleapis.com chartVersion: v1.2.0 releaseName: test releaseNamespace: testNamespace Usage via plugin Arguments  ChartName string\nChartVersion string\nChartRepoURL string\nChartHome string\nChartRepoName string\nHelmBin string\nHelmHome string\nValues string\nReleaseName string\nReleaseNamespace string\nExtraArgs []string\n Example  apiVersion: builtin kind: HelmChartInflationGenerator metadata: name: myMap chartName: minecraft chartRepoUrl: https://kubernetes-charts.storage.googleapis.com chartVersion: v1.2.0 helmBin: /usr/bin/helm helmHome: /tmp/helmHome releaseName: test releaseNamespace: testNamespace values: values.yaml extraArgs: - --include-crds  ","excerpt":"Builtin Plugins A list of kustomize\u0026rsquo;s builtin plugins - both generators and transformers.\nFor …","ref":"/guides/extending_kustomize/builtins/","title":"Builtin Plugins"},{"body":"为所有资源添加注释，如果资源上已经存在注解键，该值将被覆盖。\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationcommonAnnotations:oncallPager:800-555-1212Example 输入文件 # kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationcommonAnnotations:oncallPager:800-555-1212resources:- deploy.yaml# deploy.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:examplespec:...构建输出 apiVersion:apps/v1kind:Deploymentmetadata:name:exampleannotations:oncallPager:800-555-1212spec:...","excerpt":"为所有资源添加注释，如果资源上已经存在注解键，该值将被覆盖。 …","ref":"/zh/api-reference/kustomization/commonannotations/","title":"commonAnnotations"},{"body":"为所有资源和 selectors 增加标签。如果资源上已经存在注解键，该值将被覆盖。\n一旦将资源应用于集群，就不应更改诸如 Deployments 和 Services 之类的资源选择器。\n将 commonLabels 更改为可变资源可能会导致部署失败。\n apiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationcommonLabels:someName:someValueowner:aliceapp:bingoExample 文件输入 # kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationcommonLabels:someName:someValueowner:aliceapp:bingoresources:- deploy.yaml- service.yaml# deploy.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:example# service.yamlapiVersion:v1kind:Servicemetadata:name:example构建输出 apiVersion:v1kind:Servicemetadata:labels:app:bingoowner:alicesomeName:someValuename:examplespec:selector:app:bingoowner:alicesomeName:someValue---apiVersion:apps/v1kind:Deploymentmetadata:labels:app:bingoowner:alicesomeName:someValuename:examplespec:selector:matchLabels:app:bingoowner:alicesomeName:someValuetemplate:metadata:labels:app:bingoowner:alicesomeName:someValue","excerpt":"为所有资源和 selectors 增加标签。如果资源上已经存在注解键，该值将被覆盖。\n一旦将资源应用于集群，就不应更改诸如 Deployments 和 Services 之类的资源选择器。 …","ref":"/zh/api-reference/kustomization/commonlabels/","title":"commonLabels"},{"body":"Coming soon\n","excerpt":"Coming soon","ref":"/zh/api-reference/kustomization/components/","title":"components"},{"body":"列表中的每个条目都将生成一个 ConfigMap （合计可以生成 n 个 ConfigMap）。\n以下示例创建四个 ConfigMap：\n 第一个使用给定文件的名称和内容创建数据 第二个使用文件中的键/值对将数据创建为键/值 第三个使用 literals 中的键/值对创建数据作为键/值 第四个通过 options 设置单个 ConfigMap 的注释和标签  每个 configMapGenerator 项均接受的参数 behavior: [create|replace|merge]，这个参数允许修改或替换父级现有的 configMap。\n此外，每个条目都有一个 options 字段，该字段具有与 kustomization 文件的 generatorOptions 字段相同的子字段。\noptions 字段允许用户为生成的实例添加标签和（或）注释，或者分别禁用该实例名称的哈希后缀。此处添加的标签和注释不会被 kustomization 文件 generatorOptions 字段关联的全局选项覆盖。但是如果全局 generatorOptions 字段指定 disableNameSuffixHash: true，其他 options 的设置将无法将其覆盖。\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomization# 这个标签将添加到所有的 ConfigMap 和 Secret 中。generatorOptions:labels:fruit:appleconfigMapGenerator:- name:my-java-server-propsbehavior:mergefiles:- application.properties- more.properties- name:my-java-server-env-file-varsenvs:- my-server-env.properties- more-server-props.env- name:my-java-server-env-varsliterals:- JAVA_HOME=/opt/java/jdk- JAVA_TOOL_OPTIONS=-agentlib:hprofoptions:disableNameSuffixHash:truelabels:pet:dog- name:dashboardsfiles:- mydashboard.jsonoptions:annotations:dashboard:\u0026#34;1\u0026#34;labels:app.kubernetes.io/name:\u0026#34;app1\u0026#34;这里也可以定义一个 key 来为文件设置不同名称。\n下面这个示例会创建一个 ConfigMap，并将 whatever.ini 重命名为 myFileName.ini：\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationconfigMapGenerator:- name:app-whateverfiles:- myFileName.ini=whatever.ini","excerpt":"列表中的每个条目都将生成一个 ConfigMap （合计可以生成 n 个 ConfigMap）。\n以下示例创建四个 ConfigMap：\n 第一个使用给定文件的名称和内容创建数据 第二个使用文件中的 …","ref":"/zh/api-reference/kustomization/configmapgenerator/","title":"configMapGenerator"},{"body":"Follow are resources for Kubectl / Kustomize contributors.\n","excerpt":"Follow are resources for Kubectl / Kustomize contributors.","ref":"/contributing/","title":"Contributing"},{"body":"此列表中的每个条目都应该是自定义资源定义（CRD）文件的相对路径。\n该字段的存在是为了让 kustomize 识别用户自定义的 CRD ，并对这些类型中的对象应用适当的转换。\n典型用例：CRD 引用 ConfigMap 对象\n在 kustomization 中，ConfigMap 对象名称可能会通过 namePrefix 、nameSuffix 或 hashing 来更改 CRD 对象中该 ConfigMap 对象的名称， 引用时需要以相同的方式使用 namePrefix 、 nameSuffix 或 hashing 来进行更新。\nAnnotations 可以放入 openAPI 的定义中：\n \u0026ldquo;x-kubernetes-annotation\u0026rdquo;: \u0026quot;\u0026quot; \u0026ldquo;x-kubernetes-label-selector\u0026rdquo;: \u0026quot;\u0026quot; \u0026ldquo;x-kubernetes-identity\u0026rdquo;: \u0026quot;\u0026quot; \u0026ldquo;x-kubernetes-object-ref-api-version\u0026rdquo;: \u0026ldquo;v1\u0026rdquo;, \u0026ldquo;x-kubernetes-object-ref-kind\u0026rdquo;: \u0026ldquo;Secret\u0026rdquo;, \u0026ldquo;x-kubernetes-object-ref-name-key\u0026rdquo;: \u0026ldquo;name\u0026rdquo;,  apiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationcrds:- crds/typeA.yaml- crds/typeB.yaml","excerpt":"此列表中的每个条目都应该是自定义资源定义（CRD）文件的相对路径。\n该字段的存在是为了让 kustomize 识别用户自定义的 CRD ，并对这些类型中的对象应用适当的转换。\n典型用例：CRD …","ref":"/zh/api-reference/kustomization/crds/","title":"crds"},{"body":"This is a (no reading allowed!) 60 second copy/paste guided example. Full plugin docs here.\nThis demo writes and uses a somewhat ridiculous exec plugin (written in bash) that generates a ConfigMap.\nThis is a guide to try it without damaging your current setup.\nrequirements  linux, git, curl, Go 1.13  Make a place to work DEMO=$(mktemp -d) Create a kustomization Make a kustomization directory to hold all your config:\nMYAPP=$DEMO/myapp mkdir -p $MYAPP Make a deployment config:\n# $MYAPP/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: the-deployment spec: replicas: 3 template: spec: containers: - name: the-container image: monopole/hello:1 command: [\u0026#34;/hello\u0026#34;, \u0026#34;--port=8080\u0026#34;, \u0026#34;--date=$(THE_DATE)\u0026#34;, \u0026#34;--enableRiskyFeature=$(ENABLE_RISKY)\u0026#34;] ports: - containerPort: 8080 env: - name: THE_DATE valueFrom: configMapKeyRef: name: the-map key: today - name: ALT_GREETING valueFrom: configMapKeyRef: name: the-map key: altGreeting - name: ENABLE_RISKY valueFrom: configMapKeyRef: name: the-map key: enableRisky EOF Make a service config:\n# $MYAPP/service.yaml kind: Service apiVersion: v1 metadata: name: the-service spec: type: LoadBalancer ports: - protocol: TCP port: 8666 targetPort: 8080 EOF Now make a config file for the plugin you\u0026rsquo;re about to write.\nThis config file is just another k8s resource object. The values of its apiVersion and kind fields are used to find the plugin code on your filesystem (more on this later).\n# $MYAPP/cmGenerator.yaml apiVersion: myDevOpsTeam kind: SillyConfigMapGenerator metadata: name: whatever argsOneLiner: Bienvenue true EOF Finally, make a kustomization file referencing all of the above:\n# $MYAPP/kustomization.yaml commonLabels: app: hello resources: - deployment.yaml - service.yaml generators: - cmGenerator.yaml EOF Review the files\nls -C1 $MYAPP Make a home for plugins Plugins must live in a particular place for kustomize to find them.\nThis demo will use the ephemeral directory:\nPLUGIN_ROOT=$DEMO/kustomize/plugin The plugin config defined above in $MYAPP/cmGenerator.yaml specifies:\n apiVersion: myDevOpsTeam kind: SillyConfigMapGenerator  This means the plugin must live in a directory named:\nMY_PLUGIN_DIR=$PLUGIN_ROOT/myDevOpsTeam/sillyconfigmapgenerator mkdir -p $MY_PLUGIN_DIR The directory name is the plugin config\u0026rsquo;s apiVersion followed by its lower-cased kind.\nA plugin gets its own directory to hold itself, its tests and any supplemental data files it might need.\nCreate the plugin There are two kinds of plugins, exec and Go.\nMake an exec plugin, installing it to the correct directory and file name. The file name must match the plugin\u0026rsquo;s kind (in this case, SillyConfigMapGenerator):\n# $MY_PLUGIN_DIR/SillyConfigMapGenerator #!/bin/bash # Skip the config file name argument. shift today=`date +%F` echo \u0026#34; kind: ConfigMap apiVersion: v1 metadata: name: the-map data: today: $todayaltGreeting: \u0026#34;$1\u0026#34; enableRisky: \u0026#34;$2\u0026#34; \u0026#34; EOF By definition, an exec plugin must be executable:\nchmod a+x $MY_PLUGIN_DIR/SillyConfigMapGenerator Install kustomize Per the instructions:\ncurl -s \u0026#34;https://raw.githubusercontent.com/\\ kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash mkdir -p $DEMO/bin mv kustomize $DEMO/bin Review the layout tree $DEMO Build your app, using the plugin XDG_CONFIG_HOME=$DEMO $DEMO/bin/kustomize build --enable_alpha_plugins $MYAPP Above, if you had set\n PLUGIN_ROOT=$HOME/.config/kustomize/plugin  there would be no need to use XDG_CONFIG_HOME in the kustomize command above.\n","excerpt":"This is a (no reading allowed!) 60 second copy/paste guided example. Full plugin docs here.\nThis …","ref":"/guides/extending_kustomize/execpluginguidedexample/","title":"Exec plugin on linux"},{"body":"","excerpt":"","ref":"/faq/","title":"FAQ"},{"body":"security: file \u0026lsquo;foo\u0026rsquo; is not in or below \u0026lsquo;bar\u0026rsquo; v2.0 added a security check that prevents kustomizations from reading files outside their own directory root.\nThis was meant to help protect the person inclined to download kustomization directories from the web and use them without inspection to control their production cluster (see #693, #700, #995 and #998)\nResources (including configmap and secret generators) can still be shared via the recommended best practice of placing them in a directory with their own kustomization file, and referring to this directory as a base from any kustomization that wants to use it. This encourages modularity and relocatability.\nTo disable this, use v3, and the load_restrictor flag:\nkustomize build --load_restrictor none $target Some field is not transformed by kustomize Example: #1319, #1322, #1347 and etc.\nThe fields transformed by kustomize is configured explicitly in defaultconfig. The configuration itself can be customized by including configurations in kustomization.yaml, e.g.\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationconfigurations:- kustomizeconfig.yamlThe configuration directive allows customization of the following transformers:\ncommonAnnotations:[]commonLabels:[]nameprefix:[]namespace:[]varreference:[]namereference:[]images:[]replicas:[]To persist the changes to default configuration, submit a PR like #1338, #1348 and etc.\n","excerpt":"security: file \u0026lsquo;foo\u0026rsquo; is not in or below \u0026lsquo;bar\u0026rsquo; v2.0 added a security check …","ref":"/zh/faq/","title":"FAQ"},{"body":"此外，在每个生成器中，还可以按每个资源级别设置 generatorOptions，具体使用方法请参见configMapGenerator和secretGenerator。\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationgeneratorOptions:# labels to add to all generated resourceslabels:kustomize.generated.resources:somevalue# annotations to add to all generated resourcesannotations:kustomize.generated.resource:somevalue# disableNameSuffixHash is true disables the default behavior of adding a# suffix to the names of generated resources that is a hash of# the resource contents.disableNameSuffixHash:true","excerpt":"此外，在每个生成器中，还可以按每个资源级别设置 generatorOptions，具体使用方法请参见configMapGenerator和secretGenerator。 …","ref":"/zh/api-reference/kustomization/generatoroptions/","title":"generatorOptions"},{"body":"A Go plugin is a compilation artifact described by the Go plugin package. It is built with special flags and cannot run on its own. It must be loaded into a running Go program.\n A normal program written in Go might be usable as exec plugin, but is not a Go plugin.\n Go plugins allow kustomize extensions that run without the cost marshalling/unmarshalling all resource data to/from a subprocess for each plugin run. The Go plugin API assures a certain level of consistency to avoid confusing downstream transformers.\nGo plugins work as described in the plugin package, but fall short of common notions associated with the word plugin.\nThe skew problem Go plugin compilation creates an ELF formatted .so file, which by definition has no information about the provenance of the object code.\nSkew between the compilation conditions (versions of package dependencies, GOOS, GOARCH) of the main program ELF and the plugin ELF will cause plugin load failure, with non-helpful error messages.\nExec plugins also lack provenance, but won\u0026rsquo;t fail due to compilation skew.\nIn either case, the only sensible way to share a plugin is as some kind of bundle (a git repo URL, a git archive file, a tar file, etc.) containing source code, tests and associated data, unpackable under $XDG_CONFIG_HOME/kustomize/plugin.\nIn the case of a Go plugin, an end user accepting a shared plugin must compile both kustomize and the plugin.\nThis means a one-time run of\n# Or whatever is appropriate at time of reading GOPATH=${whatever} GO111MODULE=on go get sigs.k8s.io/kustomize/api and then a normal development cycle using\ngo build -buildmode plugin \\  -o ${wherever}/${kind}.so ${wherever}/${kind}.go with paths and the release version tag (e.g. v3.0.0) adjusted as needed.\nFor comparison, consider what one must do to write a tensorflow plugin.\nWhy support Go plugins Safety The Go plugin developer sees the same API offered to native kustomize operations, assuring certain semantics, invariants, checks, etc. An exec plugin sub-process dealing with this via stdin/stdout will have an easier time screwing things up for downstream transformers and consumers.\nMinor point: if the plugin reads files via the kustomize-provided file Loader interface, it will be constrained by kustomize file loading restrictions. Of course, nothing but a code audit prevents a Go plugin from importing the io package and doing whatever it wants.\nDebugging A Go plugin developer can debug the plugin in situ, setting breakpoints inside the plugin and elsewhere while running a plugin in feature tests.\nTo get the best of both worlds (shareability and safety), a developer can write an .go program that functions as an exec plugin, but can be processed by go generate to emit a Go plugin (or vice versa).\nUnit of contribution All the builtin generators and transformers are themselves Go plugins. This means that the kustomize maintainers can promote a contributed plugin to a builtin without needing code changes (beyond those mandated by normal code review).\nEcosystems grow through use Tooling could ease Go plugin sharing, but this requires some critical mass of Go plugin authoring, which in turn is hampered by confusion around sharing. Go modules, once they are more widely adopted, will solve the biggest plugin sharing difficulty: ambiguous plugin vs host dependencies.\n","excerpt":"A Go plugin is a compilation artifact described by the Go plugin package. It is built with special …","ref":"/guides/extending_kustomize/goplugincaveats/","title":"Go plugin Caveats"},{"body":"Go Plugin Guided Example for Linux This is a (no reading allowed!) 60 second copy/paste guided example.\nFull plugin docs here. Be sure to read the Go plugin caveats.\nThis demo uses a Go plugin, SopsEncodedSecrets, that lives in the sopsencodedsecrets repository. This is an inprocess Go plugin, not an sub-process exec plugin that happens to be written in Go (which is another option for Go authors).\nThis is a guide to try it without damaging your current setup.\nrequirements  linux, git, curl, Go 1.13  For encryption\n gpg  Or\n Google cloud (gcloud) install a Google account with KMS permission  Make a place to work # Keeping these separate to avoid cluttering the DEMO dir. DEMO=$(mktemp -d) tmpGoPath=$(mktemp -d) Install kustomize Need v3.0.0 for what follows, and you must compile it (not download the binary from the release page):\nGOPATH=$tmpGoPath go install sigs.k8s.io/kustomize/kustomize Make a home for plugins A kustomize plugin is fully determined by its configuration file and source code.\nKustomize plugin configuration files are formatted as kubernetes resource objects, meaning apiVersion, kind and metadata are required fields in these config files.\nThe kustomize program reads the config file (because the config file name appears in the generators or transformers field in the kustomization file), then locates the Go plugin\u0026rsquo;s object code at the following location:\n $XDG_CONFIG_HOME/kustomize/plugin/$apiVersion/$lKind/$kind.so  where lKind holds the lowercased kind. The plugin is then loaded and fed its config, and the plugin\u0026rsquo;s output becomes part of the overall kustomize build process.\nThe same plugin might be used multiple times in one kustomize build, but with different config files. Also, kustomize might customize config data before sending it to the plugin, for whatever reason. For these reasons, kustomize owns the mapping between plugins and config data; it\u0026rsquo;s not left to plugins to find their own config.\nThis demo will house the plugin it uses at the ephemeral directory\nPLUGIN_ROOT=$DEMO/kustomize/plugin and ephemerally set XDG_CONFIG_HOME on a command line below.\nWhat apiVersion and kind At this stage in the development of kustomize plugins, plugin code doesn\u0026rsquo;t know or care what apiVersion or kind appears in the config file sent to it.\nThe plugin could check these fields, but it\u0026rsquo;s the remaining fields that provide actual configuration data, and at this point the successful parsing of these other fields are the only thing that matters to a plugin.\nThis demo uses a plugin called SopsEncodedSecrets, and it lives in the SopsEncodedSecrets repository.\nSomewhat arbitrarily, we\u0026rsquo;ll chose to install this plugin with\napiVersion=mygenerators kind=SopsEncodedSecrets Define the plugin\u0026rsquo;s home dir By convention, the ultimate home of the plugin code and supplemental data, tests, documentation, etc. is the lowercase form of its kind.\nlKind=$(echo $kind | awk \u0026#39;{print tolower($0)}\u0026#39;) Download the SopsEncodedSecrets plugin In this case, the repo name matches the lowercase kind already, so we just clone the repo and get the proper directory name automatically:\nmkdir -p $PLUGIN_ROOT/${apiVersion} cd $PLUGIN_ROOT/${apiVersion} git clone git@github.com:monopole/sopsencodedsecrets.git Remember this directory:\nMY_PLUGIN_DIR=$PLUGIN_ROOT/${apiVersion}/${lKind} Try the plugin\u0026rsquo;s own test Plugins may come with their own tests. This one does, and it hopefully passes:\ncd $MY_PLUGIN_DIR go test SopsEncodedSecrets_test.go Build the object code for use by kustomize:\ncd $MY_PLUGIN_DIR GOPATH=$tmpGoPath go build -buildmode plugin -o ${kind}.so ${kind}.go This step may succeed, but kustomize might ultimately fail to load the plugin because of dependency skew.\nOn load failure\n  be sure to build the plugin with the same version of Go (go1.13) on the same $GOOS (linux) and $GOARCH (amd64) used to build the kustomize being used in this demo.\n  change the plugin\u0026rsquo;s dependencies in its go.mod to match the versions used by kustomize (check kustomize\u0026rsquo;s go.mod used in its tagged commit).\n  Lacking tools and metadata to allow this to be automated, there won\u0026rsquo;t be a Go plugin ecosystem.\nKustomize has adopted a Go plugin architecture as to ease accept new generators and transformers (just write a plugin), and to be sure that native operations (also constructed and tested as plugins) are compartmentalized, orderable and reusable instead of bizarrely woven throughout the code as a individual special cases.\nCreate a kustomization Make a kustomization directory to hold all your config:\nMYAPP=$DEMO/myapp mkdir -p $MYAPP Make a config file for the SopsEncodedSecrets plugin.\nIts apiVersion and kind allow the plugin to be found:\ncat \u0026lt;\u0026lt;EOF \u0026gt;$MYAPP/secGenerator.yaml apiVersion: ${apiVersion} kind: ${kind} metadata: name: mySecretGenerator name: forbiddenValues namespace: production file: myEncryptedData.yaml keys: - ROCKET - CAR EOF This plugin expects to find more data in myEncryptedData.yaml; we\u0026rsquo;ll get to that shortly.\nMake a kustomization file referencing the plugin config:\ncat \u0026lt;\u0026lt;EOF \u0026gt;$MYAPP/kustomization.yaml commonLabels: app: hello generators: - secGenerator.yaml EOF Now generate the real encrypted data.\nAssure you have an encryption tool installed We\u0026rsquo;re going to use sops to encode a file. Choose either GPG or Google Cloud KMS as the secret provider to continue.\nGPG Try this:\ngpg --list-keys If it returns a list, presumably you\u0026rsquo;ve already created keys. If not, try import test keys from sops for dev.\ncurl https://raw.githubusercontent.com/mozilla/sops/master/pgp/sops_functional_tests_key.asc | gpg --import SOPS_PGP_FP=\u0026#34;1022470DE3F0BC54BC6AB62DE05550BC07FB1A0A\u0026#34; Google Cloude KMS Try this:\ngcloud kms keys list --location global --keyring sops If it succeeds, presumably you\u0026rsquo;ve already created keys and placed them in a keyring called sops. If not, do this:\ngcloud kms keyrings create sops --location global gcloud kms keys create sops-key --location global \\  --keyring sops --purpose encryption Extract your keyLocation for use below:\nkeyLocation=$(\\  gcloud kms keys list --location global --keyring sops |\\  grep GOOGLE | cut -d \u0026#34; \u0026#34; -f1) echo $keyLocation Install sops GOPATH=$tmpGoPath go install go.mozilla.org/sops/cmd/sops Create data encrypted with your private key Create raw data to encrypt:\ncat \u0026lt;\u0026lt;EOF \u0026gt;$MYAPP/myClearData.yaml VEGETABLE: carrot ROCKET: saturn-v FRUIT: apple CAR: dymaxion EOF Encrypt the data into file the plugin wants to read:\nWith PGP\n$tmpGoPath/bin/sops --encrypt \\  --pgp $SOPS_PGP_FP \\  $MYAPP/myClearData.yaml \u0026gt;$MYAPP/myEncryptedData.yaml Or GCP KMS\n$tmpGoPath/bin/sops --encrypt \\  --gcp-kms $keyLocation \\  $MYAPP/myClearData.yaml \u0026gt;$MYAPP/myEncryptedData.yaml Review the files\ntree $DEMO This should look something like:\n /tmp/tmp.0kIE9VclPt ├── kustomize │ └── plugin │ └── mygenerators │ └── sopsencodedsecrets │ ├── go.mod │ ├── go.sum │ ├── LICENSE │ ├── README.md │ ├── SopsEncodedSecrets.go │ ├── SopsEncodedSecrets.so │ └── SopsEncodedSecrets_test.go └── myapp ├── kustomization.yaml ├── myClearData.yaml ├── myEncryptedData.yaml └── secGenerator.yaml  Build your app, using the plugin XDG_CONFIG_HOME=$DEMO $tmpGoPath/bin/kustomize build --enable_alpha_plugins $MYAPP This should emit a kubernetes secret, with encrypted data for the names ROCKET and CAR.\nAbove, if you had set\n PLUGIN_ROOT=$HOME/.config/kustomize/plugin  there would be no need to use XDG_CONFIG_HOME in the kustomize command above.\n","excerpt":"Go Plugin Guided Example for Linux This is a (no reading allowed!) 60 second copy/paste guided …","ref":"/guides/extending_kustomize/gopluginguidedexample/","title":"Go plugin example"},{"body":"修改镜像的名称、tag 或 image digest ，而无需使用 patches 。例如，对于这种 kubernetes Deployment 片段：\nkind:Deployment...spec:template:spec:containers:- name:mypostgresdbimage:postgres:8- name:nginxappimage:nginx:1.7.9- name:myappimage:my-demo-app:latest- name:alpine-appimage:alpine:3.7想要将 image 做如下更改：\n 将 postgres:8 改为 my-registry/my-postgres:v1 将 nginx tag 从 1.7.9 改为 1.8.0 将镜像名称 my-demo-app 改为 my-app 将 alpine 的 tag 3.7 改为 digest 值  只需在 kustomization 中添加以下内容：\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationimages:- name:postgresnewName:my-registry/my-postgresnewTag:v1- name:nginxnewTag:1.8.0- name:my-demo-appnewName:my-app- name:alpinedigest:sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3","excerpt":"修改镜像的名称、tag 或 image digest ，而无需使用 patches 。例如，对于这种 kubernetes Deployment 片段： …","ref":"/zh/api-reference/kustomization/images/","title":"images"},{"body":"This document serves as a knowledge bank of SIG-Cli\u0026rsquo;s stance on certain recurring issues and historical decisions.\nEnhancing kubectl create/run TODO: Condense PR#914\nkubectl get all kubectl get all is a legacy command and is actually implemented with a hardcoded server side list that is not easy to maintain. There is potential that it will be removed in the future and therefore will not be expanded upon or improved.\nAssociated issue: ISSUE#151\nWe recommend using ketall which can be installed standalone or via krew.\nConfirmation when using --all Introducing --all would be a breaking change. We’ve made an attempt in the past (see PR#62167) and we’ve decided that the suggested approach is to encourage cluster owners to implement something like PR#17740 or currently using validating webhooks (check here) but no such effort will be undertaken in kubectl itself.\nSupported versions The Kubernetes project maintains release branches for the most recent three minor releases (1.19, 1.18, 1.17 as of this writing). Kubernetes Version Skew Policy\n","excerpt":"This document serves as a knowledge bank of SIG-Cli\u0026rsquo;s stance on certain recurring issues and …","ref":"/faq/kubectl/","title":"Kubectl"},{"body":"kubectl doesn\u0026rsquo;t have the latest kustomize, when will it be updated? TLDR: This is blocked on either moving kubectl into its own repo, or changing its dependencies. ETA k8s ~1.20.\nThe adoption of go modules in the kubernetes/kubernetes repo broke the update process for kustomize. This is due to the kustomize libraries depending on the kubernetes apimachinery libraries, which are published out of the kubernetes staging directory.\n2 pieces of work are underway which will allow kustomize to be updated in kubectl:\n migrating kubectl out of kubernetes/kubernetes (expected Kubernetes ~1.20) migrating kustomize off of the apimachinery libraries (expected Kuberntes ~1.20)  2506    Once either of these issues is resolved we will then update kubectl with the latest kustomize version.\nsecurity: file \u0026lsquo;foo\u0026rsquo; is not in or below \u0026lsquo;bar\u0026rsquo; v2.0 added a security check that prevents kustomizations from reading files outside their own directory root.\nThis was meant to help protect the person inclined to download kustomization directories from the web and use them without inspection to control their production cluster (see #693, #700, #995 and #998)\nResources (including configmap and secret generators) can still be shared via the recommended best practice of placing them in a directory with their own kustomization file, and referring to this directory as a base from any kustomization that wants to use it. This encourages modularity and relocatability.\nTo disable this, use v3, and the load_restrictor flag:\nkustomize build --load_restrictor none $target Some field is not transformed by kustomize Example: #1319, #1322, #1347 and etc.\nThe fields transformed by kustomize is configured explicitly in defaultconfig. The configuration itself can be customized by including configurations in kustomization.yaml, e.g.\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationconfigurations:- kustomizeconfig.yamlThe configuration directive allows customization of the following transformers:\ncommonAnnotations:[]commonLabels:[]nameprefix:[]namespace:[]varreference:[]namereference:[]images:[]replicas:[]To persist the changes to default configuration, submit a PR like #1338, #1348 and etc.\n","excerpt":"kubectl doesn\u0026rsquo;t have the latest kustomize, when will it be updated? TLDR: This is blocked on …","ref":"/faq/kustomize/","title":"Kustomize"},{"body":" Kustomize Kustomize 提供了一种自定义 Kubernetes 资源配置的解决方案，该方案摆脱了模板和 DSL。\nGet Started   Contribute             参与贡献 欢迎在 Github 提交 RP，贡献你的力量。\n    ","excerpt":" Kustomize Kustomize 提供了一种自定义 Kubernetes 资源配置的解决方案，该方案摆脱了模板和 DSL。\nGet Started   Contribute …","ref":"/zh/","title":"Kustomize"},{"body":"apiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationnamePrefix:alices-deployment 名称从 wordpress 变为 alices-wordpress。\n","excerpt":"apiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationnamePrefix:alices-deployment …","ref":"/zh/api-reference/kustomization/nameprefix/","title":"namePrefix"},{"body":"apiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationnamespace:my-namespace如果在资源上设置了现有 namespace，则将覆盖现有 namespace；如果在资源上未设置现有 namespace，则使用现有 namespace。\n","excerpt":"apiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationnamespace:my-namespace …","ref":"/zh/api-reference/kustomization/namespace/","title":"namespace"},{"body":"apiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationnameSuffix:-v2deployment 名称从 wordpress 变为 wordpress-v2。\n注意: 如果资源类型是 ConfigMap 或 Secret，则在哈希值之前添加后缀。\n","excerpt":"apiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationnameSuffix:-v2deployment 名称从 wordpress 变 …","ref":"/zh/api-reference/kustomization/namesuffix/","title":"nameSuffix"},{"body":"Patches 在资源上添加或覆盖字段，Kustomization 使用 patches 字段来提供该功能。\npatches 字段包含要按指定顺序应用的 patch 列表。\npatch 可以:\n 是一个 strategic merge patch，或者是一个 JSON patch。 也可以是 patch 文件或 inline string 针对单个资源或多个资源  目标选择器可以通过 group、version、kind、name、namespace、标签选择器和注释选择器来选择资源，选择一个或多个匹配所有指定字段的资源来应用 patch。\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationpatches:- path:patch.yamltarget:group:appsversion:v1kind:Deploymentname:deploy.*labelSelector:\u0026#34;env=dev\u0026#34;annotationSelector:\u0026#34;zone=west\u0026#34;- patch:|-- op: replace path: /some/existing/path value: new valuetarget:kind:MyKindlabelSelector:\u0026#34;env=dev\u0026#34;patch 目标选择器的 name 和 namespace 字段是自动锚定的正则表达式。这意味着 myapp 的值相当于 ^myapp$。\n","excerpt":"Patches 在资源上添加或覆盖字段，Kustomization 使用 patches 字段来提供该功能。\npatches 字段包含要按指定顺序应用的 patch 列表。\npatch 可以:\n 是一 …","ref":"/zh/api-reference/kustomization/patches/","title":"patches"},{"body":"patchesJson6902 列表中的每个条目都应可以解析为 kubernetes 对象和将应用于该对象的 JSON patch。\n目标字段指向的 kubernetes 对象的 group、 version、 kind、 name 和 namespace 在同一 kustomization 内 path 字段内容是 JSON patch 文件的相对路径。\npatch 文件中的内容可以如下这种 JSON 格式：\n[ {\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/some/new/path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;value\u0026#34;}, {\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/some/existing/path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;new value\u0026#34;} ] 也可以使用 YAML 格式表示：\n- op:addpath:/some/new/pathvalue:value- op:replacepath:/some/existing/pathvalue:new valueapiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationpatchesJson6902:- target:version:v1kind:Deploymentname:my-deploymentpath:add_init_container.yaml- target:version:v1kind:Servicename:my-servicepath:add_service_annotation.yamlpatch 内容也可以是一个inline string：\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationpatchesJson6902:- target:version:v1kind:Deploymentname:my-deploymentpatch:|-- op: add path: /some/new/path value: value - op: replace path: /some/existing/path value: \u0026#34;new value\u0026#34;","excerpt":"patchesJson6902 列表中的每个条目都应可以解析为 kubernetes 对象和将应用于该对象的 JSON patch。\n目标字段指向的 kubernetes 对象的 group、 …","ref":"/zh/api-reference/kustomization/patchesjson6902/","title":"patchesJson6902"},{"body":"此列表中的每个条目都应可以解析为 StrategicMergePatch.\n这些（也可能是部分的）资源文件中的 name 必须与已经通过 resources 加载的 name 字段匹配，或者通过 bases 中的 name 字段匹配。这些条目将用于 patch（修改）已知资源。\n推荐使用小的 patches，例如：修改内存的 request/limit，更改 ConfigMap 中的 env 变量等。小的 patches 易于维护和查看，并且易于在 overlays 中混合使用。\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationpatchesStrategicMerge:- service_port_8888.yaml- deployment_increase_replicas.yaml- deployment_increase_memory.yamlpatch 内容也可以是一个inline string：\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationpatchesStrategicMerge:- |-apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: template: spec: containers: - name: nginx image: nignx:latest请注意，kustomize 不支持同一个 patch 对象中包含多个 删除 指令。要从一个对象中删除多个字段或切片元素，需要创建一个单独的 patch，以执行所有需要的删除。\n","excerpt":"此列表中的每个条目都应可以解析为 StrategicMergePatch.\n这些（也可能是部分的）资源文件中的 name 必须与已经通过 resources 加载的 name 字段匹配， …","ref":"/zh/api-reference/kustomization/patchesstrategicmerge/","title":"patchesStrategicMerge"},{"body":"对于如下 kubernetes Deployment 片段：\n# deployment.yamlkind:Deploymentmetadata:name:deployment-namespec:replicas:3在 kustomization 中添加以下内容，将副本数更改为 5：\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationreplicas:- name:deployment-namecount:5该字段内容为列表，所以可以同时修改许多资源。\n由于这个声明无法设置 kind: 或 group:，所以他只能匹配如下资源中的一种：\n Deployment ReplicationController ReplicaSet StatefulSet  对于更复杂的用例，请使用 patch 。\n","excerpt":"对于如下 kubernetes Deployment 片段：\n# …","ref":"/zh/api-reference/kustomization/replicas/","title":"replicas"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/zh/search/","title":"Search Results"},{"body":"列表中的每个条目都将生成一个 Secret（合计可以生成 n 个 Secrets）。\n功能与之前描述的 configMapGenerator 字段类似。\napiVersion:kustomize.config.k8s.io/v1beta1kind:KustomizationsecretGenerator:- name:app-tlsfiles:- secret/tls.cert- secret/tls.keytype:\u0026#34;kubernetes.io/tls\u0026#34;- name:app-tls-namespaced# you can define a namespace to generate# a secret in, defaults to: \u0026#34;default\u0026#34;namespace:appsfiles:- tls.crt=catsecret/tls.cert- tls.key=secret/tls.keytype:\u0026#34;kubernetes.io/tls\u0026#34;- name:env_file_secretenvs:- env.txttype:Opaque- name:secret-with-annotationfiles:- app-config.yamltype:Opaqueoptions:annotations:app_config:\u0026#34;true\u0026#34;labels:app.kubernetes.io/name:\u0026#34;app2\u0026#34;","excerpt":"列表中的每个条目都将生成一个 Secret（合计可以生成 n 个 Secrets）。\n功能与之前描述的 configMapGenerator 字段类似。 …","ref":"/zh/api-reference/kustomization/secretegenerator/","title":"secretGenerator"},{"body":"  Guides and API References for Kubectl and Kustomize.\nGet Started         Kubectl is a Kubernetes CLI, which provides a swiss army knife of functionality for working with Kubernetes clusters. It can be used to deploy and manage applications on Kubernetes, and for scripting and building higher-level frameworks. Goto Reference        Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. Goto Reference          Join us on Slack Join the community on Slack\nJoin us …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nContribute to Kubectl / Kustomize …\n   Follow us on Twitter! For announcement of latest features etc.\nFollow us …\n    ","excerpt":"Guides and API References for Kubectl and Kustomize.\nGet Started         Kubectl is a Kubernetes …","ref":"/","title":"SIG CLI"},{"body":"Vars 用于从一个 resource 字段中获取值，并将该值插入指定位置 - 反射功能。\n例如，假设需要在容器的 command 中指定了 Service 对象的名称，并在容器的 env 中指定了 Secret 对象的名称来确保以下内容可以正常工作：\ncontainers:- image:myimagecommand:[\u0026#34;start\u0026#34;,\u0026#34;--host\u0026#34;,\u0026#34;$(MY_SERVICE_NAME)\u0026#34;]env:- name:SECRET_TOKENvalue:$(SOME_SECRET_NAME)则可以在 vars： 中添加如下内容：\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationvars:- name:SOME_SECRET_NAMEobjref:kind:Secretname:my-secretapiVersion:v1- name:MY_SERVICE_NAMEobjref:kind:Servicename:my-serviceapiVersion:v1fieldref:fieldpath:metadata.name- name:ANOTHER_DEPLOYMENTS_POD_RESTART_POLICYobjref:kind:Deploymentname:my-deploymentapiVersion:apps/v1fieldref:fieldpath:spec.template.spec.restartPolicyvar 是包含该对象的变量名、对象引用和字段引用的元组。\n字段引用是可选的，默认为 metadata.name，这是正常的默认值，因为 kustomize 用于生成或修改 resources 的名称。\n在撰写本文档时，仅支持字符串类型字段，不支持 ints，bools，arrays 等。例如，在某些pod模板的容器编号2中提取镜像的名称是不可能的。\n变量引用，即字符串 \u0026lsquo;$(FOO)\u0026rsquo; ，只能放在 kustomize 配置指定的特定对象的特定字段中。\n关于 vars 的默认配置数据可以查看： /api/konfig/builtinpluginconsts/varreference.go\n默认目标是所有容器 command args 和 env 字段。\nVars 不应该 被用于 kustomize 已经处理过的配置中插入 names 。 例如， Deployment 可以通过 name 引用 ConfigMap ，如果 kustomize 更改 ConfigMap 的名称，则知道更改 Deployment 中的引用的 name 。\n","excerpt":"Vars 用于从一个 resource 字段中获取值，并将该值插入指定位置 - 反射功能。\n例如，假设需要在容器的 command 中指定了 Service 对象的名称，并在容器的 env …","ref":"/zh/api-reference/kustomization/vars/","title":"vars"},{"body":"","excerpt":"","ref":"/zh/blog/","title":"Kustomize 博客"},{"body":"以下是 Kustomize 贡献指南。\n","excerpt":"以下是 Kustomize 贡献指南。","ref":"/zh/contributing/","title":"贡献指南"}]